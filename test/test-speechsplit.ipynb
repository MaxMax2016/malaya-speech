{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "frank-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "supreme-freeware",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "SOURCE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__name__)))\n",
    "sys.path.insert(0, SOURCE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "general-tolerance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install pysptk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "million-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya_speech\n",
    "from pysptk import sptk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "introductory-upper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 100, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.random.normal(size = (10, 100, 2))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adult-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "honey-basis",
   "metadata": {},
   "outputs": [],
   "source": [
    "vggvox_v2 = malaya_speech.gender.deep_model(model = 'vggvox-v2')\n",
    "speaker_model = malaya_speech.speaker_vector.deep_model('vggvox-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "loaded-jimmy",
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = {'female': [100, 600], 'male': [50, 250]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "enormous-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import get_window\n",
    "from scipy import signal\n",
    "import soundfile as sf\n",
    "import random\n",
    "\n",
    "sr = 22050\n",
    "\n",
    "def butter_highpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = signal.butter(order, normal_cutoff, btype='high', analog=False)\n",
    "    return b, a\n",
    "\n",
    "b, a = butter_highpass(30, sr, order=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vietnamese-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "def speaker_normalization(f0, index_nonzero, mean_f0, std_f0):\n",
    "    f0 = f0.astype(float).copy()\n",
    "    f0[index_nonzero] = (f0[index_nonzero] - mean_f0) / std_f0\n",
    "    f0[index_nonzero] = np.clip(f0[index_nonzero], -3, 4)\n",
    "    return f0\n",
    "\n",
    "def preprocess_wav(x):\n",
    "    if x.shape[0] % 256 == 0:\n",
    "        x = np.concatenate((x, np.array([1e-06])), axis=0)\n",
    "    y = signal.filtfilt(b, a, x)\n",
    "    wav = y * 0.96 + (np.random.uniform(size = y.shape[0]) - 0.5)*1e-06\n",
    "    return wav\n",
    "\n",
    "def get_f0(wav, lo, hi):\n",
    "    f0_rapt = sptk.rapt(wav.astype(np.float32)*32768, sr, 256, min=lo, max=hi, otype=2)\n",
    "    index_nonzero = (f0_rapt != -1e10)\n",
    "    mean_f0, std_f0 = np.mean(f0_rapt[index_nonzero]), np.std(f0_rapt[index_nonzero])\n",
    "    return speaker_normalization(f0_rapt, index_nonzero, mean_f0, std_f0)\n",
    "\n",
    "def pad_seq(x, base = 8):\n",
    "    len_out = int(base * ceil(float(x.shape[0]) / base))\n",
    "    len_pad = len_out - x.shape[0]\n",
    "    assert len_pad >= 0\n",
    "    return np.pad(x, ((0, len_pad), (0, 0)), 'constant'), x.shape[0]\n",
    "\n",
    "def get_speech(f, hop_size = 256):\n",
    "    x, fs = malaya_speech.load(f, sr = sr)\n",
    "    wav = preprocess_wav(x)\n",
    "    lo, hi = freqs.get(vggvox_v2(x), [50, 250])\n",
    "    f0 = np.expand_dims(get_f0(wav, lo, hi), -1)\n",
    "    mel = malaya_speech.featurization.universal_mel(wav)\n",
    "    \n",
    "    batch_max_steps = random.randint(16384, 77175)\n",
    "    batch_max_frames = batch_max_steps // hop_size\n",
    "    \n",
    "    if len(mel) > batch_max_frames:\n",
    "        interval_start = 0\n",
    "        interval_end = len(mel) - batch_max_frames\n",
    "        start_frame = random.randint(interval_start, interval_end)\n",
    "        start_step = start_frame * hop_size\n",
    "        wav = wav[start_step : start_step + batch_max_steps]\n",
    "        mel = mel[start_frame : start_frame + batch_max_frames, :]\n",
    "        f0 = f0[start_frame : start_frame + batch_max_frames, :]\n",
    "    \n",
    "    mel, _ = pad_seq(mel)\n",
    "    f0, _ = pad_seq(f0) \n",
    "        \n",
    "    v = speaker_model([wav])[0]\n",
    "    v = v / v.max()\n",
    "    return wav, mel, f0, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "competent-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav, mel, f0, v = get_speech('../speech/example-speaker/female.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "electrical-strengthening",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_1, mel_1, f0_1, v_1 = get_speech('../speech/example-speaker/khalil-nooh.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "alternate-luther",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 168, 80), [120, 168])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mels, mel_lens = malaya_speech.padding.sequence_nd([mel, mel_1], dim = 0, return_len = True)\n",
    "mels.shape, mel_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "continent-homework",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2, 168, 1), [120, 168])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0s, f0_lens = malaya_speech.padding.sequence_nd([f0, f0_1], dim = 0, return_len = True)\n",
    "f0s.shape, f0_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "altered-salon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 512)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = malaya_speech.padding.sequence_nd([v, v_1], dim = 0)\n",
    "vs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "close-costs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "framed-manchester",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, None, 80])\n",
    "X_f0 = tf.placeholder(tf.float32, [None, None, 1])\n",
    "len_X = tf.placeholder(tf.int32, [None])\n",
    "V = tf.placeholder(tf.float32, [None, 512])\n",
    "\n",
    "# X = tf.convert_to_tensor(mels.astype(np.float32))\n",
    "# X_f0 = tf.convert_to_tensor(f0s.astype(np.float32))\n",
    "# len_X = tf.convert_to_tensor(mel_lens)\n",
    "# V = tf.convert_to_tensor(vs.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "jewish-designer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:38: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:39: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:40: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:42: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:43: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from malaya_speech.train.model import speechsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "promising-literature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "hparams = speechsplit.hparams\n",
    "\n",
    "interplnr = speechsplit.InterpLnr(hparams)\n",
    "model = speechsplit.Model(hparams)\n",
    "model_F0 = speechsplit.Model_F0(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "liberal-token",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "bottleneck_speaker = tf.keras.layers.Dense(hparams.dim_spk_emb)\n",
    "speaker_dim = bottleneck_speaker(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "large-iraqi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(81)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_f0_intrp = interplnr(tf.concat([X, X_f0], axis = -1), len_X)\n",
    "x_f0_intrp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "distinct-original",
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_org_intrp = speechsplit.quantize_f0_tf(x_f0_intrp[:,:,-1])\n",
    "x_f0_intrp_org = tf.concat((x_f0_intrp[:,:,:-1], f0_org_intrp), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "remarkable-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "f0_org = speechsplit.quantize_f0_tf(X_f0[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "mounted-montana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Reshape_1:0' shape=(?, ?, 257) dtype=float32>,\n",
       " <tf.Tensor 'concat_1:0' shape=(?, ?, 337) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder:0' shape=(?, ?, 80) dtype=float32>,\n",
       " <tf.Tensor 'dense_2/BiasAdd:0' shape=(?, 128) dtype=float32>,\n",
       " <tf.Tensor 'Reshape_3:0' shape=(?, ?, 257) dtype=float32>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0_org_intrp, x_f0_intrp_org, X, speaker_dim, f0_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "interstate-wireless",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"speechsplit/Encoder_7/strided_slice:0\", shape=(?, ?, 80), dtype=float32) Tensor(\"speechsplit/Encoder_7/strided_slice_1:0\", shape=(?, ?, 257), dtype=float32)\n",
      "Tensor(\"speechsplit/Encoder_7/strided_slice_4:0\", shape=(?, ?, 512), dtype=float32) Tensor(\"speechsplit/Encoder_7/strided_slice_5:0\", shape=(?, ?, 256), dtype=float32)\n",
      "Tensor(\"speechsplit/Encoder_7/strided_slice_8:0\", shape=(?, ?, 512), dtype=float32) Tensor(\"speechsplit/Encoder_7/strided_slice_9:0\", shape=(?, ?, 256), dtype=float32)\n",
      "(?, ?, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(None), Dimension(16)]),\n",
       " TensorShape([Dimension(None), Dimension(None), Dimension(64)]),\n",
       " TensorShape([Dimension(None), Dimension(None), Dimension(2)]),\n",
       " TensorShape([Dimension(None), Dimension(None), Dimension(210)]),\n",
       " TensorShape([Dimension(None), Dimension(None), Dimension(80)]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes_x, codes_f0, codes_2, encoder_outputs, mel_outputs = model(x_f0_intrp_org, X, speaker_dim)\n",
    "codes_x.shape, codes_f0.shape, codes_2.shape, encoder_outputs.shape, mel_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "valid-magnitude",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, ?, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'speechsplit_f0/Decoder_4/LinearNorm/dense_1/BiasAdd:0' shape=(?, ?, 257) dtype=float32>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, _, _, f0_target = model_F0(X, f0_org)\n",
    "f0_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "traditional-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "supposed-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = sess.run([x_f0_intrp], feed_dict = {\n",
    "    X: mels, X_f0: f0s, len_X: mel_lens, V: vs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "loose-entertainment",
   "metadata": {},
   "outputs": [],
   "source": [
    "o = sess.run([codes_x, codes_f0, codes_2, encoder_outputs, mel_outputs], feed_dict = {\n",
    "    X: mels, X_f0: f0s, len_X: mel_lens, V: vs\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "technological-restoration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 168, 257)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = sess.run([f0_target], feed_dict = {\n",
    "    X: mels, X_f0: f0s, len_X: mel_lens, V: vs\n",
    "})\n",
    "o[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "charitable-david",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_2/kernel:0' shape=(512, 128) dtype=float32>,\n",
       " <tf.Variable 'dense_2/bias:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential/ConvNorm/conv1d/kernel:0' shape=(5, 80, 512) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential/ConvNorm/conv1d/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential/group_normalization/gamma:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential/group_normalization/beta:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_1/ConvNorm/conv1d_1/kernel:0' shape=(5, 257, 256) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_1/ConvNorm/conv1d_1/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_1/group_normalization_1/gamma:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_1/group_normalization_1/beta:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_2/ConvNorm/conv1d_2/kernel:0' shape=(5, 512, 512) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_2/ConvNorm/conv1d_2/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_2/group_normalization_2/gamma:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_2/group_normalization_2/beta:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_3/ConvNorm/conv1d_3/kernel:0' shape=(5, 256, 256) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_3/ConvNorm/conv1d_3/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_3/group_normalization_3/gamma:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_3/group_normalization_3/beta:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_4/ConvNorm/conv1d_4/kernel:0' shape=(5, 512, 512) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_4/ConvNorm/conv1d_4/bias:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_4/group_normalization_4/gamma:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_4/group_normalization_4/beta:0' shape=(512,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_5/ConvNorm/conv1d_5/kernel:0' shape=(5, 256, 256) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_5/ConvNorm/conv1d_5/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_5/group_normalization_5/gamma:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/sequential_5/group_normalization_5/beta:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional/forward_lstm/kernel:0' shape=(512, 32) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional/forward_lstm/recurrent_kernel:0' shape=(8, 32) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional/forward_lstm/bias:0' shape=(32,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional/backward_lstm/kernel:0' shape=(512, 32) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional/backward_lstm/recurrent_kernel:0' shape=(8, 32) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional/backward_lstm/bias:0' shape=(32,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional_1/forward_lstm_1/kernel:0' shape=(256, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional_1/forward_lstm_1/recurrent_kernel:0' shape=(32, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional_1/forward_lstm_1/bias:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional_1/backward_lstm_1/kernel:0' shape=(256, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional_1/backward_lstm_1/recurrent_kernel:0' shape=(32, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_7/bidirectional_1/backward_lstm_1/bias:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/sequential_6/ConvNorm/conv1d_6/kernel:0' shape=(5, 80, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/sequential_6/ConvNorm/conv1d_6/bias:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/sequential_6/group_normalization_6/gamma:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/sequential_6/group_normalization_6/beta:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/bidirectional_2/forward_lstm_2/kernel:0' shape=(128, 4) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/bidirectional_2/forward_lstm_2/recurrent_kernel:0' shape=(1, 4) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/bidirectional_2/forward_lstm_2/bias:0' shape=(4,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/bidirectional_2/backward_lstm_2/kernel:0' shape=(128, 4) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/bidirectional_2/backward_lstm_2/recurrent_kernel:0' shape=(1, 4) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Encoder_t/bidirectional_2/backward_lstm_2/bias:0' shape=(4,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_3/forward_lstm_3/kernel:0' shape=(210, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_3/forward_lstm_3/recurrent_kernel:0' shape=(512, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_3/forward_lstm_3/bias:0' shape=(2048,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_3/backward_lstm_3/kernel:0' shape=(210, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_3/backward_lstm_3/recurrent_kernel:0' shape=(512, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_3/backward_lstm_3/bias:0' shape=(2048,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_4/forward_lstm_4/kernel:0' shape=(1024, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_4/forward_lstm_4/recurrent_kernel:0' shape=(512, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_4/forward_lstm_4/bias:0' shape=(2048,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_4/backward_lstm_4/kernel:0' shape=(1024, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_4/backward_lstm_4/recurrent_kernel:0' shape=(512, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_4/backward_lstm_4/bias:0' shape=(2048,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_5/forward_lstm_5/kernel:0' shape=(1024, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_5/forward_lstm_5/recurrent_kernel:0' shape=(512, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_5/forward_lstm_5/bias:0' shape=(2048,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_5/backward_lstm_5/kernel:0' shape=(1024, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_5/backward_lstm_5/recurrent_kernel:0' shape=(512, 2048) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/sequential_7/bidirectional_5/backward_lstm_5/bias:0' shape=(2048,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/LinearNorm/dense/kernel:0' shape=(1024, 80) dtype=float32>,\n",
       " <tf.Variable 'speechsplit/Decoder_3/LinearNorm/dense/bias:0' shape=(80,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/sequential_8/ConvNorm/conv1d_7/kernel:0' shape=(5, 80, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/sequential_8/ConvNorm/conv1d_7/bias:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/sequential_8/group_normalization_7/gamma:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/sequential_8/group_normalization_7/beta:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/bidirectional_6/forward_lstm_6/kernel:0' shape=(128, 4) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/bidirectional_6/forward_lstm_6/recurrent_kernel:0' shape=(1, 4) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/bidirectional_6/forward_lstm_6/bias:0' shape=(4,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/bidirectional_6/backward_lstm_6/kernel:0' shape=(128, 4) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/bidirectional_6/backward_lstm_6/recurrent_kernel:0' shape=(1, 4) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_t/bidirectional_6/backward_lstm_6/bias:0' shape=(4,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_9/ConvNorm/conv1d_8/kernel:0' shape=(5, 257, 256) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_9/ConvNorm/conv1d_8/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_9/group_normalization_8/gamma:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_9/group_normalization_8/beta:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_10/ConvNorm/conv1d_9/kernel:0' shape=(5, 256, 256) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_10/ConvNorm/conv1d_9/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_10/group_normalization_9/gamma:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_10/group_normalization_9/beta:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_11/ConvNorm/conv1d_10/kernel:0' shape=(5, 256, 256) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_11/ConvNorm/conv1d_10/bias:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_11/group_normalization_10/gamma:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/sequential_11/group_normalization_10/beta:0' shape=(256,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/bidirectional_7/forward_lstm_7/kernel:0' shape=(256, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/bidirectional_7/forward_lstm_7/recurrent_kernel:0' shape=(32, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/bidirectional_7/forward_lstm_7/bias:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/bidirectional_7/backward_lstm_7/kernel:0' shape=(256, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/bidirectional_7/backward_lstm_7/recurrent_kernel:0' shape=(32, 128) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Encoder_6/bidirectional_7/backward_lstm_7/bias:0' shape=(128,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_8/forward_lstm_8/kernel:0' shape=(66, 1024) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_8/forward_lstm_8/recurrent_kernel:0' shape=(256, 1024) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_8/forward_lstm_8/bias:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_8/backward_lstm_8/kernel:0' shape=(66, 1024) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_8/backward_lstm_8/recurrent_kernel:0' shape=(256, 1024) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_8/backward_lstm_8/bias:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_9/forward_lstm_9/kernel:0' shape=(512, 1024) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_9/forward_lstm_9/recurrent_kernel:0' shape=(256, 1024) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_9/forward_lstm_9/bias:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_9/backward_lstm_9/kernel:0' shape=(512, 1024) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_9/backward_lstm_9/recurrent_kernel:0' shape=(256, 1024) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/sequential_12/bidirectional_9/backward_lstm_9/bias:0' shape=(1024,) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/LinearNorm/dense_1/kernel:0' shape=(512, 257) dtype=float32>,\n",
       " <tf.Variable 'speechsplit_f0/Decoder_4/LinearNorm/dense_1/bias:0' shape=(257,) dtype=float32>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "adjustable-acrobat",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "severe-interface",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test/model.ckpt'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, 'test/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "reasonable-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 186296\r\n",
      "-rw-r--r--  1 huseinzolkepli  staff    77B May 12 19:59 checkpoint\r\n",
      "-rw-r--r--  1 huseinzolkepli  staff    88M May 12 19:59 model.ckpt.data-00000-of-00001\r\n",
      "-rw-r--r--  1 huseinzolkepli  staff   5.2K May 12 19:59 model.ckpt.index\r\n",
      "-rw-r--r--  1 huseinzolkepli  staff   2.6M May 12 19:59 model.ckpt.meta\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh test\n",
    "!rm -rf test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-starter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
