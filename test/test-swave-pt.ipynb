{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "processed-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "modified-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "SOURCE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__name__)))\n",
    "sys.path.insert(0, SOURCE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "serious-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya_speech\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "geological-vertical",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smaller-england",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "numeric-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(20, 6, 10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "linear-force",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 6, 10, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.GroupNorm(3, 6)(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rental-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tf = tf.random.normal(shape = (20, 10, 10, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "polished-memphis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(20), Dimension(10), Dimension(10), Dimension(6)])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.contrib.layers.group_norm(x_tf, groups = 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "similar-costa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 90090)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr = 8000\n",
    "y, _ = malaya_speech.load('../speech/example-speaker/husein-zolkepli.wav')\n",
    "y = np.expand_dims(y, 0).astype(np.float32)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "uniform-radical",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pt = torch.from_numpy(y)\n",
    "y_tf = tf.convert_to_tensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "understanding-hormone",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 9.6471e-05, -2.3510e-05, -4.9451e-05,  ..., -1.4349e-04,\n",
       "         -9.6471e-05,  0.0000e+00]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alleged-narrow",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(126, 257)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 128\n",
    "L = 8\n",
    "H = 128\n",
    "R = 6\n",
    "C = 2\n",
    "input_normalize = False\n",
    "sample_rate = 8000\n",
    "segment = 4\n",
    "context_len = 2 * sr / 1000\n",
    "context = int(sr * context_len / 1000)\n",
    "layer = R\n",
    "filter_dim = context * 2 + 1\n",
    "num_spk = C\n",
    "segment_size = int(np.sqrt(2 * sr * segment / (L/2)))\n",
    "segment_size, filter_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "selected-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_PT(nn.Module):\n",
    "    def __init__(self, L, N):\n",
    "        super(Encoder_PT, self).__init__()\n",
    "        self.L, self.N = L, N\n",
    "        # setting 50% overlap\n",
    "        self.conv = nn.Conv1d(\n",
    "            1, N, kernel_size=L, stride=L // 2, bias=False)\n",
    "\n",
    "    def forward(self, mixture):\n",
    "        mixture = torch.unsqueeze(mixture, 1)\n",
    "        mixture_w = F.relu(self.conv(mixture))\n",
    "        return mixture_w\n",
    "    \n",
    "class MulCatBlock_PT(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, dropout=0, bidirectional=False):\n",
    "        super(MulCatBlock_PT, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_direction = int(bidirectional) + 1\n",
    "\n",
    "        self.rnn = nn.LSTM(input_size, hidden_size, 1, dropout=dropout,\n",
    "                           batch_first=True, bidirectional=bidirectional)\n",
    "        self.rnn_proj = nn.Linear(hidden_size * self.num_direction, input_size)\n",
    "\n",
    "        self.gate_rnn = nn.LSTM(input_size, hidden_size, num_layers=1,\n",
    "                                batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.gate_rnn_proj = nn.Linear(\n",
    "            hidden_size * self.num_direction, input_size)\n",
    "\n",
    "        self.block_projection = nn.Linear(input_size * 2, input_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "        # run rnn module\n",
    "        rnn_output, _ = self.rnn(output)\n",
    "        rnn_output = self.rnn_proj(rnn_output.contiguous(\n",
    "        ).view(-1, rnn_output.shape[2])).view(output.shape).contiguous()\n",
    "        # run gate rnn module\n",
    "        gate_rnn_output, _ = self.gate_rnn(output)\n",
    "        gate_rnn_output = self.gate_rnn_proj(gate_rnn_output.contiguous(\n",
    "        ).view(-1, gate_rnn_output.shape[2])).view(output.shape).contiguous()\n",
    "        # apply gated rnn\n",
    "        gated_output = torch.mul(rnn_output, gate_rnn_output)\n",
    "        gated_output = torch.cat([gated_output, output], 2)\n",
    "        gated_output = self.block_projection(\n",
    "            gated_output.contiguous().view(-1, gated_output.shape[2])).view(output.shape)\n",
    "        return gated_output\n",
    "        \n",
    "class ByPass_PT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ByPass_PT, self).__init__()\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "\n",
    "class DPMulCat_PT(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_spk,\n",
    "                 dropout=0, num_layers=1, bidirectional=True, input_normalize=False):\n",
    "        super(DPMulCat_PT, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_norm = input_normalize\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.rows_grnn = nn.ModuleList([])\n",
    "        self.cols_grnn = nn.ModuleList([])\n",
    "        self.rows_normalization = nn.ModuleList([])\n",
    "        self.cols_normalization = nn.ModuleList([])\n",
    "\n",
    "        # create the dual path pipeline\n",
    "        for i in range(num_layers):\n",
    "            self.rows_grnn.append(MulCatBlock_PT(\n",
    "                input_size, hidden_size, dropout, bidirectional=bidirectional))\n",
    "            self.cols_grnn.append(MulCatBlock_PT(\n",
    "                input_size, hidden_size, dropout, bidirectional=bidirectional))\n",
    "            if self.in_norm:\n",
    "                self.rows_normalization.append(\n",
    "                    nn.GroupNorm(1, input_size, eps=1e-8))\n",
    "                self.cols_normalization.append(\n",
    "                    nn.GroupNorm(1, input_size, eps=1e-8))\n",
    "            else:\n",
    "                # used to disable normalization\n",
    "                self.rows_normalization.append(ByPass_PT())\n",
    "                self.cols_normalization.append(ByPass_PT())\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "            nn.PReLU(), nn.Conv2d(input_size, output_size * num_spk, 1))\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size, _, d1, d2 = input.shape\n",
    "        output = input\n",
    "        output_all = []\n",
    "        for i in range(self.num_layers):\n",
    "            row_input = output.permute(0, 3, 2, 1).contiguous().view(batch_size * d2, d1, -1)\n",
    "            \n",
    "            row_output = self.rows_grnn[i](row_input)\n",
    "            row_output = row_output.view(\n",
    "                batch_size, d2, d1, -1).permute(0, 3, 2, 1).contiguous()\n",
    "            row_output = self.rows_normalization[i](row_output)\n",
    "            # apply a skip connection\n",
    "            output = output + row_output\n",
    "            \n",
    "            print(i, row_input.shape, row_output.shape, output.shape)\n",
    "\n",
    "            col_input = output.permute(0, 2, 3, 1).contiguous().view(\n",
    "                batch_size * d1, d2, -1)\n",
    "            col_output = self.cols_grnn[i](col_input)\n",
    "            col_output = col_output.view(\n",
    "                batch_size, d1, d2, -1).permute(0, 3, 1, 2).contiguous()\n",
    "            col_output = self.cols_normalization[i](col_output).contiguous()\n",
    "            # apply a skip connection\n",
    "            output = output + col_output\n",
    "            \n",
    "            print(i, col_input.shape, col_output.shape, output.shape)\n",
    "\n",
    "            output_i = self.output(output)\n",
    "            output_all.append(output_i)\n",
    "        return output_all\n",
    "\n",
    "        \n",
    "class Separator_PT(nn.Module):\n",
    "    def __init__(self, input_dim, feature_dim, hidden_dim, output_dim, num_spk=2,\n",
    "                 layer=4, segment_size=100, input_normalize=False, bidirectional=True):\n",
    "        super(Separator_PT, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.layer = layer\n",
    "        self.segment_size = segment_size\n",
    "        self.num_spk = num_spk\n",
    "        self.input_normalize = input_normalize\n",
    "\n",
    "        self.rnn_model = DPMulCat_PT(self.feature_dim, self.hidden_dim,\n",
    "                                  self.feature_dim, self.num_spk, num_layers=layer, bidirectional=bidirectional, input_normalize=input_normalize)\n",
    "\n",
    "    # ======================================= #\n",
    "    # The following code block was borrowed and modified from https://github.com/yluo42/TAC\n",
    "    # ================ BEGIN ================ #\n",
    "    def pad_segment(self, input, segment_size):\n",
    "        # input is the features: (B, N, T)\n",
    "        batch_size, dim, seq_len = input.shape\n",
    "        segment_stride = segment_size // 2\n",
    "        rest = segment_size - (segment_stride + seq_len %\n",
    "                               segment_size) % segment_size\n",
    "        if rest > 0:\n",
    "            pad = Variable(torch.zeros(batch_size, dim, rest)\n",
    "                           ).type(input.type())\n",
    "            input = torch.cat([input, pad], 2)\n",
    "\n",
    "        pad_aux = Variable(torch.zeros(\n",
    "            batch_size, dim, segment_stride)).type(input.type())\n",
    "        input = torch.cat([pad_aux, input, pad_aux], 2)\n",
    "        return input, rest\n",
    "\n",
    "    def create_chuncks(self, input, segment_size):\n",
    "        # split the feature into chunks of segment size\n",
    "        # input is the features: (B, N, T)\n",
    "\n",
    "        input, rest = self.pad_segment(input, segment_size)\n",
    "        batch_size, dim, seq_len = input.shape\n",
    "        segment_stride = segment_size // 2\n",
    "\n",
    "        segments1 = input[:, :, :-segment_stride].contiguous().view(batch_size,\n",
    "                                                                    dim, -1, segment_size)\n",
    "        segments2 = input[:, :, segment_stride:].contiguous().view(\n",
    "            batch_size, dim, -1, segment_size)\n",
    "        segments = torch.cat([segments1, segments2], 3).view(\n",
    "            batch_size, dim, -1, segment_size).transpose(2, 3)\n",
    "        return segments.contiguous(), rest\n",
    "\n",
    "    def merge_chuncks(self, input, rest):\n",
    "\n",
    "        batch_size, dim, segment_size, _ = input.shape\n",
    "        segment_stride = segment_size // 2\n",
    "        input = input.transpose(2, 3).contiguous().view(\n",
    "            batch_size, dim, -1, segment_size*2)  # B, N, K, L\n",
    "\n",
    "        input1 = input[:, :, :, :segment_size].contiguous().view(\n",
    "            batch_size, dim, -1)[:, :, segment_stride:]\n",
    "        input2 = input[:, :, :, segment_size:].contiguous().view(\n",
    "            batch_size, dim, -1)[:, :, :-segment_stride]\n",
    "\n",
    "        output = input1 + input2\n",
    "        if rest > 0:\n",
    "            output = output[:, :, :-rest]\n",
    "        return output.contiguous()\n",
    "    \n",
    "    def forward(self, input):\n",
    "        # create chunks\n",
    "        enc_segments, enc_rest = self.create_chuncks(\n",
    "            input, self.segment_size)\n",
    "        output_all = self.rnn_model(enc_segments)\n",
    "        \n",
    "        output_all_wav = []\n",
    "        for ii in range(len(output_all)):\n",
    "            output_ii = self.merge_chuncks(\n",
    "                output_all[ii], enc_rest)\n",
    "            print(ii, output_all[ii].shape)\n",
    "            output_all_wav.append(output_ii)\n",
    "        return output_all_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "associate-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder_pt = Encoder_PT(L, N)\n",
    "# separator_pt = Separator_PT(filter_dim + N, N, H,\n",
    "#                       filter_dim, num_spk, layer, segment_size, input_normalize)\n",
    "# e_pt = encoder_pt(y_pt)\n",
    "# o_pt = separator_pt(e_pt)\n",
    "# # l.shape, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fossil-invasion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [i.shape for i in o_pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "genuine-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_list(x):\n",
    "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "    static = x.shape.as_list()\n",
    "    dynamic = tf.shape(x)\n",
    "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, L, N, **kwargs):\n",
    "        super(Encoder, self).__init__(name = 'Encoder', **kwargs)\n",
    "        self.conv = tf.keras.layers.Conv1D(N, kernel_size=L, strides=L // 2, use_bias=False)\n",
    "    \n",
    "    def call(self, mixture):\n",
    "        mixture = tf.expand_dims(mixture, -1)\n",
    "        mixture_w = tf.nn.relu(self.conv(mixture))\n",
    "        return mixture_w\n",
    "    \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, L, **kwargs):\n",
    "        super(Decoder, self).__init__(name = 'Decoder', **kwargs)\n",
    "        self.L = L\n",
    "\n",
    "    def call(self, est_source):\n",
    "        # torch.Size([1, 256, 22521])\n",
    "        # pt (1, 2, 128, 22521), tf (1, 22521, 2, 128)\n",
    "        est_source = tf.transpose(est_source, (0, 1, 3, 2))\n",
    "        est_source = tf.compat.v1.layers.average_pooling2d(est_source, 1, (1,8),\n",
    "                                     padding = 'SAME')\n",
    "        est_source = tf.signal.overlap_and_add(tf.transpose(est_source, (0, 3, 1, 2)), self.L // 2)\n",
    "\n",
    "        return est_source\n",
    "    \n",
    "class MulCatBlock(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, dropout=0, bidirectional=False, **kwargs):\n",
    "        super(MulCatBlock, self).__init__(name = 'MulCatBlock', **kwargs)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_direction = int(bidirectional) + 1\n",
    "        \n",
    "        if bidirectional:\n",
    "            self.rnn = tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "            )\n",
    "            self.gate_rnn = tf.keras.layers.Bidirectional(\n",
    "                tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "            )\n",
    "        else:\n",
    "            self.rnn = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "            self.gate_rnn = tf.keras.layers.LSTM(hidden_size, return_sequences = True)\n",
    "        \n",
    "        self.rnn_proj = tf.keras.layers.Dense(input_size)\n",
    "        self.gate_rnn_proj = tf.keras.layers.Dense(input_size)\n",
    "        self.block_projection = tf.keras.layers.Dense(input_size)\n",
    "    \n",
    "    def call(self, input):\n",
    "        output = input\n",
    "        rnn_output = self.rnn(output)\n",
    "        rnn_output = self.rnn_proj(rnn_output)\n",
    "        gate_rnn_output = self.gate_rnn(output)\n",
    "        gate_rnn_output = self.gate_rnn_proj(gate_rnn_output)\n",
    "        gated_output = tf.multiply(rnn_output, gate_rnn_output)\n",
    "        gated_output = tf.concat([gated_output, output], 2)\n",
    "        gated_output = self.block_projection(gated_output)\n",
    "        return gated_output\n",
    "    \n",
    "class GroupNorm(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DPMulCat, self).__init__(name = 'GroupNorm', **kwargs)\n",
    "    \n",
    "    def call(self, input):\n",
    "        return tf.contrib.layers.group_norm(x_tf, groups = 1, epsilon = 1e-8)\n",
    "    \n",
    "class ByPass(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ByPass, self).__init__(name = 'ByPass', **kwargs)\n",
    "\n",
    "    def call(self, input):\n",
    "        return input\n",
    "        \n",
    "class DPMulCat(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_spk,\n",
    "                 dropout=0, num_layers=1, bidirectional=True, input_normalize=False, **kwargs):\n",
    "        super(DPMulCat, self).__init__(name = 'DPMulCat', **kwargs)\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.in_norm = input_normalize\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rows_grnn = []\n",
    "        self.cols_grnn = []\n",
    "        self.rows_normalization = []\n",
    "        self.cols_normalization = []\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            self.rows_grnn.append(MulCatBlock(\n",
    "                input_size, hidden_size, dropout, bidirectional=bidirectional))\n",
    "            self.cols_grnn.append(MulCatBlock(\n",
    "                input_size, hidden_size, dropout, bidirectional=bidirectional))\n",
    "            if self.in_norm:\n",
    "                self.rows_normalization.append(GroupNorm())\n",
    "                self.cols_normalization.append(GroupNorm())\n",
    "            else:\n",
    "                # used to disable normalization\n",
    "                self.rows_normalization.append(ByPass())\n",
    "                self.cols_normalization.append(ByPass())\n",
    "                \n",
    "        self.outputs = tf.keras.Sequential()     \n",
    "        self.outputs.add(tf.keras.layers.PReLU())\n",
    "        self.outputs.add(tf.keras.layers.Conv2D(output_size * num_spk, 1, padding = 'SAME'))\n",
    "    \n",
    "    def call(self, input):\n",
    "        # original, [b, d3, d1, d2]\n",
    "        input = tf.transpose(input, (0, 2, 1, 3))\n",
    "        batch_size, d3, d1, d2 = shape_list(input)\n",
    "        output = input\n",
    "        output_all = []\n",
    "        for i in range(self.num_layers):\n",
    "            row_input = tf.transpose(output, [0, 3, 2, 1])\n",
    "            row_input = tf.reshape(row_input, (batch_size * d2, d1, d3))\n",
    "            row_output = self.rows_grnn[i](row_input)\n",
    "            row_output = tf.reshape(row_output, (batch_size, d2, d1, d3))\n",
    "            row_output = tf.transpose(row_output, (0, 3, 2, 1))\n",
    "            row_output = self.rows_normalization[i](row_output)\n",
    "            output = output + row_output\n",
    "            \n",
    "            print(i, row_input.shape, row_output.shape, output.shape)\n",
    "            \n",
    "            col_input = tf.transpose(output, [0, 2, 3, 1])\n",
    "            col_input = tf.reshape(col_input, (batch_size * d1, d2, d3))\n",
    "            col_output = self.cols_grnn[i](col_input)\n",
    "            col_output = tf.reshape(col_output, (batch_size, d1, d2, d3))\n",
    "            col_output = tf.transpose(col_output, (0, 3, 1, 2))\n",
    "            col_output = self.cols_normalization[i](col_output)\n",
    "            \n",
    "            output = output + col_output\n",
    "            \n",
    "            print(i, col_input.shape, col_output.shape, output.shape)\n",
    "            \n",
    "            # torch.Size([1, 128, 126, 360]\n",
    "            output_i = self.outputs(tf.transpose(output, [0, 2, 3, 1]))\n",
    "            output_all.append(output_i)\n",
    "        return output_all\n",
    "    \n",
    "class Separator(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim, feature_dim, hidden_dim, output_dim, num_spk=2,\n",
    "                 layer=4, segment_size=100, input_normalize=False, bidirectional=True, **kwargs):\n",
    "        super(Separator, self).__init__(name = 'Separator', **kwargs)\n",
    "        self.input_dim = input_dim\n",
    "        self.feature_dim = feature_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.layer = layer\n",
    "        self.segment_size = segment_size\n",
    "        self.num_spk = num_spk\n",
    "        self.input_normalize = input_normalize\n",
    "        \n",
    "        self.rnn_model = DPMulCat(self.feature_dim, self.hidden_dim,\n",
    "                                  self.feature_dim, self.num_spk, num_layers=layer, \n",
    "                                  bidirectional=bidirectional, input_normalize=input_normalize)\n",
    "    \n",
    "    def pad_segment(self, input, segment_size):\n",
    "        # input is the features: (B, N, T)\n",
    "        \n",
    "        batch_size, seq_len, dim = shape_list(input)\n",
    "        segment_stride = segment_size // 2\n",
    "        rest = segment_size - (segment_stride + seq_len %\n",
    "                               segment_size) % segment_size\n",
    "        if rest > 0:\n",
    "            pad = tf.Variable(tf.zeros(shape=(batch_size, rest, dim)))\n",
    "            input = tf.concat([input, pad], 1)\n",
    "\n",
    "        pad_aux = tf.Variable(tf.zeros(shape=(batch_size, segment_stride, dim)))\n",
    "        input = tf.concat([pad_aux, input, pad_aux], 1)\n",
    "        return input, rest\n",
    "    \n",
    "    def create_chuncks(self, input, segment_size):\n",
    "\n",
    "        input, rest = self.pad_segment(input, segment_size)\n",
    "        batch_size, seq_len, dim = shape_list(input)\n",
    "        segment_stride = segment_size // 2\n",
    "        segments1 = tf.reshape(input[:, :-segment_stride], (batch_size, -1, dim, segment_size))\n",
    "        segments2 = tf.reshape(input[:, segment_stride:], (batch_size, -1, dim, segment_size))\n",
    "        segments = tf.concat([segments1, segments2], axis = 3)\n",
    "        segments = tf.reshape(segments, (batch_size, -1, dim, segment_size))\n",
    "        segments = tf.transpose(segments, perm = [0, 3, 2, 1])\n",
    "        return segments, rest\n",
    "    \n",
    "    def merge_chuncks(self, input, rest):\n",
    "        # original, [b, dim, segment_size, _]\n",
    "        # torch.Size([1, 256, 126, 360])\n",
    "        # (1, 126, 360, 256)\n",
    "        input = tf.transpose(input, perm = [0, 3, 1, 2])\n",
    "        batch_size, dim, segment_size, _ = shape_list(input)\n",
    "        segment_stride = segment_size // 2\n",
    "        # original, [b, dim, _, segment_size]\n",
    "        input = tf.transpose(input, perm = [0, 1, 3, 2])\n",
    "        input = tf.reshape(input, (batch_size, dim, -1, segment_size * 2))\n",
    "        \n",
    "        input1 = tf.reshape(input[:, :, :, :segment_size], (batch_size, dim, -1))[:, :, segment_stride:]\n",
    "        input2 = tf.reshape(input[:, :, :, segment_size:], (batch_size, dim, -1))[:, :, :-segment_stride]\n",
    "        \n",
    "        output = input1 + input2\n",
    "        if rest > 0:\n",
    "            output = output[:, :, :-rest]\n",
    "            \n",
    "        return tf.transpose(output, perm = [0, 2, 1])\n",
    "        \n",
    "    def call(self, input):\n",
    "        # create chunks\n",
    "        enc_segments, enc_rest = self.create_chuncks(\n",
    "            input, self.segment_size)\n",
    "        output_all = self.rnn_model(enc_segments)\n",
    "        output_all_wav = []\n",
    "        for ii in range(len(output_all)):\n",
    "            print(ii, output_all[ii].shape)\n",
    "            output_ii = self.merge_chuncks(\n",
    "                output_all[ii], enc_rest)\n",
    "            output_all_wav.append(output_ii)\n",
    "        return output_all_wav\n",
    "    \n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        N = 128,\n",
    "        L = 8,\n",
    "        H = 128,\n",
    "        R = 6,\n",
    "        C = 2,\n",
    "        input_normalize = False,\n",
    "        sample_rate = 8000,\n",
    "        segment = 4,\n",
    "        context_len = 2 * sr / 1000,\n",
    "        context = int(sr * context_len / 1000),\n",
    "        layer = R,\n",
    "        filter_dim = context * 2 + 1,\n",
    "        num_spk = C,\n",
    "        segment_size = int(np.sqrt(2 * sr * segment / (L / 2))),\n",
    "        **kwargs\n",
    "    ):\n",
    "        super(Model, self).__init__(name = 'swave', **kwargs)\n",
    "        self.C = C\n",
    "        self.N = N\n",
    "        self.encoder = Encoder(L, N)\n",
    "        self.separator = Separator(\n",
    "            filter_dim + N,\n",
    "            N,\n",
    "            H,\n",
    "            filter_dim,\n",
    "            num_spk,\n",
    "            layer,\n",
    "            segment_size,\n",
    "            input_normalize,\n",
    "        )\n",
    "        self.decoder = Decoder(L)\n",
    "\n",
    "    def call(self, mixture):\n",
    "        mixture_w = self.encoder(mixture)\n",
    "        output_all = self.separator(mixture_w)\n",
    "        T_mix = tf.shape(mixture)[1]\n",
    "        batch_size = tf.shape(mixture)[0]\n",
    "        T_mix_w = tf.shape(mixture_w)[1]\n",
    "        # generate wav after each RNN block and optimize the loss\n",
    "        outputs = []\n",
    "        for ii in range(len(output_all)):\n",
    "            output_ii = tf.reshape(\n",
    "                output_all[ii], (batch_size, T_mix_w, self.C, self.N)\n",
    "            )\n",
    "            output_ii = self.decoder(output_ii)\n",
    "            output_ii = tf.cond(\n",
    "                tf.shape(output_ii)[2] >= T_mix,\n",
    "                lambda: output_ii[:, :, :T_mix],\n",
    "                lambda: tf.pad(\n",
    "                    output_ii,\n",
    "                    [[0, 0], [0, 0], [0, T_mix - tf.shape(output_ii)[2]]],\n",
    "                ),\n",
    "            )\n",
    "            outputs.append(output_ii)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "conservative-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "\n",
    "# class Separator(nn.Module):\n",
    "#     def __init__(self, input_dim, feature_dim, hidden_dim, output_dim, num_spk=2,\n",
    "#                  layer=4, segment_size=100, input_normalize=False, bidirectional=True):\n",
    "\n",
    "# class MulCatBlock(nn.Module):\n",
    "\n",
    "#     def __init__(self, input_size, hidden_size, dropout=0, bidirectional=False):\n",
    "#         super(MulCatBlock, self).__init__()\n",
    "        \n",
    "# self.rnn_model = DPMulCat(self.feature_dim, self.hidden_dim,\n",
    "#                           self.feature_dim, self.num_spk, num_layers=layer, bidirectional=bidirectional, input_normalize=input_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fundamental-polish",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(1), Dimension(90090)])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "secret-infrared",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (360, 126, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "0 (126, 360, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "1 (360, 126, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "1 (126, 360, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "2 (360, 126, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "2 (126, 360, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "3 (360, 126, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "3 (126, 360, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "4 (360, 126, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "4 (126, 360, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "5 (360, 126, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "5 (126, 360, 128) (1, 128, 126, 360) (1, 128, 126, 360)\n",
      "0 (1, 126, 360, 256)\n",
      "1 (1, 126, 360, 256)\n",
      "2 (1, 126, 360, 256)\n",
      "3 (1, 126, 360, 256)\n",
      "4 (1, 126, 360, 256)\n",
      "5 (1, 126, 360, 256)\n",
      "WARNING:tensorflow:From <ipython-input-18-849ef4ab4d2a>:27: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.AveragePooling2D instead.\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/layers/pooling.py:238: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "outputs = model(y_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "knowing-royalty",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=989986, shape=(1, 2, 90090), dtype=float32, numpy=\n",
       " array([[[-1.1719704e-03, -4.6524021e-04, -9.3938681e-05, ...,\n",
       "           1.4410544e-03, -3.5640300e-04, -4.6988868e-04],\n",
       "         [-4.6890823e-04,  3.0910695e-04, -6.9982489e-05, ...,\n",
       "          -3.9416447e-04,  3.4138164e-04, -6.2367937e-05]]], dtype=float32)>,\n",
       " <tf.Tensor: id=990095, shape=(1, 2, 90090), dtype=float32, numpy=\n",
       " array([[[-2.5863254e-03,  4.0950871e-04, -1.8142318e-04, ...,\n",
       "           3.4108611e-03, -5.1781628e-04, -2.0579109e-03],\n",
       "         [ 8.2968705e-05,  7.3135237e-04,  9.3379873e-05, ...,\n",
       "           1.1433249e-03,  1.6798971e-03, -1.6701058e-03]]], dtype=float32)>,\n",
       " <tf.Tensor: id=990204, shape=(1, 2, 90090), dtype=float32, numpy=\n",
       " array([[[-2.7442961e-03,  2.0390499e-04, -9.5309957e-04, ...,\n",
       "           1.3520857e-03,  2.5793747e-04, -4.6325871e-04],\n",
       "         [ 3.0651595e-04,  3.5258654e-05, -1.7121775e-04, ...,\n",
       "          -1.0525491e-03,  2.0909510e-03, -2.2979626e-03]]], dtype=float32)>,\n",
       " <tf.Tensor: id=990313, shape=(1, 2, 90090), dtype=float32, numpy=\n",
       " array([[[-5.2498570e-03, -2.8126672e-04, -2.1741020e-03, ...,\n",
       "          -3.7511927e-03,  8.5460488e-06,  5.1424848e-03],\n",
       "         [ 2.8020504e-04,  3.9183698e-04, -2.1838429e-03, ...,\n",
       "          -4.0260917e-03,  4.1851224e-03, -4.8523727e-03]]], dtype=float32)>,\n",
       " <tf.Tensor: id=990422, shape=(1, 2, 90090), dtype=float32, numpy=\n",
       " array([[[-6.4644357e-03, -2.7639014e-03, -4.2148456e-03, ...,\n",
       "          -1.4033211e-02, -4.1615870e-04,  1.6729498e-03],\n",
       "         [ 6.3334359e-05,  2.8819581e-03, -5.5104219e-03, ...,\n",
       "          -1.2051394e-02,  1.2035366e-02, -1.4725696e-02]]], dtype=float32)>,\n",
       " <tf.Tensor: id=990531, shape=(1, 2, 90090), dtype=float32, numpy=\n",
       " array([[[-0.01018909,  0.00152426, -0.00668646, ..., -0.00214193,\n",
       "           0.0103996 , -0.00155523],\n",
       "         [ 0.00260293,  0.00046376, -0.00845932, ..., -0.02757442,\n",
       "           0.01238063, -0.02469309]]], dtype=float32)>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 torch.Size([360, 126, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 0 torch.Size([126, 360, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 1 torch.Size([360, 126, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 1 torch.Size([126, 360, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 2 torch.Size([360, 126, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 2 torch.Size([126, 360, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 3 torch.Size([360, 126, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 3 torch.Size([126, 360, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 4 torch.Size([360, 126, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 4 torch.Size([126, 360, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 5 torch.Size([360, 126, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])\n",
    "# 5 torch.Size([126, 360, 128]) torch.Size([1, 128, 126, 360]) torch.Size([1, 128, 126, 360])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-birthday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 torch.Size([1, 256, 126, 360])\n",
    "# 1 torch.Size([1, 256, 126, 360])\n",
    "# 2 torch.Size([1, 256, 126, 360])\n",
    "# 3 torch.Size([1, 256, 126, 360])\n",
    "# 4 torch.Size([1, 256, 126, 360])\n",
    "# 5 torch.Size([1, 256, 126, 360])\n",
    "\n",
    "# [torch.Size([1, 256, 22521]),\n",
    "#  torch.Size([1, 256, 22521]),\n",
    "#  torch.Size([1, 256, 22521]),\n",
    "#  torch.Size([1, 256, 22521]),\n",
    "#  torch.Size([1, 256, 22521]),\n",
    "#  torch.Size([1, 256, 22521])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "natural-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "est_source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-gardening",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.pad(output_ii, (0, 90199 - T_est)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "(0, 90099 - T_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "established-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [F.pad(output_ii, (0, 90199 - T_est)), F.pad(output_ii, (0, 90199 - T_est))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bound-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(p).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-dating",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.pad(outputs[0], [[0,0], [0,0], [0,3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-leisure",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.shape(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-psychology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
