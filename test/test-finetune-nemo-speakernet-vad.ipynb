{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import malaya_speech\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from datasets import Audio\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import namedtuple\n",
    "\n",
    "Batch = namedtuple(\"Batch\", [\"features\", 'features_length', \"targets\"])\n",
    "\n",
    "class Dataset(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    sr = 16000\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Dataset).__init__()\n",
    "        \n",
    "        files = random.sample(glob('/home/husein/ssd2/LibriSpeech/*/*/*/*.flac'), 10000)\n",
    "        edge_tts = random.sample(glob('/home/husein/ssd2/*-tts-wav/*.wav'), 10000)\n",
    "        wavenet = random.sample(glob('/home/husein/ssd2/ms-MY-Wavenet-*/*.mp3'), 10000)\n",
    "        musan_speech = glob('/home/husein/ssd2/noise/musan/speech/*/*')\n",
    "        vctk = random.sample(glob('/home/husein/ssd2/wav48_silence_trimmed/*/*.flac'), 10000)\n",
    "\n",
    "        speeches = files + edge_tts + wavenet + musan_speech + vctk\n",
    "        random.shuffle(speeches)\n",
    "        self.speeches = speeches\n",
    "        \n",
    "        mic_noise = glob('/home/husein/ssd2/noise/mic-noise/*')\n",
    "        non_speech = glob('/home/husein/ssd2/noise/Nonspeech/*')\n",
    "        musan_noise = glob('/home/husein/ssd2/noise/musan/noise/*/*.wav')\n",
    "        musan_music = glob('/home/husein/ssd2/noise/musan/music/*/*.wav')\n",
    "        noises = mic_noise + non_speech + musan_noise + musan_music\n",
    "        noises = [f for f in noises if os.path.getsize(f) / 1e6 < 10]\n",
    "        random.shuffle(noises)\n",
    "        self.noises = noises\n",
    "        \n",
    "        ami = glob('/home/husein/speech-bahasa/ami/amicorpus/*/*/*.wav')\n",
    "        self.ami = {os.path.split(f)[1].replace('.wav', ''): f for f in ami}\n",
    "        self.annotations = malaya_speech.extra.rttm.load('/home/husein/speech-bahasa/MixHeadset.train.rttm')\n",
    "        self.annotations_keys = list(self.annotations.keys())\n",
    "        \n",
    "        self.audio = Audio(sampling_rate=self.sr)\n",
    "        \n",
    "        self.frame_sizes = [30, 50, 63]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            for i in range(len(self.speeches)):\n",
    "                f = self.speeches[i]\n",
    "                y = self.audio.decode_example(self.audio.encode_example(f))['array']\n",
    "                if random.random() > 0.6:\n",
    "                    y = malaya_speech.augmentation.waveform.random_pitch(y)\n",
    "                \n",
    "                y_int = malaya_speech.astype.float_to_int(y)\n",
    "                vad = malaya_speech.vad.webrtc(minimum_amplitude = int(np.quantile(np.abs(y_int), 0.3)))\n",
    "                frames_int = malaya_speech.generator.frames(y_int, 30, self.sr, False)\n",
    "                frames = malaya_speech.generator.frames(y, 30, self.sr, False)\n",
    "                frames = [(frames[no], vad(frame)) for no, frame in enumerate(frames_int)]\n",
    "                grouped = malaya_speech.group.group_frames(frames)\n",
    "                \n",
    "                x, y = [], []\n",
    "                for g in grouped:\n",
    "                    if random.random() > 0.8:\n",
    "                        if g[1]:\n",
    "                            factor = random.uniform(0.1, 0.4)\n",
    "                        else:\n",
    "                            factor = random.uniform(0.4, 0.9)\n",
    "                        \n",
    "                        n = self.audio.decode_example(self.audio.encode_example(random.choice(noises)))['array']\n",
    "                        g[0].array = malaya_speech.augmentation.waveform.add_noise(g[0].array, n, \n",
    "                                                                                    factor = factor)\n",
    "                    \n",
    "                    frame_size = random.choice(self.frame_sizes)\n",
    "                    frames = malaya_speech.generator.frames(g[0].array, frame_size, self.sr, False)\n",
    "                    frames = [f.array for f in frames]\n",
    "                    x.extend(frames)\n",
    "                    y.extend([int(g[1])] * len(frames))\n",
    "                \n",
    "                x, y = shuffle(x, y)\n",
    "                for k in range(len(x)):\n",
    "                    yield torch.tensor(x[k], dtype = torch.float32), y[k]\n",
    "                \n",
    "                \n",
    "                mix = random.choice(self.annotations_keys)\n",
    "                sample = self.annotations[mix]\n",
    "                y, _ = malaya_speech.load(self.ami[mix])\n",
    "                if random.random() > 0.6:\n",
    "                    y = malaya_speech.augmentation.waveform.random_pitch(y)\n",
    "                \n",
    "                frame_size = random.choice(self.frame_sizes)\n",
    "                frames = malaya_speech.generator.frames(y, frame_size, self.sr, False)\n",
    "                for k in range(len(frames)):\n",
    "                    if len(sample.crop(frames[k].timestamp, frames[k].timestamp + frames[k].duration)._labelNeedsUpdate):\n",
    "                        label = 1\n",
    "                    else:\n",
    "                        label = 0\n",
    "                    \n",
    "                    yield torch.tensor(frames[k].array, dtype = torch.float32), label\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "# dataset = iter(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(batches):\n",
    "    \n",
    "    features = torch.nn.utils.rnn.pad_sequence([b[0] for b in batches], batch_first=True)\n",
    "    features_length = torch.tensor([len(b[0]) for b in batches], dtype = torch.int32)\n",
    "    targets = torch.tensor([b[1] for b in batches], dtype = torch.int64)\n",
    "    return Batch(features, features_length, targets)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size = 4, collate_fn = batch)\n",
    "loader = iter(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    b = next(loader)\n",
    "    if b.targets.numpy().mean() > 0:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
