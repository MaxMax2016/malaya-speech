{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gross-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exterior-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "SOURCE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__name__)))\n",
    "sys.path.insert(0, SOURCE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "existing-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intensive-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:38: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:39: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:40: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:42: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:43: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import malaya_speech.train.model.conformer as conformer\n",
    "import malaya_speech.train.model.transducer as transducer\n",
    "import malaya_speech\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "disturbed-photograph",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint                     model.ckpt.data-00000-of-00001\r\n",
      "frozen_model.pb                model.ckpt.index\r\n",
      "frozen_model.pb.quantized      model.ckpt.meta\r\n"
     ]
    }
   ],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/v1/vocab/malaya-speech.tokenizer.subwords\n",
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/pretrained/asr-small-conformer-output.tar.gz\n",
    "# !tar -zxf asr-small-conformer-output.tar.gz\n",
    "!ls asr-small-conformer-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surgical-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install tensorflow-datasets==3.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "proper-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords = malaya_speech.subword.load('transducer.tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "sticky-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = malaya_speech.tf_featurization.STTFeaturizer(\n",
    "    normalize_per_feature = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fatty-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '../speech/record/savewav_2020-11-26_22-36-06_294832.wav',\n",
    "    '../speech/record/savewav_2020-11-26_22-40-56_929661.wav',\n",
    "    '../speech/record/675.wav',\n",
    "    '../speech/record/664.wav',\n",
    "    '../speech/example-speaker/husein-zolkepli.wav',\n",
    "    '../speech/example-speaker/mas-aisyah.wav',\n",
    "    '../speech/example-speaker/khalil-nooh.wav',\n",
    "    '../speech/example-speaker/shafiqah-idayu.wav',\n",
    "    '../speech/khutbah/wadi-annuar.wav',\n",
    "]\n",
    "\n",
    "ys = [malaya_speech.load(f)[0] for f in files[:1]]\n",
    "padded, lens = malaya_speech.padding.sequence_1d(ys, return_len = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "representative-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.convert_to_tensor(padded.astype(np.float32))\n",
    "X_len = tf.convert_to_tensor(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "respiratory-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.compat.v1.placeholder(tf.float32, [None, None], name = 'X_placeholder')\n",
    "# X_len = tf.compat.v1.placeholder(tf.int32, [None], name = 'X_len_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unique-jesus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=350, shape=(1, 598, 80, 1), dtype=float32, numpy=\n",
       " array([[[[-2.2109754 ],\n",
       "          [-1.5769207 ],\n",
       "          [-1.416817  ],\n",
       "          ...,\n",
       "          [-0.86547065],\n",
       "          [-0.55510426],\n",
       "          [-0.55248064]],\n",
       " \n",
       "         [[-1.2292197 ],\n",
       "          [-1.4852118 ],\n",
       "          [-1.3831501 ],\n",
       "          ...,\n",
       "          [-0.9887544 ],\n",
       "          [-0.88915175],\n",
       "          [-0.6995302 ]],\n",
       " \n",
       "         [[-1.6323165 ],\n",
       "          [-2.165198  ],\n",
       "          [-2.1111379 ],\n",
       "          ...,\n",
       "          [-0.97814846],\n",
       "          [-0.9973233 ],\n",
       "          [-0.8568515 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.2239536 ],\n",
       "          [-1.4475546 ],\n",
       "          [-1.3428142 ],\n",
       "          ...,\n",
       "          [-0.66373694],\n",
       "          [-0.57195127],\n",
       "          [-0.92957073]],\n",
       " \n",
       "         [[-1.8517525 ],\n",
       "          [-2.1294234 ],\n",
       "          [-2.0033896 ],\n",
       "          ...,\n",
       "          [-0.5434857 ],\n",
       "          [-0.7122619 ],\n",
       "          [-0.4921957 ]],\n",
       " \n",
       "         [[-0.7100879 ],\n",
       "          [-1.2015659 ],\n",
       "          [-1.1553437 ],\n",
       "          ...,\n",
       "          [-0.9490056 ],\n",
       "          [-0.8389743 ],\n",
       "          [-0.41203937]]]], dtype=float32)>,\n",
       " <tf.Tensor: id=348, shape=(1,), dtype=int32, numpy=array([598], dtype=int32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = tf.shape(X)[0]\n",
    "features = tf.TensorArray(dtype = tf.float32, size = batch_size, dynamic_size = True, infer_shape = False)\n",
    "features_len = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "\n",
    "init_state = (0, features, features_len)\n",
    "\n",
    "def condition(i, features, features_len):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, features, features_len):\n",
    "    f = featurizer(X[i, :X_len[i]])\n",
    "    f_len = tf.shape(f)[0]\n",
    "    return i + 1, features.write(i, f), features_len.write(i, f_len)\n",
    "\n",
    "_, features, features_len = tf.while_loop(condition, body, init_state)\n",
    "features_len = features_len.stack()\n",
    "padded_features = tf.TensorArray(dtype = tf.float32, size = batch_size)\n",
    "padded_lens = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "maxlen = tf.reduce_max(features_len)\n",
    "\n",
    "init_state = (0, padded_features, padded_lens)\n",
    "\n",
    "def condition(i, padded_features, padded_lens):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, padded_features, padded_lens):\n",
    "    f = features.read(i)\n",
    "    len_f = tf.shape(f)[0]\n",
    "    f = tf.pad(f, [[0, maxlen - tf.shape(f)[0]], [0,0]])\n",
    "    return i + 1, padded_features.write(i, f), padded_lens.write(i, len_f)\n",
    "\n",
    "_, padded_features, padded_lens = tf.while_loop(condition, body, init_state)\n",
    "padded_features = padded_features.stack()\n",
    "padded_lens = padded_lens.stack()\n",
    "padded_lens.set_shape((None))\n",
    "padded_features.set_shape((None, None, 80))\n",
    "padded_features = tf.expand_dims(padded_features, -1)\n",
    "padded_features, padded_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mexican-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = tf.identity(padded_features, name = 'padded_features')\n",
    "padded_lens = tf.identity(padded_lens, name = 'padded_lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "rising-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = malaya_speech.config.conformer_small_encoder_config\n",
    "conformer_model = conformer.Model(**config)\n",
    "decoder_config = malaya_speech.config.conformer_small_decoder_config\n",
    "transducer_model = transducer.rnn.Model(\n",
    "    conformer_model, vocabulary_size = subwords.vocab_size, **decoder_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "proper-atmosphere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transducer_model.encoder.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "important-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
    "# z = tf.zeros((tf.shape(p)[0], 1),dtype=tf.int32)\n",
    "# c = tf.concat([z, p], axis = 1)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "black-developer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=365, shape=(1, 6), dtype=int32, numpy=array([[0, 2, 2, 2, 2, 2]], dtype=int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tf.constant([[2,2,2,2,2]])\n",
    "z = tf.zeros((tf.shape(p)[0], 1),dtype=tf.int32)\n",
    "c = tf.concat([z, p], axis = 1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stuffed-drama",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/model/transducer/layer.py:37: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=12535, shape=(1, 150, 6, 1019), dtype=float32, numpy=\n",
       "array([[[[-0.25897768, -0.14856242,  0.23322178, ..., -0.2817614 ,\n",
       "           0.4797243 ,  0.35735556],\n",
       "         [-0.22066203, -0.20651416, -0.08014343, ..., -0.31459433,\n",
       "           0.49390563,  0.32725453],\n",
       "         [-0.18967927, -0.25266835, -0.27281678, ..., -0.3442219 ,\n",
       "           0.5056441 ,  0.32741722],\n",
       "         [-0.17414731, -0.29365426, -0.38682497, ..., -0.36731827,\n",
       "           0.512721  ,  0.33246335],\n",
       "         [-0.17100766, -0.33064258, -0.45512354, ..., -0.3858618 ,\n",
       "           0.51783454,  0.33501387],\n",
       "         [-0.17579561, -0.36290574, -0.49706692, ..., -0.40034205,\n",
       "           0.5218854 ,  0.33453697]],\n",
       "\n",
       "        [[-0.20249595, -0.01759617,  0.22455043, ..., -0.15141545,\n",
       "           0.23480755,  0.24609727],\n",
       "         [-0.1759717 , -0.07384203, -0.12037007, ..., -0.1660912 ,\n",
       "           0.27415752,  0.21369547],\n",
       "         [-0.15666611, -0.13124442, -0.3230819 , ..., -0.18627921,\n",
       "           0.30935213,  0.21658869],\n",
       "         [-0.15187511, -0.18237191, -0.44052806, ..., -0.20686916,\n",
       "           0.33768037,  0.22935703],\n",
       "         [-0.15795864, -0.22658168, -0.51063406, ..., -0.22530164,\n",
       "           0.3602747 ,  0.24113545],\n",
       "         [-0.17034768, -0.26359734, -0.5541181 , ..., -0.24031505,\n",
       "           0.37791196,  0.249327  ]],\n",
       "\n",
       "        [[-0.14931314, -0.10317713,  0.2542071 , ..., -0.0876582 ,\n",
       "           0.30447134,  0.18127143],\n",
       "         [-0.12076992, -0.14962189, -0.10023632, ..., -0.10664266,\n",
       "           0.330478  ,  0.14797305],\n",
       "         [-0.09314165, -0.19953024, -0.31029466, ..., -0.12830994,\n",
       "           0.34896907,  0.14848211],\n",
       "         [-0.08022943, -0.24580291, -0.4315024 , ..., -0.14881109,\n",
       "           0.3631308 ,  0.15912925],\n",
       "         [-0.07960761, -0.28670433, -0.50295484, ..., -0.16669393,\n",
       "           0.3747815 ,  0.16895851],\n",
       "         [-0.08670772, -0.32135078, -0.5464606 , ..., -0.18110079,\n",
       "           0.38417464,  0.17527948]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.25587305, -0.13971782,  0.30883813, ..., -0.34848565,\n",
       "           0.5952505 ,  0.5004919 ],\n",
       "         [-0.23793805, -0.1941788 , -0.02906068, ..., -0.3379179 ,\n",
       "           0.5930244 ,  0.4806042 ],\n",
       "         [-0.21794656, -0.24624644, -0.23708841, ..., -0.3352687 ,\n",
       "           0.59346366,  0.48375857],\n",
       "         [-0.2070489 , -0.29436448, -0.36084747, ..., -0.339589  ,\n",
       "           0.59642327,  0.4899264 ],\n",
       "         [-0.20507306, -0.33785987, -0.43548247, ..., -0.3470624 ,\n",
       "           0.6009501 ,  0.49336416],\n",
       "         [-0.2093172 , -0.3756841 , -0.48149163, ..., -0.35459572,\n",
       "           0.6057125 ,  0.49355364]],\n",
       "\n",
       "        [[-0.29886645, -0.07509825,  0.32235843, ..., -0.3965735 ,\n",
       "           0.58488536,  0.5585254 ],\n",
       "         [-0.2765963 , -0.12970063, -0.01206411, ..., -0.38688874,\n",
       "           0.58369696,  0.5376311 ],\n",
       "         [-0.25009087, -0.18313989, -0.21782625, ..., -0.38214445,\n",
       "           0.5858263 ,  0.5384748 ],\n",
       "         [-0.23483875, -0.23267351, -0.34002525, ..., -0.38405803,\n",
       "           0.5907861 ,  0.5427971 ],\n",
       "         [-0.23054616, -0.27716047, -0.41352925, ..., -0.38947016,\n",
       "           0.59734774,  0.5451044 ],\n",
       "         [-0.23373397, -0.31556317, -0.45867783, ..., -0.39535135,\n",
       "           0.60406333,  0.54468954]],\n",
       "\n",
       "        [[-0.3356057 , -0.11217858,  0.2209219 , ..., -0.30324394,\n",
       "           0.41347623,  0.31167066],\n",
       "         [-0.30767953, -0.15947391, -0.11116147, ..., -0.28772932,\n",
       "           0.4104177 ,  0.28442612],\n",
       "         [-0.28094685, -0.21586975, -0.3115713 , ..., -0.2823253 ,\n",
       "           0.41910967,  0.29252955],\n",
       "         [-0.26827136, -0.26917523, -0.42832723, ..., -0.28628427,\n",
       "           0.431372  ,  0.30740803],\n",
       "         [-0.26758012, -0.31643254, -0.49732372, ..., -0.29461607,\n",
       "           0.44369248,  0.31920028],\n",
       "         [-0.2743874 , -0.35662848, -0.53910255, ..., -0.30352905,\n",
       "           0.4544304 ,  0.32622313]]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = transducer_model([padded_features, c], training = False)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "automated-compiler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py:1249: NameBasedSaverStatus.__init__ (from tensorflow.python.training.tracking.util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.NameBasedSaverStatus at 0x146bd0210>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transducer_model.load_weights('asr-small-conformer-output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "gorgeous-agreement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=21262, shape=(150, 144), dtype=float32, numpy=\n",
       "array([[-0.6777401 ,  0.26190603,  1.2471519 , ..., -0.47753942,\n",
       "         0.51624364,  0.7863159 ],\n",
       "       [-0.9035885 ,  0.37142202,  0.91828126, ..., -0.6102744 ,\n",
       "         0.40827867,  0.8482963 ],\n",
       "       [-0.48765135,  0.11068531,  0.03869477, ..., -0.44163948,\n",
       "         0.17970625,  0.11945242],\n",
       "       ...,\n",
       "       [ 0.8294179 , -0.59820116, -0.39091122, ..., -0.6904948 ,\n",
       "        -0.36062357, -0.8441791 ],\n",
       "       [ 0.50202835, -0.7099134 , -0.80913955, ..., -0.76475304,\n",
       "        -0.43825176, -0.37389588],\n",
       "       [ 0.3508923 , -1.1374176 , -0.7055942 , ..., -0.52722216,\n",
       "        -0.37121135, -0.26164132]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = transducer_model.encoder_inference(padded_features[0])\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "vulnerable-tribute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/model/transducer/rnn.py:392: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Hypothesis(index=<tf.Tensor: id=63137, shape=(), dtype=int32, numpy=870>, prediction=<tf.Tensor: id=63142, shape=(150,), dtype=int32, numpy=\n",
       "array([  0, 596, 206,   0,   0,   0, 795,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0, 514,   0,   0,   0,   0,   0, 795,   0,\n",
       "        30,   0,   0,   0,   0, 795,   0,   0,   0, 266,   0,   0,   0,\n",
       "         0,   0,   0,   0, 175,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        30,   0,   0, 795,   0,  17,   0,   0,   0, 421,   0,   0,   0,\n",
       "       795, 204,   0,   0,   0,   0,  27,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 405,   0,   0,\n",
       "         0,  54,   0,   0,  30,   0,   0,   0,   0, 795,   0,   0,   0,\n",
       "         0, 136,   0,   0,   0,   0,   0,   0,   0,   0, 870,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0], dtype=int32)>, states=<tf.Tensor: id=63138, shape=(1, 2, 1, 320), dtype=float32, numpy=\n",
       "array([[[[ 5.60698450e-01,  2.32693739e-02, -1.38049498e-01,\n",
       "           2.77668238e-01, -9.28736944e-03,  5.67813158e-01,\n",
       "          -1.20832194e-02,  9.41104665e-02,  6.25789985e-02,\n",
       "           1.61799323e-02, -1.25140637e-01,  8.92624483e-02,\n",
       "           5.69487214e-02,  2.42520757e-02, -8.97588059e-02,\n",
       "           1.71416745e-01,  3.55067730e-01, -6.61696121e-02,\n",
       "          -1.48847282e-01, -3.25950414e-01, -5.81640601e-01,\n",
       "          -3.48649174e-01,  4.06244516e-01, -9.19005200e-02,\n",
       "          -4.79568169e-02,  1.39089659e-01,  1.11615531e-01,\n",
       "          -7.76370615e-03, -3.28745604e-01, -4.95600253e-02,\n",
       "          -3.18764746e-02,  2.47987662e-03, -2.87413299e-02,\n",
       "          -1.26749382e-03,  5.12747606e-03, -7.91574940e-02,\n",
       "           3.54620367e-02, -1.59279287e-01, -1.30000383e-01,\n",
       "          -1.84685051e-01,  8.09972286e-02, -0.00000000e+00,\n",
       "          -3.99230272e-02,  5.17459214e-02, -3.50258946e-01,\n",
       "          -3.80047351e-01, -2.75959820e-02, -1.44913346e-01,\n",
       "           3.70448492e-02,  9.52316344e-01,  3.83078665e-01,\n",
       "          -6.29706532e-02,  3.25228304e-01,  2.85224229e-01,\n",
       "           1.32521197e-01,  1.11165754e-01,  1.58601135e-01,\n",
       "           1.04184203e-01,  3.02274942e-01,  4.18956071e-01,\n",
       "           4.71594036e-02,  3.88173312e-02, -2.28948280e-01,\n",
       "          -4.34954534e-04, -7.28165358e-03, -2.84159929e-02,\n",
       "           2.61230897e-02, -6.98262379e-02, -1.54413491e-01,\n",
       "           0.00000000e+00,  7.05048591e-02, -3.76328304e-02,\n",
       "           8.81545153e-03,  1.31286666e-01, -3.30560863e-01,\n",
       "           1.69387758e-01, -3.81281488e-02, -4.47381616e-01,\n",
       "          -1.06457248e-02,  1.97155312e-01,  1.47604316e-01,\n",
       "           1.59698948e-01, -2.39014715e-01,  4.84950989e-02,\n",
       "           0.00000000e+00, -1.22270666e-01,  9.99633148e-02,\n",
       "           1.19609468e-01,  1.30527228e-01, -1.15142567e-02,\n",
       "          -2.60987524e-02,  1.07105300e-02,  1.45410709e-02,\n",
       "          -3.86863112e-01,  5.74477911e-01,  9.54663102e-03,\n",
       "          -1.16428345e-01,  4.30750027e-02, -3.64597768e-01,\n",
       "           2.54886538e-01,  2.98134327e-01,  2.18634326e-02,\n",
       "          -1.80479199e-01,  2.21802548e-01, -3.86850424e-02,\n",
       "          -1.47479326e-02, -2.20150091e-02,  5.53529151e-02,\n",
       "          -1.53098451e-02,  3.78328025e-01, -4.41159084e-02,\n",
       "          -4.56619151e-02,  4.25933599e-01,  2.97686011e-01,\n",
       "          -2.28365526e-01,  3.45781803e-01, -1.92847960e-02,\n",
       "           5.49950562e-02,  3.20878059e-01,  2.76475679e-02,\n",
       "          -8.91766101e-02, -1.20579854e-01,  0.00000000e+00,\n",
       "          -6.59268256e-03, -6.46416917e-02,  3.80719490e-02,\n",
       "          -8.82566813e-03, -1.82490841e-01, -4.66998927e-02,\n",
       "          -1.01242080e-01,  1.12543195e-01,  1.24470647e-02,\n",
       "          -2.41014380e-02, -3.40784080e-02, -3.05641741e-01,\n",
       "          -6.97433650e-02, -1.47204503e-01,  2.33275279e-01,\n",
       "           5.25813699e-02, -9.31078047e-02,  8.37108642e-02,\n",
       "          -1.38476908e-01,  1.27359014e-02, -1.26001472e-02,\n",
       "          -7.64967203e-02, -3.55848260e-02,  6.90695122e-02,\n",
       "          -4.49241847e-01,  1.97930813e-01,  1.21676803e-01,\n",
       "           0.00000000e+00,  9.07697901e-03, -2.52215087e-01,\n",
       "          -1.03503935e-01, -4.41258341e-01, -1.93274021e-02,\n",
       "          -9.35879871e-02,  1.57910854e-01, -3.03989053e-02,\n",
       "           2.23390460e-01,  2.98455339e-02, -1.73201952e-02,\n",
       "          -3.25066112e-02, -9.72222723e-03,  1.37606069e-01,\n",
       "           7.08938539e-02,  2.16368258e-01, -2.57339209e-01,\n",
       "          -8.12847167e-02,  8.99750516e-02,  5.43860793e-02,\n",
       "          -1.45266771e-01,  3.55448797e-02, -1.60646178e-02,\n",
       "          -2.80377716e-01, -3.08095008e-01,  6.88932538e-02,\n",
       "           4.30454053e-02,  7.54690096e-02,  1.57685086e-01,\n",
       "           4.87075523e-02,  3.55253875e-01,  3.14330608e-02,\n",
       "           2.71876976e-02, -1.97296720e-02,  5.02502248e-02,\n",
       "           2.92052422e-03,  1.62226200e-01,  2.42035285e-01,\n",
       "           7.35615864e-02,  6.71305135e-02, -6.37195930e-02,\n",
       "           2.66025960e-01,  5.52162826e-01, -7.18435124e-02,\n",
       "           2.04323307e-01,  4.27011661e-02, -9.52793192e-03,\n",
       "           2.20601082e-01, -1.33051127e-01, -3.32162231e-02,\n",
       "           4.23027873e-01,  1.38892392e-02,  2.53780186e-02,\n",
       "           1.43826604e-01,  4.42563668e-02, -1.13448858e-01,\n",
       "           8.65180418e-03, -4.81704287e-02, -1.39117733e-01,\n",
       "           8.09576421e-04, -6.13587163e-02,  2.52100796e-01,\n",
       "           8.07788447e-02,  1.42995998e-01, -2.13003963e-01,\n",
       "           2.24554554e-01,  4.07764822e-01, -2.31140614e-01,\n",
       "           4.11002315e-04,  6.45676553e-02,  1.11496776e-01,\n",
       "           1.34241179e-01,  2.75950819e-01, -3.49900305e-01,\n",
       "           3.42460752e-01,  3.54797468e-02, -8.66627842e-02,\n",
       "          -5.54297715e-02,  3.16547871e-01, -2.32073385e-02,\n",
       "          -1.24530848e-02, -1.62761793e-01,  8.88445139e-01,\n",
       "           7.79547379e-04,  1.95031986e-01, -1.80716272e-02,\n",
       "           1.16859470e-02,  5.94129205e-01,  2.75305003e-01,\n",
       "          -7.57758915e-02,  0.00000000e+00, -4.07356769e-01,\n",
       "          -7.53440522e-03,  2.83309072e-01, -7.23107755e-02,\n",
       "           1.53017536e-01,  2.44965956e-01,  1.11829981e-01,\n",
       "          -2.15448499e-01, -1.73697010e-01,  1.77256614e-01,\n",
       "          -9.60940272e-02,  1.16375059e-01,  1.54218571e-02,\n",
       "           8.82470459e-02,  0.00000000e+00,  2.23221555e-01,\n",
       "           5.58853783e-02, -6.18938245e-02, -1.35796696e-01,\n",
       "          -1.28676966e-01, -2.51894712e-01,  4.76990819e-01,\n",
       "           6.96439818e-02,  4.25969511e-02, -5.98878553e-03,\n",
       "           1.80337336e-02,  1.38421403e-03, -5.44631202e-03,\n",
       "           8.14880151e-03,  5.38636595e-02, -7.59310275e-02,\n",
       "          -2.64502376e-01, -1.47103844e-02,  2.74058372e-01,\n",
       "          -2.02301279e-01,  2.16082662e-01,  0.00000000e+00,\n",
       "           2.19128071e-03, -5.03110923e-02, -2.74535902e-02,\n",
       "          -1.60043448e-01, -1.83049515e-01, -3.02167237e-01,\n",
       "           6.86152950e-02, -8.55383128e-02, -0.00000000e+00,\n",
       "          -9.55981836e-02,  4.46916938e-01,  2.37721384e-01,\n",
       "           7.70271361e-01, -1.52611926e-01,  1.23079285e-01,\n",
       "          -1.82748437e-02,  4.22291547e-01,  1.58356838e-02,\n",
       "          -2.10270643e-01, -4.48203236e-02,  4.10598554e-02,\n",
       "          -8.77235606e-02, -1.15069607e-02, -1.51741039e-02,\n",
       "           1.11550473e-01, -5.42211812e-03, -5.97595843e-03,\n",
       "           7.37400129e-02, -9.36825052e-02,  1.34525821e-01,\n",
       "          -9.23431590e-02,  3.35204422e-01,  4.32393588e-02,\n",
       "          -3.71118672e-02,  3.81819993e-01,  5.61994389e-02,\n",
       "           2.04013716e-02,  0.00000000e+00,  2.17942804e-01,\n",
       "           2.81293280e-02,  2.28252653e-02]],\n",
       "\n",
       "        [[ 6.33851290e-01,  1.23663679e-01, -2.12985843e-01,\n",
       "           6.89370275e-01, -1.08143948e-01,  8.62349212e-01,\n",
       "          -8.23743641e-02,  9.43898037e-02,  3.63299161e-01,\n",
       "           5.95062040e-02, -2.12947875e-01,  1.41670063e-01,\n",
       "           1.19055636e-01,  7.95968771e-02, -4.11120683e-01,\n",
       "           3.40564877e-01,  5.46922147e-01, -9.92762893e-02,\n",
       "          -5.35751283e-01, -4.08631057e-01, -7.26700604e-01,\n",
       "          -5.59869349e-01,  7.87211955e-01, -1.98807478e-01,\n",
       "          -4.79936376e-02,  3.56237292e-01,  2.46621341e-01,\n",
       "          -2.41971552e-01, -3.41421217e-01, -1.27794936e-01,\n",
       "          -3.65555435e-01,  1.86632518e-02, -5.66620529e-02,\n",
       "          -1.94157939e-03,  3.47411692e-01, -1.68727636e-01,\n",
       "           2.09990427e-01, -2.08784550e-01, -3.02598953e-01,\n",
       "          -3.71177554e-01,  2.27840140e-01, -8.39556009e-02,\n",
       "          -1.72613412e-01,  6.56818002e-02, -4.56733316e-01,\n",
       "          -4.32509422e-01, -1.21329628e-01, -2.74786234e-01,\n",
       "           1.93030864e-01,  1.85609245e+00,  5.44522166e-01,\n",
       "          -2.30668247e-01,  4.14625257e-01,  4.52055067e-01,\n",
       "           3.98467958e-01,  5.37468195e-01,  2.63205707e-01,\n",
       "           2.02357844e-01,  6.55587256e-01,  7.02264190e-01,\n",
       "           4.71944138e-02,  1.46107331e-01, -4.86680210e-01,\n",
       "          -1.54725614e-03, -1.62133984e-02, -5.89300618e-02,\n",
       "           1.32512212e-01, -3.81298006e-01, -3.15627217e-01,\n",
       "           2.20631421e-01,  4.99058992e-01, -5.51471114e-02,\n",
       "           2.78965235e-02,  3.69915992e-01, -4.40242082e-01,\n",
       "           3.61319602e-01, -9.97119918e-02, -6.12319589e-01,\n",
       "          -3.26549150e-02,  3.62121552e-01,  2.45491594e-01,\n",
       "           3.08522820e-01, -5.46631873e-01,  9.44578946e-02,\n",
       "           1.06151804e-01, -2.84701884e-01,  3.29358131e-01,\n",
       "           2.56089896e-01,  3.64982098e-01, -3.05842087e-02,\n",
       "          -9.03811231e-02,  5.31123988e-02,  5.14340848e-02,\n",
       "          -4.08105791e-01,  8.73767853e-01,  1.98672377e-02,\n",
       "          -3.36038262e-01,  1.08363762e-01, -5.72456002e-01,\n",
       "           3.55791181e-01,  4.67574745e-01,  5.92405736e-01,\n",
       "          -3.29101533e-01,  5.55127323e-01, -9.19669867e-02,\n",
       "          -5.42140119e-02, -2.20185705e-02,  1.72284052e-01,\n",
       "          -1.04107842e-01,  6.30756378e-01, -2.67646432e-01,\n",
       "          -1.36835128e-01,  5.49298227e-01,  6.40088320e-01,\n",
       "          -5.18159628e-01,  4.97147441e-01, -7.30403066e-02,\n",
       "           1.11975126e-01,  6.90977871e-01,  4.38789189e-01,\n",
       "          -1.73305511e-01, -1.21779501e-01,  4.50829536e-01,\n",
       "          -2.41448916e-02, -9.54820961e-02,  1.05439693e-01,\n",
       "          -1.89597026e-01, -1.84558153e-01, -1.12798765e-01,\n",
       "          -1.01590142e-01,  3.38862449e-01,  1.58888549e-02,\n",
       "          -6.03790954e-02, -5.60978949e-02, -5.10278881e-01,\n",
       "          -1.33808181e-01, -4.57963705e-01,  4.73757982e-01,\n",
       "           9.06915963e-02, -2.52157450e-01,  2.84645885e-01,\n",
       "          -3.54939818e-01,  2.38579884e-02, -1.57643743e-02,\n",
       "          -2.20099479e-01, -1.13702983e-01,  6.91796690e-02,\n",
       "          -4.83750015e-01,  3.40438694e-01,  4.40020978e-01,\n",
       "           4.05226499e-01,  3.64780992e-01, -3.44209760e-01,\n",
       "          -2.26957709e-01, -7.37131596e-01, -1.01203345e-01,\n",
       "          -3.15896332e-01,  2.32476979e-01, -4.42621149e-02,\n",
       "           5.64093173e-01,  2.38337696e-01, -1.42249301e-01,\n",
       "          -4.46453542e-02, -2.56939977e-02,  1.61635116e-01,\n",
       "           2.40421295e-01,  3.15946341e-01, -3.35197240e-01,\n",
       "          -1.94712773e-01,  1.27029523e-01,  1.64347261e-01,\n",
       "          -2.91782141e-01,  1.04881987e-01, -4.38013747e-02,\n",
       "          -4.06717241e-01, -4.09686923e-01,  4.19314474e-01,\n",
       "           2.13840872e-01,  2.05443725e-01,  5.32492638e-01,\n",
       "           1.96206301e-01,  4.48051542e-01,  2.33547702e-01,\n",
       "           4.20635603e-02, -1.97322350e-02,  4.47759122e-01,\n",
       "           7.52992555e-03,  2.76631027e-01,  3.92107934e-01,\n",
       "           1.80947185e-01,  8.25022310e-02, -9.48077887e-02,\n",
       "           5.37218809e-01,  8.61986876e-01, -9.90856588e-02,\n",
       "           4.96406317e-01,  4.80997145e-01, -3.10351551e-02,\n",
       "           5.07466555e-01, -2.39873141e-01, -3.33027333e-01,\n",
       "           7.15119839e-01,  3.06983858e-01,  6.31407201e-02,\n",
       "           3.36373359e-01,  1.50622457e-01, -1.53664172e-01,\n",
       "           2.68315319e-02, -1.14498891e-01, -3.24677557e-01,\n",
       "           3.86330575e-01, -2.25628972e-01,  3.59604478e-01,\n",
       "           3.37647259e-01,  2.69629598e-01, -4.04523849e-01,\n",
       "           6.06509149e-01,  4.92179513e-01, -3.77190739e-01,\n",
       "           5.63258901e-02,  3.87226790e-01,  1.76117167e-01,\n",
       "           5.04246652e-01,  4.20012593e-01, -3.65330189e-01,\n",
       "           6.33285642e-01,  5.53728528e-02, -1.60631135e-01,\n",
       "          -3.44643533e-01,  5.69837093e-01, -1.05571516e-01,\n",
       "          -1.45183563e-01, -4.14359301e-01,  3.31804371e+00,\n",
       "           2.71743536e-03,  2.44526595e-01, -5.82752824e-02,\n",
       "           1.09341621e-01,  7.62328088e-01,  5.82368016e-01,\n",
       "          -1.98224604e-01,  6.45190775e-01, -4.63302672e-01,\n",
       "          -2.09827125e-02,  3.15966994e-01, -1.20601311e-01,\n",
       "           3.35712522e-01,  6.76086068e-01,  1.19289815e-01,\n",
       "          -3.43686491e-01, -4.88326788e-01,  2.79046834e-01,\n",
       "          -1.59326747e-01,  2.22511649e-01,  9.51871872e-02,\n",
       "           3.57829094e-01,  3.82256627e-01,  4.61006790e-01,\n",
       "           2.24404559e-01, -2.35530123e-01, -5.35907269e-01,\n",
       "          -2.99700558e-01, -4.31880385e-01,  5.19991100e-01,\n",
       "           2.03557909e-01,  2.74401277e-01, -1.53238671e-02,\n",
       "           3.15228105e-02,  1.39147788e-03, -2.90097803e-01,\n",
       "           1.48399457e-01,  1.12455621e-01, -1.84636861e-01,\n",
       "          -4.03204381e-01, -2.88693309e-02,  3.94043446e-01,\n",
       "          -3.91967952e-01,  4.28352535e-01,  2.15857387e-01,\n",
       "           7.01646693e-03, -2.70453006e-01, -1.51847273e-01,\n",
       "          -4.40483123e-01, -2.04997450e-01, -6.26972139e-01,\n",
       "           2.31525868e-01, -3.80310327e-01, -2.19188407e-01,\n",
       "          -2.68952936e-01,  4.91578311e-01,  5.45370519e-01,\n",
       "           1.40577126e+00, -2.69102752e-01,  3.48921239e-01,\n",
       "          -6.96798787e-02,  7.70993829e-01,  9.03905258e-02,\n",
       "          -3.35634023e-01, -6.22015446e-02,  1.41418993e-01,\n",
       "          -1.36243433e-01, -7.97617733e-02, -4.61657085e-02,\n",
       "           4.36771929e-01, -1.51146147e-02, -8.84507895e-02,\n",
       "           2.16432944e-01, -1.76269874e-01,  1.86260670e-01,\n",
       "          -9.26069915e-02,  6.45115614e-01,  8.36302489e-02,\n",
       "          -1.03775054e-01,  4.41736400e-01,  9.44788009e-02,\n",
       "           9.89985764e-02,  9.59595218e-02,  5.78283846e-01,\n",
       "           5.03366329e-02,  9.96421427e-02]]]], dtype=float32)>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = transducer_model._perform_greedy(encoded, tf.shape(encoded)[0],\n",
    "                                tf.constant(0, dtype = tf.int32),\n",
    "                                transducer_model.predict_net.get_initial_state())\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "satellite-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = g.prediction\n",
    "minus_one = -1 * tf.ones_like(indices, dtype=tf.int32)\n",
    "blank_like = 0 * tf.ones_like(indices, dtype=tf.int32)\n",
    "indices = tf.where(indices == minus_one, blank_like, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "recovered-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = tf.cast(tf.shape(X[0])[0], dtype=tf.float32)\n",
    "total_time_reduction_factor = featurizer.frame_step\n",
    "stime = tf.range(0, num_samples, delta=total_time_reduction_factor, dtype=tf.float32)\n",
    "stime /= tf.cast(featurizer.sample_rate, dtype=tf.float32)\n",
    "etime = tf.range(total_time_reduction_factor, num_samples, delta=total_time_reduction_factor, dtype=tf.float32)\n",
    "etime /= tf.cast(featurizer.sample_rate, dtype=tf.float32)\n",
    "non_blank = tf.where(tf.not_equal(indices, 0))\n",
    "non_blank_transcript = tf.gather_nd(indices, non_blank)\n",
    "non_blank_stime = tf.gather_nd(tf.repeat(tf.expand_dims(stime, axis=-1), tf.shape(indices)[-1], axis=-1), non_blank)[:,0]\n",
    "non_blank_etime = tf.gather_nd(tf.repeat(tf.expand_dims(etime, axis=-1), tf.shape(indices)[-1], axis=-1), non_blank)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "about-korea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=110056, shape=(1, 598), dtype=int32, numpy=\n",
       "array([[  0, 596, 206,   0,   0,   0, 795,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0, 514,   0,   0,   0,   0,   0, 795,   0,\n",
       "         30,   0,   0,   0,   0, 795,   0,   0,   0, 266,   0,   0,   0,\n",
       "          0,   0,   0,   0, 175,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         30,   0,   0, 795,   0,  17,   0,   0,   0, 421,   0,   0,   0,\n",
       "        795, 204,   0,   0,   0,   0,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 405,   0,   0,\n",
       "          0,  54,   0,   0,  30,   0,   0,   0,   0, 795,   0,   0,   0,\n",
       "          0, 136,   0,   0,   0,   0,   0,   0,   0,   0, 870,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = transducer_model.greedy_decoder(padded_features, padded_lens, training = False)\n",
    "decoded = tf.identity(decoded, name = 'greedy_decoder')\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_width = tf.placeholder(tf.int32, None, name = 'beam_width')\n",
    "# decoded_beam = transducer_model.beam_decoder(padded_features, padded_lens, \n",
    "#                                              beam_width = beam_width, training = False)\n",
    "# decoded_beam = tf.identity(decoded_beam, name = 'beam_decoder')\n",
    "# decoded_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = transducer_model.encoder(padded_features, training = False)\n",
    "encoded = tf.identity(encoded, name = 'encoded')\n",
    "encoded_placeholder = tf.placeholder(tf.float32, [config['dmodel']], name = 'encoded_placeholder')\n",
    "predicted_placeholder = tf.placeholder(tf.int32, None, name = 'predicted_placeholder')\n",
    "t = transducer_model.predict_net.get_initial_state().shape\n",
    "states_placeholder = tf.placeholder(tf.float32, [int(i) for i in t], name = 'states_placeholder')\n",
    "\n",
    "ytu, new_states = transducer_model.decoder_inference(\n",
    "    encoded=encoded_placeholder,\n",
    "    predicted=predicted_placeholder,\n",
    "    states=states_placeholder,\n",
    "    training = True\n",
    ")\n",
    "\n",
    "ytu = tf.identity(ytu, name = 'ytu')\n",
    "new_states = tf.identity(new_states, name = 'new_states')\n",
    "ytu, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = transducer_model.predict_net.get_initial_state()\n",
    "initial_states = tf.identity(initial_states, name = 'initial_states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_list)\n",
    "saver.restore(sess, 'asr-small-conformer-output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-switzerland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-scott",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sess.run([non_blank_transcript, non_blank_stime, non_blank_etime], feed_dict = {X: padded, X_len: lens})\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "BeamHypothesis = collections.namedtuple(\n",
    "    'BeamHypothesis', ('score', 'prediction', 'states')\n",
    ")\n",
    "\n",
    "\n",
    "def transducer(\n",
    "    enc,\n",
    "    total,\n",
    "    initial_states,\n",
    "    encoded_placeholder,\n",
    "    predicted_placeholder,\n",
    "    states_placeholder,\n",
    "    ytu,\n",
    "    new_states,\n",
    "    sess,\n",
    "    beam_width = 10,\n",
    "    norm_score = True,\n",
    "):\n",
    "    kept_hyps = [\n",
    "        BeamHypothesis(score = 0.0, prediction = [0], states = initial_states)\n",
    "    ]\n",
    "    B = kept_hyps\n",
    "    for i in range(total):\n",
    "        A = B\n",
    "        B = []\n",
    "        while True:\n",
    "            y_hat = max(A, key = lambda x: x.score)\n",
    "            A.remove(y_hat)\n",
    "            ytu_, new_states_ = sess.run(\n",
    "                [ytu, new_states],\n",
    "                feed_dict = {\n",
    "                    encoded_placeholder: enc[i],\n",
    "                    predicted_placeholder: y_hat.prediction[-1],\n",
    "                    states_placeholder: y_hat.states,\n",
    "                },\n",
    "            )\n",
    "            for k in range(ytu_.shape[0]):\n",
    "                beam_hyp = BeamHypothesis(\n",
    "                    score = (y_hat.score + float(ytu_[k])),\n",
    "                    prediction = y_hat.prediction,\n",
    "                    states = y_hat.states,\n",
    "                )\n",
    "                if k == 0:\n",
    "                    B.append(beam_hyp)\n",
    "                else:\n",
    "                    beam_hyp = BeamHypothesis(\n",
    "                        score = beam_hyp.score,\n",
    "                        prediction = (beam_hyp.prediction + [int(k)]),\n",
    "                        states = new_states_,\n",
    "                    )\n",
    "                    A.append(beam_hyp)\n",
    "            if len(B) > beam_width:\n",
    "                break\n",
    "    if norm_score:\n",
    "        kept_hyps = sorted(\n",
    "            B, key = lambda x: x.score / len(x.prediction), reverse = True\n",
    "        )[:beam_width]\n",
    "    else:\n",
    "        kept_hyps = sorted(B, key = lambda x: x.score, reverse = True)[\n",
    "            :beam_width\n",
    "        ]\n",
    "    return kept_hyps[0].prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = sess.run(decoded, feed_dict = {X: padded, X_len: lens})\n",
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "encoded_, padded_lens_  = sess.run([encoded, padded_lens], feed_dict = {X: padded, X_len: lens})\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor\n",
    "s = sess.run(initial_states)\n",
    "\n",
    "for i in range(len(encoded_)):\n",
    "    r = transducer(\n",
    "        enc = encoded_[i],\n",
    "        total = padded_lens_[i],\n",
    "        initial_states = s,\n",
    "        encoded_placeholder = encoded_placeholder,\n",
    "        predicted_placeholder = predicted_placeholder,\n",
    "        states_placeholder = states_placeholder,\n",
    "        ytu = ytu,\n",
    "        new_states = new_states,\n",
    "        sess = sess,\n",
    "        beam_width = 1,\n",
    "    )\n",
    "\n",
    "    print(malaya_speech.subword.decode(subwords, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'gather' in n.op.lower()\n",
    "        or 'placeholder' in n.name\n",
    "        or 'encoded' in n.name\n",
    "        or 'decoder' in n.name\n",
    "        or 'ytu' in n.name\n",
    "        or 'new_states' in n.name\n",
    "        or 'padded_' in n.name\n",
    "        or 'initial_states' in n.name)\n",
    "        and 'adam' not in n.name\n",
    "        and 'global_step' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "        and 'ReadVariableOp' not in n.name\n",
    "        and 'Gather' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_graph('output', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "                \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('output/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = [\n",
    "    'X_placeholder',\n",
    "    'X_len_placeholder',\n",
    "    'encoded_placeholder',\n",
    "    'predicted_placeholder',\n",
    "    'states_placeholder',\n",
    "]\n",
    "output_nodes = [\n",
    "    'greedy_decoder',\n",
    "    'encoded',\n",
    "    'ytu',\n",
    "    'new_states',\n",
    "    'padded_features',\n",
    "    'padded_lens',\n",
    "    'initial_states'\n",
    "]\n",
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_, padded_lens_, s  = test_sess.run([outputs['encoded'], outputs['padded_lens'], outputs['initial_states']], \n",
    "                                        feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                     inputs['X_len_placeholder']: lens})\n",
    "\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "r = transducer(\n",
    "    enc = encoded_[i],\n",
    "    total = padded_lens_[i],\n",
    "    initial_states = s,\n",
    "    encoded_placeholder = inputs['encoded_placeholder'],\n",
    "    predicted_placeholder = inputs['predicted_placeholder'],\n",
    "    states_placeholder = inputs['states_placeholder'],\n",
    "    ytu = outputs['ytu'],\n",
    "    new_states = outputs['new_states'],\n",
    "    sess = test_sess,\n",
    "    beam_width = 1,\n",
    ")\n",
    "\n",
    "malaya_speech.subword.decode(subwords, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.graph_transforms import TransformGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ['add_default_attributes',\n",
    "             'remove_nodes(op=Identity, op=CheckNumerics, op=Dropout)',\n",
    "             'fold_batch_norms',\n",
    "             'fold_old_batch_norms',\n",
    "             'quantize_weights(fallback_min=-10, fallback_max=10)',\n",
    "             'strip_unused_nodes',\n",
    "             'sort_by_execution_order']\n",
    "\n",
    "pb = 'output/frozen_model.pb'\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.FastGFile(pb, 'rb') as f:\n",
    "    input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "transformed_graph_def = TransformGraph(input_graph_def, \n",
    "                                           input_nodes,\n",
    "                                           output_nodes, transforms)\n",
    "    \n",
    "with tf.gfile.GFile(f'{pb}.quantized', 'wb') as f:\n",
    "    f.write(transformed_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('output/frozen_model.pb.quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}\n",
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_, padded_lens_, s  = test_sess.run([outputs['encoded'], outputs['padded_lens'], outputs['initial_states']], \n",
    "                                        feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                     inputs['X_len_placeholder']: lens})\n",
    "\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "r = transducer(\n",
    "    enc = encoded_[i],\n",
    "    total = padded_lens_[i],\n",
    "    initial_states = s,\n",
    "    encoded_placeholder = inputs['encoded_placeholder'],\n",
    "    predicted_placeholder = inputs['predicted_placeholder'],\n",
    "    states_placeholder = inputs['states_placeholder'],\n",
    "    ytu = outputs['ytu'],\n",
    "    new_states = outputs['new_states'],\n",
    "    sess = test_sess,\n",
    "    beam_width = 1,\n",
    ")\n",
    "\n",
    "malaya_speech.subword.decode(subwords, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf output asr-small-conformer-output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
