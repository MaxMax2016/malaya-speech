{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gross-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "exterior-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "SOURCE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__name__)))\n",
    "sys.path.insert(0, SOURCE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "existing-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "intensive-lunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:38: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:39: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:40: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:42: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/optimizer/__init__.py:43: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import malaya_speech.train.model.conformer as conformer\n",
    "import malaya_speech.train.model.transducer as transducer\n",
    "import malaya_speech\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "disturbed-photograph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/v1/vocab/malaya-speech.tokenizer.subwords\n",
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/pretrained/asr-small-conformer-output.tar.gz\n",
    "# !tar -zxf asr-small-conformer-output.tar.gz\n",
    "# !ls asr-small-conformer-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proper-reaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords = malaya_speech.subword.load('transducer.tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sticky-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = malaya_speech.tf_featurization.STTFeaturizer(\n",
    "    normalize_per_feature = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fatty-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '../speech/record/savewav_2020-11-26_22-36-06_294832.wav',\n",
    "    '../speech/record/savewav_2020-11-26_22-40-56_929661.wav',\n",
    "    '../speech/record/675.wav',\n",
    "    '../speech/record/664.wav',\n",
    "    '../speech/example-speaker/husein-zolkepli.wav',\n",
    "    '../speech/example-speaker/mas-aisyah.wav',\n",
    "    '../speech/example-speaker/khalil-nooh.wav',\n",
    "    '../speech/example-speaker/shafiqah-idayu.wav',\n",
    "    '../speech/khutbah/wadi-annuar.wav',\n",
    "]\n",
    "\n",
    "ys = [malaya_speech.load(f)[0] for f in files[:1]]\n",
    "padded, lens = malaya_speech.padding.sequence_1d(ys, return_len = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "representative-standard",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.convert_to_tensor(padded.astype(np.float32))\n",
    "X_len = tf.convert_to_tensor(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "respiratory-thirty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.compat.v1.placeholder(tf.float32, [None, None], name = 'X_placeholder')\n",
    "# X_len = tf.compat.v1.placeholder(tf.int32, [None], name = 'X_len_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unique-jesus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=350, shape=(1, 598, 80, 1), dtype=float32, numpy=\n",
       " array([[[[-2.2034364 ],\n",
       "          [-1.577853  ],\n",
       "          [-1.4177448 ],\n",
       "          ...,\n",
       "          [-0.8329339 ],\n",
       "          [-0.57104784],\n",
       "          [-0.51345015]],\n",
       " \n",
       "         [[-1.2287856 ],\n",
       "          [-1.4864113 ],\n",
       "          [-1.384421  ],\n",
       "          ...,\n",
       "          [-0.9460609 ],\n",
       "          [-0.9094682 ],\n",
       "          [-0.7640816 ]],\n",
       " \n",
       "         [[-1.6237903 ],\n",
       "          [-2.1593304 ],\n",
       "          [-2.1062427 ],\n",
       "          ...,\n",
       "          [-0.8943915 ],\n",
       "          [-0.92150915],\n",
       "          [-0.9778747 ]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.2223    ],\n",
       "          [-1.4476264 ],\n",
       "          [-1.3430028 ],\n",
       "          ...,\n",
       "          [-0.64533424],\n",
       "          [-0.51785934],\n",
       "          [-0.97902083]],\n",
       " \n",
       "         [[-1.8572193 ],\n",
       "          [-2.1320732 ],\n",
       "          [-2.005242  ],\n",
       "          ...,\n",
       "          [-0.59267133],\n",
       "          [-0.7728883 ],\n",
       "          [-0.570435  ]],\n",
       " \n",
       "         [[-0.7082459 ],\n",
       "          [-1.2005471 ],\n",
       "          [-1.1544559 ],\n",
       "          ...,\n",
       "          [-0.984708  ],\n",
       "          [-0.802305  ],\n",
       "          [-0.43610337]]]], dtype=float32)>,\n",
       " <tf.Tensor: id=348, shape=(1,), dtype=int32, numpy=array([598], dtype=int32)>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = tf.shape(X)[0]\n",
    "features = tf.TensorArray(dtype = tf.float32, size = batch_size, dynamic_size = True, infer_shape = False)\n",
    "features_len = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "\n",
    "init_state = (0, features, features_len)\n",
    "\n",
    "def condition(i, features, features_len):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, features, features_len):\n",
    "    f = featurizer(X[i, :X_len[i]])\n",
    "    f_len = tf.shape(f)[0]\n",
    "    return i + 1, features.write(i, f), features_len.write(i, f_len)\n",
    "\n",
    "_, features, features_len = tf.while_loop(condition, body, init_state)\n",
    "features_len = features_len.stack()\n",
    "padded_features = tf.TensorArray(dtype = tf.float32, size = batch_size)\n",
    "padded_lens = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "maxlen = tf.reduce_max(features_len)\n",
    "\n",
    "init_state = (0, padded_features, padded_lens)\n",
    "\n",
    "def condition(i, padded_features, padded_lens):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, padded_features, padded_lens):\n",
    "    f = features.read(i)\n",
    "    len_f = tf.shape(f)[0]\n",
    "    f = tf.pad(f, [[0, maxlen - tf.shape(f)[0]], [0,0]])\n",
    "    return i + 1, padded_features.write(i, f), padded_lens.write(i, len_f)\n",
    "\n",
    "_, padded_features, padded_lens = tf.while_loop(condition, body, init_state)\n",
    "padded_features = padded_features.stack()\n",
    "padded_lens = padded_lens.stack()\n",
    "padded_lens.set_shape((None))\n",
    "padded_features.set_shape((None, None, 80))\n",
    "padded_features = tf.expand_dims(padded_features, -1)\n",
    "padded_features, padded_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "mexican-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = tf.identity(padded_features, name = 'padded_features')\n",
    "padded_lens = tf.identity(padded_lens, name = 'padded_lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "rising-pocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = malaya_speech.config.conformer_small_encoder_config\n",
    "conformer_model = conformer.Model(**config)\n",
    "decoder_config = malaya_speech.config.conformer_small_decoder_config\n",
    "transducer_model = transducer.rnn.Model(\n",
    "    conformer_model, vocabulary_size = subwords.vocab_size, **decoder_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "proper-atmosphere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transducer_model.encoder.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "important-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
    "# z = tf.zeros((tf.shape(p)[0], 1),dtype=tf.int32)\n",
    "# c = tf.concat([z, p], axis = 1)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "black-developer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=365, shape=(1, 6), dtype=int32, numpy=array([[0, 2, 2, 2, 2, 2]], dtype=int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tf.constant([[2,2,2,2,2]])\n",
    "z = tf.zeros((tf.shape(p)[0], 1),dtype=tf.int32)\n",
    "c = tf.concat([z, p], axis = 1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "united-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tf.constant([6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "stuffed-drama",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/model/transducer/layer.py:37: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=12614, shape=(1, 150, 6, 1019), dtype=float32, numpy=\n",
       "array([[[[-2.76941061e-01, -2.60642856e-01, -1.75765157e-03, ...,\n",
       "          -2.16737702e-01,  5.91613829e-01,  5.78533053e-01],\n",
       "         [-4.41472203e-01, -2.55001217e-01,  1.58214152e-01, ...,\n",
       "          -1.11223325e-01,  7.46190548e-01,  4.71162140e-01],\n",
       "         [-4.95740712e-01, -2.53666669e-01,  2.56271839e-01, ...,\n",
       "          -1.96872503e-02,  8.31345797e-01,  3.93989265e-01],\n",
       "         [-4.98325020e-01, -2.61838228e-01,  3.10256064e-01, ...,\n",
       "           4.21543568e-02,  8.76945257e-01,  3.44420612e-01],\n",
       "         [-4.79894221e-01, -2.74984956e-01,  3.38736534e-01, ...,\n",
       "           7.84203559e-02,  9.04845238e-01,  3.13910723e-01],\n",
       "         [-4.54447985e-01, -2.89010227e-01,  3.53366613e-01, ...,\n",
       "           9.74038392e-02,  9.24812436e-01,  2.95339197e-01]],\n",
       "\n",
       "        [[-1.83465496e-01, -3.93688977e-01, -1.25008374e-01, ...,\n",
       "          -1.82381213e-01,  5.76377749e-01,  5.06338775e-01],\n",
       "         [-3.22037280e-01, -4.11164492e-01,  1.99647248e-02, ...,\n",
       "          -9.03738588e-02,  7.32782841e-01,  4.02886420e-01],\n",
       "         [-3.60317349e-01, -4.24636871e-01,  1.17296159e-01, ...,\n",
       "          -1.76751316e-02,  8.19279015e-01,  3.35135341e-01],\n",
       "         [-3.53367388e-01, -4.39602733e-01,  1.77047402e-01, ...,\n",
       "           3.16882133e-02,  8.64844978e-01,  2.92802989e-01],\n",
       "         [-3.29249084e-01, -4.55750704e-01,  2.11723879e-01, ...,\n",
       "           6.14419132e-02,  8.91690552e-01,  2.66445726e-01],\n",
       "         [-3.00495297e-01, -4.71289873e-01,  2.31336117e-01, ...,\n",
       "           7.77101070e-02,  9.10059154e-01,  2.49846697e-01]],\n",
       "\n",
       "        [[-5.26076481e-02, -3.96300763e-01, -1.92217231e-02, ...,\n",
       "          -1.14636227e-01,  5.85096419e-01,  5.15195429e-01],\n",
       "         [-2.05160081e-01, -4.18116957e-01,  1.28172413e-01, ...,\n",
       "          -1.01476908e-04,  7.24491537e-01,  3.98161083e-01],\n",
       "         [-2.62141883e-01, -4.34417129e-01,  2.23448738e-01, ...,\n",
       "           9.23073441e-02,  8.04966927e-01,  3.13291341e-01],\n",
       "         [-2.70210445e-01, -4.50283825e-01,  2.79606044e-01, ...,\n",
       "           1.53249025e-01,  8.49509239e-01,  2.59421855e-01],\n",
       "         [-2.56534696e-01, -4.66226131e-01,  3.11043262e-01, ...,\n",
       "           1.88949212e-01,  8.76707256e-01,  2.26195663e-01],\n",
       "         [-2.34762281e-01, -4.81003612e-01,  3.28269273e-01, ...,\n",
       "           2.07919583e-01,  8.95700991e-01,  2.05724210e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-3.18316519e-01, -1.06716961e-01,  7.22030252e-02, ...,\n",
       "          -1.98367238e-01,  6.02421939e-01,  5.08343995e-01],\n",
       "         [-4.89295214e-01, -1.00948036e-01,  2.30956525e-01, ...,\n",
       "          -1.06969729e-01,  7.37321973e-01,  4.05545652e-01],\n",
       "         [-5.49847722e-01, -1.07045799e-01,  3.31832349e-01, ...,\n",
       "          -2.39836425e-02,  8.12556505e-01,  3.34327638e-01],\n",
       "         [-5.56576133e-01, -1.23329014e-01,  3.88070822e-01, ...,\n",
       "           3.37970853e-02,  8.51878762e-01,  2.86482871e-01],\n",
       "         [-5.40503621e-01, -1.43002361e-01,  4.18030411e-01, ...,\n",
       "           6.88949227e-02,  8.75177085e-01,  2.54980057e-01],\n",
       "         [-5.16164064e-01, -1.61730051e-01,  4.33562785e-01, ...,\n",
       "           8.82836655e-02,  8.91712070e-01,  2.34285265e-01]],\n",
       "\n",
       "        [[-3.07228059e-01, -1.57852918e-01,  5.84479719e-02, ...,\n",
       "          -1.80161640e-01,  5.86830854e-01,  5.54927349e-01],\n",
       "         [-4.83447373e-01, -1.44985765e-01,  2.26663113e-01, ...,\n",
       "          -8.96790028e-02,  7.15571880e-01,  4.46620882e-01],\n",
       "         [-5.45844555e-01, -1.47013962e-01,  3.32338482e-01, ...,\n",
       "          -8.67903233e-03,  7.89244235e-01,  3.72725815e-01],\n",
       "         [-5.52818894e-01, -1.61439806e-01,  3.89853001e-01, ...,\n",
       "           4.74446267e-02,  8.28632593e-01,  3.23777378e-01],\n",
       "         [-5.36534786e-01, -1.80204839e-01,  4.19634908e-01, ...,\n",
       "           8.13529193e-02,  8.52239788e-01,  2.91841209e-01],\n",
       "         [-5.11925817e-01, -1.98485404e-01,  4.34489846e-01, ...,\n",
       "           9.98918787e-02,  8.69024992e-01,  2.71001101e-01]],\n",
       "\n",
       "        [[-3.53382200e-01, -1.24863625e-01, -9.44480300e-04, ...,\n",
       "          -1.74397200e-01,  6.01606607e-01,  4.31067795e-01],\n",
       "         [-4.94026124e-01, -1.24391496e-01,  1.41563907e-01, ...,\n",
       "          -8.37048441e-02,  7.53140450e-01,  3.32818449e-01],\n",
       "         [-5.37302852e-01, -1.34365648e-01,  2.32959136e-01, ...,\n",
       "          -1.06726587e-02,  8.41071188e-01,  2.68529445e-01],\n",
       "         [-5.34657359e-01, -1.52941763e-01,  2.85525352e-01, ...,\n",
       "           3.76048535e-02,  8.88254344e-01,  2.25621432e-01],\n",
       "         [-5.13298035e-01, -1.73627049e-01,  3.14632118e-01, ...,\n",
       "           6.60698861e-02,  9.16517913e-01,  1.97082087e-01],\n",
       "         [-4.85912859e-01, -1.92612231e-01,  3.30507666e-01, ...,\n",
       "           8.11598897e-02,  9.36421394e-01,  1.78081989e-01]]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = transducer_model([padded_features, c, l], training = False)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "automated-compiler",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/tf-1.15/env/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py:1249: NameBasedSaverStatus.__init__ (from tensorflow.python.training.tracking.util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Restoring a name-based tf.train.Saver checkpoint using the object-based restore API. This mode uses global names to match variables, and so is somewhat fragile. It also adds new restore ops to the graph each time it is called when graph building. Prefer re-encoding training checkpoints in the object-based format: run save() on the object-based saver (the same one this message is coming from) and use that checkpoint in the future.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.NameBasedSaverStatus at 0x14fa83710>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transducer_model.load_weights('asr-small-conformer-output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "brave-portable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=62985, shape=(1, 598), dtype=int32, numpy=\n",
       "array([[  0, 596, 206,   0,   0,   0, 795,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0, 514,   0,   0,   0,   0,   0, 795,   0,\n",
       "         30,   0,   0,   0,   0, 795,   0,   0,   0, 266,   0,   0,   0,\n",
       "          0,   0,   0,   0, 175,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         30,   0,   0, 795,   0,  17,   0,   0,   0, 421,   0,   0,   0,\n",
       "        795, 204,   0,   0,   0,   0,  27,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 405,   0,   0,\n",
       "          0,  54,   0,   0,  30,   0,   0,   0,   0, 795,   0,   0,   0,\n",
       "          0, 136,   0,   0,   0,   0,   0,   0,   0,   0, 870,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = transducer_model.greedy_decoder(padded_features, padded_lens, training = False)\n",
    "decoded = tf.identity(decoded, name = 'greedy_decoder')\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "sticky-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = transducer_model.greedy_decoder_alignment(padded_features[:1], padded_lens[:1], training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "hawaiian-foundation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_index = np.where(r[0].numpy() > 0)[1][-1]\n",
    "last_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "patent-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = r[0].numpy()[0,:last_index]\n",
    "r = np.exp(r[1].numpy()[0,:last_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dimensional-hudson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 1019)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "explicit-separation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 140)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = tf.nn.softmax(r[:,l]).numpy()\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "intense-priority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax1 = fig.add_subplot(311)\n",
    "im = ax1.imshow(np.rot90(s), interpolation='none')\n",
    "fig.colorbar(mappable=im, shrink=0.65, orientation='horizontal', ax=ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-agreement",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded = transducer_model.encoder_inference(padded_features[0])\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-writer",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = transducer_model._perform_greedy(encoded, tf.shape(encoded)[0],\n",
    "                                tf.constant(0, dtype = tf.int32),\n",
    "                                transducer_model.predict_net.get_initial_state())\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = g.prediction\n",
    "minus_one = -1 * tf.ones_like(indices, dtype=tf.int32)\n",
    "blank_like = 0 * tf.ones_like(indices, dtype=tf.int32)\n",
    "indices = tf.where(indices == minus_one, blank_like, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-animal",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = tf.cast(tf.shape(X[0])[0], dtype=tf.float32)\n",
    "total_time_reduction_factor = featurizer.frame_step\n",
    "stime = tf.range(0, num_samples, delta=total_time_reduction_factor, dtype=tf.float32)\n",
    "stime /= tf.cast(featurizer.sample_rate, dtype=tf.float32)\n",
    "etime = tf.range(total_time_reduction_factor, num_samples, delta=total_time_reduction_factor, dtype=tf.float32)\n",
    "etime /= tf.cast(featurizer.sample_rate, dtype=tf.float32)\n",
    "non_blank = tf.where(tf.not_equal(indices, 0))\n",
    "non_blank_transcript = tf.gather_nd(indices, non_blank)\n",
    "non_blank_stime = tf.gather_nd(tf.repeat(tf.expand_dims(stime, axis=-1), tf.shape(indices)[-1], axis=-1), non_blank)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-polls",
   "metadata": {},
   "outputs": [],
   "source": [
    "malaya_speech.subword.decode(subwords, non_blank_transcript.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-repeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords._id_to_subword(596 - 1), subwords._id_to_subword(206 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-amendment",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_blank_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_blank_stime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-device",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_width = tf.placeholder(tf.int32, None, name = 'beam_width')\n",
    "# decoded_beam = transducer_model.beam_decoder(padded_features, padded_lens, \n",
    "#                                              beam_width = beam_width, training = False)\n",
    "# decoded_beam = tf.identity(decoded_beam, name = 'beam_decoder')\n",
    "# decoded_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-jewelry",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = transducer_model.encoder(padded_features, training = False)\n",
    "encoded = tf.identity(encoded, name = 'encoded')\n",
    "encoded_placeholder = tf.placeholder(tf.float32, [config['dmodel']], name = 'encoded_placeholder')\n",
    "predicted_placeholder = tf.placeholder(tf.int32, None, name = 'predicted_placeholder')\n",
    "t = transducer_model.predict_net.get_initial_state().shape\n",
    "states_placeholder = tf.placeholder(tf.float32, [int(i) for i in t], name = 'states_placeholder')\n",
    "\n",
    "ytu, new_states = transducer_model.decoder_inference(\n",
    "    encoded=encoded_placeholder,\n",
    "    predicted=predicted_placeholder,\n",
    "    states=states_placeholder,\n",
    "    training = True\n",
    ")\n",
    "\n",
    "ytu = tf.identity(ytu, name = 'ytu')\n",
    "new_states = tf.identity(new_states, name = 'new_states')\n",
    "ytu, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = transducer_model.predict_net.get_initial_state()\n",
    "initial_states = tf.identity(initial_states, name = 'initial_states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-headquarters",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-nashville",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_list)\n",
    "saver.restore(sess, 'asr-small-conformer-output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaningful-switzerland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-scott",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-pittsburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sess.run([non_blank_transcript, non_blank_stime, non_blank_etime], feed_dict = {X: padded, X_len: lens})\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-contrast",
   "metadata": {},
   "outputs": [],
   "source": [
    "r[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "BeamHypothesis = collections.namedtuple(\n",
    "    'BeamHypothesis', ('score', 'prediction', 'states')\n",
    ")\n",
    "\n",
    "\n",
    "def transducer(\n",
    "    enc,\n",
    "    total,\n",
    "    initial_states,\n",
    "    encoded_placeholder,\n",
    "    predicted_placeholder,\n",
    "    states_placeholder,\n",
    "    ytu,\n",
    "    new_states,\n",
    "    sess,\n",
    "    beam_width = 10,\n",
    "    norm_score = True,\n",
    "):\n",
    "    kept_hyps = [\n",
    "        BeamHypothesis(score = 0.0, prediction = [0], states = initial_states)\n",
    "    ]\n",
    "    B = kept_hyps\n",
    "    for i in range(total):\n",
    "        A = B\n",
    "        B = []\n",
    "        while True:\n",
    "            y_hat = max(A, key = lambda x: x.score)\n",
    "            A.remove(y_hat)\n",
    "            ytu_, new_states_ = sess.run(\n",
    "                [ytu, new_states],\n",
    "                feed_dict = {\n",
    "                    encoded_placeholder: enc[i],\n",
    "                    predicted_placeholder: y_hat.prediction[-1],\n",
    "                    states_placeholder: y_hat.states,\n",
    "                },\n",
    "            )\n",
    "            for k in range(ytu_.shape[0]):\n",
    "                beam_hyp = BeamHypothesis(\n",
    "                    score = (y_hat.score + float(ytu_[k])),\n",
    "                    prediction = y_hat.prediction,\n",
    "                    states = y_hat.states,\n",
    "                )\n",
    "                if k == 0:\n",
    "                    B.append(beam_hyp)\n",
    "                else:\n",
    "                    beam_hyp = BeamHypothesis(\n",
    "                        score = beam_hyp.score,\n",
    "                        prediction = (beam_hyp.prediction + [int(k)]),\n",
    "                        states = new_states_,\n",
    "                    )\n",
    "                    A.append(beam_hyp)\n",
    "            if len(B) > beam_width:\n",
    "                break\n",
    "    if norm_score:\n",
    "        kept_hyps = sorted(\n",
    "            B, key = lambda x: x.score / len(x.prediction), reverse = True\n",
    "        )[:beam_width]\n",
    "    else:\n",
    "        kept_hyps = sorted(B, key = lambda x: x.score, reverse = True)[\n",
    "            :beam_width\n",
    "        ]\n",
    "    return kept_hyps[0].prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = sess.run(decoded, feed_dict = {X: padded, X_len: lens})\n",
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "encoded_, padded_lens_  = sess.run([encoded, padded_lens], feed_dict = {X: padded, X_len: lens})\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor\n",
    "s = sess.run(initial_states)\n",
    "\n",
    "for i in range(len(encoded_)):\n",
    "    r = transducer(\n",
    "        enc = encoded_[i],\n",
    "        total = padded_lens_[i],\n",
    "        initial_states = s,\n",
    "        encoded_placeholder = encoded_placeholder,\n",
    "        predicted_placeholder = predicted_placeholder,\n",
    "        states_placeholder = states_placeholder,\n",
    "        ytu = ytu,\n",
    "        new_states = new_states,\n",
    "        sess = sess,\n",
    "        beam_width = 1,\n",
    "    )\n",
    "\n",
    "    print(malaya_speech.subword.decode(subwords, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crucial-airfare",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "internal-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'gather' in n.op.lower()\n",
    "        or 'placeholder' in n.name\n",
    "        or 'encoded' in n.name\n",
    "        or 'decoder' in n.name\n",
    "        or 'ytu' in n.name\n",
    "        or 'new_states' in n.name\n",
    "        or 'padded_' in n.name\n",
    "        or 'initial_states' in n.name)\n",
    "        and 'adam' not in n.name\n",
    "        and 'global_step' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "        and 'ReadVariableOp' not in n.name\n",
    "        and 'Gather' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-profit",
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_graph('output', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "physical-benefit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "                \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('output/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = [\n",
    "    'X_placeholder',\n",
    "    'X_len_placeholder',\n",
    "    'encoded_placeholder',\n",
    "    'predicted_placeholder',\n",
    "    'states_placeholder',\n",
    "]\n",
    "output_nodes = [\n",
    "    'greedy_decoder',\n",
    "    'encoded',\n",
    "    'ytu',\n",
    "    'new_states',\n",
    "    'padded_features',\n",
    "    'padded_lens',\n",
    "    'initial_states'\n",
    "]\n",
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-johns",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovely-peter",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_, padded_lens_, s  = test_sess.run([outputs['encoded'], outputs['padded_lens'], outputs['initial_states']], \n",
    "                                        feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                     inputs['X_len_placeholder']: lens})\n",
    "\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "r = transducer(\n",
    "    enc = encoded_[i],\n",
    "    total = padded_lens_[i],\n",
    "    initial_states = s,\n",
    "    encoded_placeholder = inputs['encoded_placeholder'],\n",
    "    predicted_placeholder = inputs['predicted_placeholder'],\n",
    "    states_placeholder = inputs['states_placeholder'],\n",
    "    ytu = outputs['ytu'],\n",
    "    new_states = outputs['new_states'],\n",
    "    sess = test_sess,\n",
    "    beam_width = 1,\n",
    ")\n",
    "\n",
    "malaya_speech.subword.decode(subwords, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.graph_transforms import TransformGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ['add_default_attributes',\n",
    "             'remove_nodes(op=Identity, op=CheckNumerics, op=Dropout)',\n",
    "             'fold_batch_norms',\n",
    "             'fold_old_batch_norms',\n",
    "             'quantize_weights(fallback_min=-10, fallback_max=10)',\n",
    "             'strip_unused_nodes',\n",
    "             'sort_by_execution_order']\n",
    "\n",
    "pb = 'output/frozen_model.pb'\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.FastGFile(pb, 'rb') as f:\n",
    "    input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "transformed_graph_def = TransformGraph(input_graph_def, \n",
    "                                           input_nodes,\n",
    "                                           output_nodes, transforms)\n",
    "    \n",
    "with tf.gfile.GFile(f'{pb}.quantized', 'wb') as f:\n",
    "    f.write(transformed_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effective-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('output/frozen_model.pb.quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}\n",
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organic-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exempt-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-portugal",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_, padded_lens_, s  = test_sess.run([outputs['encoded'], outputs['padded_lens'], outputs['initial_states']], \n",
    "                                        feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                     inputs['X_len_placeholder']: lens})\n",
    "\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numerical-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "r = transducer(\n",
    "    enc = encoded_[i],\n",
    "    total = padded_lens_[i],\n",
    "    initial_states = s,\n",
    "    encoded_placeholder = inputs['encoded_placeholder'],\n",
    "    predicted_placeholder = inputs['predicted_placeholder'],\n",
    "    states_placeholder = inputs['states_placeholder'],\n",
    "    ytu = outputs['ytu'],\n",
    "    new_states = outputs['new_states'],\n",
    "    sess = test_sess,\n",
    "    beam_width = 1,\n",
    ")\n",
    "\n",
    "malaya_speech.subword.decode(subwords, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-therapy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf output asr-small-conformer-output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
