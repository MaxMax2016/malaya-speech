{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "SOURCE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__name__)))\n",
    "sys.path.insert(0, SOURCE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal(shape = (1, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=7, shape=(1, 100), dtype=float32, numpy=\n",
       "array([[0.00555614, 0.0049182 , 0.00798381, 0.00317804, 0.00678856,\n",
       "        0.00440303, 0.00233651, 0.00913075, 0.00744909, 0.0065694 ,\n",
       "        0.00060206, 0.0032713 , 0.00224006, 0.01225925, 0.01937592,\n",
       "        0.01039682, 0.03665477, 0.01101448, 0.00852368, 0.00265814,\n",
       "        0.00803169, 0.04099366, 0.00597613, 0.00437264, 0.00456577,\n",
       "        0.01096538, 0.00251443, 0.00124698, 0.00405949, 0.00581776,\n",
       "        0.00214754, 0.00809384, 0.00405603, 0.01221813, 0.00117233,\n",
       "        0.01381907, 0.00274724, 0.00627036, 0.05921398, 0.02650197,\n",
       "        0.00263057, 0.00031556, 0.00638939, 0.08120701, 0.02762496,\n",
       "        0.00945178, 0.00731255, 0.00326247, 0.00458115, 0.00223708,\n",
       "        0.02181498, 0.00259805, 0.00953586, 0.00912552, 0.00496112,\n",
       "        0.00292532, 0.0014343 , 0.01012184, 0.00660645, 0.00320788,\n",
       "        0.00407627, 0.03801437, 0.00733076, 0.00358716, 0.00846206,\n",
       "        0.0056523 , 0.01097614, 0.0014338 , 0.02984708, 0.00507708,\n",
       "        0.03690327, 0.00667278, 0.00940059, 0.00810943, 0.00058811,\n",
       "        0.01558561, 0.00065362, 0.00561755, 0.01499887, 0.00304077,\n",
       "        0.00592542, 0.01569221, 0.01771424, 0.01100055, 0.00077553,\n",
       "        0.01893885, 0.00270895, 0.00026466, 0.02531794, 0.00892945,\n",
       "        0.00645516, 0.01070183, 0.00284637, 0.01047342, 0.00102517,\n",
       "        0.00490265, 0.02073836, 0.00521896, 0.00469397, 0.00221026]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.exp(tf.nn.log_softmax(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya_speech.train.model.conformer as conformer\n",
    "import malaya_speech.train.model.transducer as transducer\n",
    "import malaya_speech\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/v1/vocab/malaya-speech.tokenizer.subwords\n",
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/pretrained/asr-small-conformer-output.tar.gz\n",
    "# !tar -zxf asr-small-conformer-output.tar.gz\n",
    "# !ls asr-small-conformer-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords = malaya_speech.subword.load('transducer.tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = malaya_speech.tf_featurization.STTFeaturizer(\n",
    "    normalize_per_feature = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    '../speech/record/savewav_2020-11-26_22-36-06_294832.wav',\n",
    "    '../speech/record/savewav_2020-11-26_22-40-56_929661.wav',\n",
    "    '../speech/record/675.wav',\n",
    "    '../speech/record/664.wav',\n",
    "    '../speech/example-speaker/husein-zolkepli.wav',\n",
    "    '../speech/example-speaker/mas-aisyah.wav',\n",
    "    '../speech/example-speaker/khalil-nooh.wav',\n",
    "    '../speech/example-speaker/shafiqah-idayu.wav',\n",
    "    '../speech/khutbah/wadi-annuar.wav',\n",
    "]\n",
    "\n",
    "ys = [malaya_speech.load(f)[0] for f in files[:1]]\n",
    "padded, lens = malaya_speech.padding.sequence_1d(ys, return_len = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.convert_to_tensor(padded.astype(np.float32))\n",
    "X_len = tf.convert_to_tensor(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = tf.compat.v1.placeholder(tf.float32, [None, None], name = 'X_placeholder')\n",
    "# X_len = tf.compat.v1.placeholder(tf.int32, [None], name = 'X_len_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=358, shape=(1, 598, 80, 1), dtype=float32, numpy=\n",
       " array([[[[-2.2245662 ],\n",
       "          [-1.5762727 ],\n",
       "          [-1.4157659 ],\n",
       "          ...,\n",
       "          [-0.84864193],\n",
       "          [-0.5130596 ],\n",
       "          [-0.53617686]],\n",
       " \n",
       "         [[-1.2285241 ],\n",
       "          [-1.4856597 ],\n",
       "          [-1.3835925 ],\n",
       "          ...,\n",
       "          [-1.0460858 ],\n",
       "          [-0.9751702 ],\n",
       "          [-0.7758349 ]],\n",
       " \n",
       "         [[-1.6258806 ],\n",
       "          [-2.161505  ],\n",
       "          [-2.1084962 ],\n",
       "          ...,\n",
       "          [-0.9996183 ],\n",
       "          [-1.0397459 ],\n",
       "          [-0.78438497]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-1.2199712 ],\n",
       "          [-1.447085  ],\n",
       "          [-1.3426789 ],\n",
       "          ...,\n",
       "          [-0.62271357],\n",
       "          [-0.5265967 ],\n",
       "          [-0.93184185]],\n",
       " \n",
       "         [[-1.8636333 ],\n",
       "          [-2.130981  ],\n",
       "          [-2.0027494 ],\n",
       "          ...,\n",
       "          [-0.595965  ],\n",
       "          [-0.746562  ],\n",
       "          [-0.48989078]],\n",
       " \n",
       "         [[-0.7103715 ],\n",
       "          [-1.2011461 ],\n",
       "          [-1.1546397 ],\n",
       "          ...,\n",
       "          [-0.9812267 ],\n",
       "          [-0.8172686 ],\n",
       "          [-0.37819   ]]]], dtype=float32)>,\n",
       " <tf.Tensor: id=356, shape=(1,), dtype=int32, numpy=array([598], dtype=int32)>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = tf.shape(X)[0]\n",
    "features = tf.TensorArray(dtype = tf.float32, size = batch_size, dynamic_size = True, infer_shape = False)\n",
    "features_len = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "\n",
    "init_state = (0, features, features_len)\n",
    "\n",
    "def condition(i, features, features_len):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, features, features_len):\n",
    "    f = featurizer(X[i, :X_len[i]])\n",
    "    f_len = tf.shape(f)[0]\n",
    "    return i + 1, features.write(i, f), features_len.write(i, f_len)\n",
    "\n",
    "_, features, features_len = tf.while_loop(condition, body, init_state)\n",
    "features_len = features_len.stack()\n",
    "padded_features = tf.TensorArray(dtype = tf.float32, size = batch_size)\n",
    "padded_lens = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "maxlen = tf.reduce_max(features_len)\n",
    "\n",
    "init_state = (0, padded_features, padded_lens)\n",
    "\n",
    "def condition(i, padded_features, padded_lens):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, padded_features, padded_lens):\n",
    "    f = features.read(i)\n",
    "    len_f = tf.shape(f)[0]\n",
    "    f = tf.pad(f, [[0, maxlen - tf.shape(f)[0]], [0,0]])\n",
    "    return i + 1, padded_features.write(i, f), padded_lens.write(i, len_f)\n",
    "\n",
    "_, padded_features, padded_lens = tf.while_loop(condition, body, init_state)\n",
    "padded_features = padded_features.stack()\n",
    "padded_lens = padded_lens.stack()\n",
    "padded_lens.set_shape((None))\n",
    "padded_features.set_shape((None, None, 80))\n",
    "padded_features = tf.expand_dims(padded_features, -1)\n",
    "padded_features, padded_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = tf.identity(padded_features, name = 'padded_features')\n",
    "padded_lens = tf.identity(padded_lens, name = 'padded_lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = malaya_speech.config.conformer_small_encoder_config\n",
    "conformer_model = conformer.Model(**config)\n",
    "decoder_config = malaya_speech.config.conformer_small_decoder_config\n",
    "transducer_model = transducer.rnn.Model(\n",
    "    conformer_model, vocabulary_size = subwords.vocab_size, **decoder_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transducer_model.encoder.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
    "# z = tf.zeros((tf.shape(p)[0], 1),dtype=tf.int32)\n",
    "# c = tf.concat([z, p], axis = 1)\n",
    "# c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=373, shape=(1, 6), dtype=int32, numpy=array([[0, 2, 2, 2, 2, 2]], dtype=int32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tf.constant([[2,2,2,2,2]])\n",
    "z = tf.zeros((tf.shape(p)[0], 1),dtype=tf.int32)\n",
    "c = tf.concat([z, p], axis = 1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = tf.constant([6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/huseinzolkepli/Documents/malaya-speech/malaya_speech/train/model/transducer/layer.py:37: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=12622, shape=(1, 150, 6, 1019), dtype=float32, numpy=\n",
       "array([[[[-0.44402975,  0.43671367, -0.4971884 , ..., -0.15641461,\n",
       "           0.30380902,  0.00193681],\n",
       "         [-0.34389183,  0.622414  , -0.44988954, ..., -0.06882282,\n",
       "           0.17790644,  0.17786662],\n",
       "         [-0.28181666,  0.72993344, -0.41961455, ..., -0.0158148 ,\n",
       "           0.06247481,  0.28637314],\n",
       "         [-0.24432124,  0.79271376, -0.41217506, ...,  0.01499708,\n",
       "          -0.03208706,  0.3518416 ],\n",
       "         [-0.22049505,  0.8326632 , -0.41824165, ...,  0.03413409,\n",
       "          -0.10767457,  0.3935852 ],\n",
       "         [-0.20472051,  0.8599612 , -0.4311045 , ...,  0.04658622,\n",
       "          -0.16788161,  0.4217404 ]],\n",
       "\n",
       "        [[-0.3847059 ,  0.36242586, -0.5694346 , ..., -0.38109237,\n",
       "           0.26851162,  0.11176512],\n",
       "         [-0.26039642,  0.55903876, -0.5082668 , ..., -0.29177108,\n",
       "           0.138309  ,  0.30559492],\n",
       "         [-0.18268086,  0.6725719 , -0.46792224, ..., -0.23437233,\n",
       "           0.03200337,  0.4196936 ],\n",
       "         [-0.13820846,  0.7383902 , -0.4544611 , ..., -0.20166704,\n",
       "          -0.05300009,  0.48958504],\n",
       "         [-0.11157236,  0.7801007 , -0.45682764, ..., -0.18209071,\n",
       "          -0.12157419,  0.5350486 ],\n",
       "         [-0.09466353,  0.8084515 , -0.4673254 , ..., -0.16991614,\n",
       "          -0.17706722,  0.5662749 ]],\n",
       "\n",
       "        [[-0.32142505,  0.29881567, -0.56597716, ..., -0.4186595 ,\n",
       "           0.27913845,  0.13927552],\n",
       "         [-0.21472152,  0.48912296, -0.5047411 , ..., -0.33156562,\n",
       "           0.155173  ,  0.3252475 ],\n",
       "         [-0.15259947,  0.5987857 , -0.46076515, ..., -0.27102003,\n",
       "           0.05235182,  0.4341196 ],\n",
       "         [-0.11708429,  0.66201735, -0.4442984 , ..., -0.2352928 ,\n",
       "          -0.03159605,  0.5012862 ],\n",
       "         [-0.09511849,  0.7019445 , -0.4449314 , ..., -0.21410681,\n",
       "          -0.10040181,  0.5458094 ],\n",
       "         [-0.08064286,  0.7290094 , -0.4545232 , ..., -0.2014001 ,\n",
       "          -0.15670732,  0.5769916 ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.43578357,  0.39894304, -0.71690756, ..., -0.41221118,\n",
       "           0.3871457 ,  0.03934337],\n",
       "         [-0.3279309 ,  0.59171236, -0.6592681 , ..., -0.32782173,\n",
       "           0.24842745,  0.20997138],\n",
       "         [-0.24987328,  0.70308316, -0.6147091 , ..., -0.27514753,\n",
       "           0.12975186,  0.31802666],\n",
       "         [-0.20049843,  0.7679824 , -0.59572524, ..., -0.2440765 ,\n",
       "           0.03540038,  0.3854415 ],\n",
       "         [-0.16928825,  0.80926484, -0.59378034, ..., -0.22466433,\n",
       "          -0.03935669,  0.429317  ],\n",
       "         [-0.14909515,  0.83735365, -0.6014533 , ..., -0.21211323,\n",
       "          -0.09873732,  0.45934266]],\n",
       "\n",
       "        [[-0.39827192,  0.42018536, -0.6842237 , ..., -0.39243367,\n",
       "           0.42208767,  0.03583109],\n",
       "         [-0.28880614,  0.61025494, -0.6241676 , ..., -0.31384534,\n",
       "           0.28075814,  0.20509115],\n",
       "         [-0.21059304,  0.7196777 , -0.57947254, ..., -0.26437914,\n",
       "           0.16184369,  0.31152827],\n",
       "         [-0.16167177,  0.7837558 , -0.5608776 , ..., -0.2345728 ,\n",
       "           0.06759766,  0.37787843],\n",
       "         [-0.13097304,  0.82469535, -0.5595219 , ..., -0.21556354,\n",
       "          -0.00710996,  0.42119363],\n",
       "         [-0.11119137,  0.85263956, -0.5678153 , ..., -0.20303932,\n",
       "          -0.06647556,  0.4509681 ]],\n",
       "\n",
       "        [[-0.5045428 ,  0.38556117, -0.6758141 , ..., -0.43324345,\n",
       "           0.37674183,  0.15164445],\n",
       "         [-0.38163966,  0.58587056, -0.6220442 , ..., -0.34427258,\n",
       "           0.23013034,  0.33135927],\n",
       "         [-0.2970488 ,  0.70363474, -0.5854073 , ..., -0.28479463,\n",
       "           0.11432359,  0.44036987],\n",
       "         [-0.24456687,  0.77358884, -0.5724277 , ..., -0.24885932,\n",
       "           0.02443531,  0.50808567],\n",
       "         [-0.2114186 ,  0.81860065, -0.5741433 , ..., -0.22680213,\n",
       "          -0.04663001,  0.55233604],\n",
       "         [-0.18965648,  0.8493988 , -0.58379775, ..., -0.21307422,\n",
       "          -0.10316779,  0.5827875 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = transducer_model([padded_features, c, l], training = False)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Unsuccessful TensorSliceReader constructor: Failed to get matching files on asr-small-conformer-output/model.ckpt: Not found: asr-small-conformer-output; No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-70d6c73a3a43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtransducer_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'asr-small-conformer-output/model.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    180\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    181\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1337\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m         \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0merrors_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLossError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mNewCheckpointReader\u001b[0;34m(filepattern)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0mNewCheckpointReader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tf_api_names_v1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train.NewCheckpointReader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mthis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_pywrap_tensorflow_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_CheckpointReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Unsuccessful TensorSliceReader constructor: Failed to get matching files on asr-small-conformer-output/model.ckpt: Not found: asr-small-conformer-output; No such file or directory"
     ]
    }
   ],
   "source": [
    "transducer_model.load_weights('asr-small-conformer-output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: id=59430, shape=(1, 149), dtype=int32, numpy=\n",
       " array([[368, 368, 716, 716, 716, 139, 139, 139, 139, 139, 131, 139, 131,\n",
       "         270, 311, 716, 368, 716, 368, 368, 368, 365, 368, 368, 311, 716,\n",
       "         716, 716, 716, 139, 139, 139, 139, 131, 139, 139, 131, 139, 131,\n",
       "         808, 368, 368, 368, 368, 368, 433, 311, 716, 311, 716, 716, 139,\n",
       "         139, 426, 368, 368, 368, 368, 311, 716, 311, 716, 716, 139, 139,\n",
       "         139, 426, 368, 368, 368, 368, 433, 368, 311, 716, 716, 508, 716,\n",
       "         716, 139, 139, 139, 131, 270, 311, 716, 139, 139, 139, 368, 368,\n",
       "         368, 433, 368, 311, 508, 716, 716, 716, 368, 716, 368, 368, 365,\n",
       "         368, 368, 365, 368, 368, 365, 368, 368, 365, 368, 368, 433, 311,\n",
       "         716, 716, 139, 139, 139, 426, 368, 368, 368, 368, 433, 368, 311,\n",
       "         716, 368, 716, 311, 716,  67, 716,  67, 716, 546, 139, 139, 139,\n",
       "         139, 368, 368, 368, 368, 433]], dtype=int32)>,\n",
       " <tf.Tensor: id=59446, shape=(1, 149, 1019), dtype=float32, numpy=\n",
       " array([[[-7.444317 , -6.563574 , -7.4974756, ..., -7.156702 ,\n",
       "          -6.696478 , -6.9983506],\n",
       "         [-7.402649 , -6.470998 , -7.647146 , ..., -7.365833 ,\n",
       "          -6.5727725, -6.917459 ],\n",
       "         [-7.3668323, -6.41646  , -7.689161 , ..., -7.3635464,\n",
       "          -6.436531 , -6.911374 ],\n",
       "         ...,\n",
       "         [-7.574627 , -6.1708674, -7.8872266, ..., -7.484812 ,\n",
       "          -6.267441 , -6.836231 ],\n",
       "         [-7.507175 , -6.1402903, -7.882697 , ..., -7.4071126,\n",
       "          -6.2289567, -6.8981066],\n",
       "         [-7.449655 , -6.102081 , -7.825501 , ..., -7.330181 ,\n",
       "          -6.1939054, -6.926873 ]]], dtype=float32)>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = transducer_model.greedy_decoder(padded_features, padded_lens, training = False)\n",
    "# decoded = tf.identity(decoded, name = 'greedy_decoder')\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = transducer_model.greedy_decoder_alignment(padded_features[:1], padded_lens[:1], training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_index = np.where(r[0].numpy() > 0)[1][-1]\n",
    "last_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = r[0].numpy()[0,:last_index]\n",
    "r = np.exp(r[1].numpy()[0,:last_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 1019)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(140, 140)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = tf.nn.softmax(r[:,l]).numpy()\n",
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax1 = fig.add_subplot(311)\n",
    "im = ax1.imshow(np.rot90(s), interpolation='none')\n",
    "fig.colorbar(mappable=im, shrink=0.65, orientation='horizontal', ax=ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded = transducer_model.encoder_inference(padded_features[0])\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = transducer_model._perform_greedy(encoded, tf.shape(encoded)[0],\n",
    "                                tf.constant(0, dtype = tf.int32),\n",
    "                                transducer_model.predict_net.get_initial_state())\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = g.prediction\n",
    "minus_one = -1 * tf.ones_like(indices, dtype=tf.int32)\n",
    "blank_like = 0 * tf.ones_like(indices, dtype=tf.int32)\n",
    "indices = tf.where(indices == minus_one, blank_like, indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = tf.cast(tf.shape(X[0])[0], dtype=tf.float32)\n",
    "total_time_reduction_factor = featurizer.frame_step\n",
    "stime = tf.range(0, num_samples, delta=total_time_reduction_factor, dtype=tf.float32)\n",
    "stime /= tf.cast(featurizer.sample_rate, dtype=tf.float32)\n",
    "etime = tf.range(total_time_reduction_factor, num_samples, delta=total_time_reduction_factor, dtype=tf.float32)\n",
    "etime /= tf.cast(featurizer.sample_rate, dtype=tf.float32)\n",
    "non_blank = tf.where(tf.not_equal(indices, 0))\n",
    "non_blank_transcript = tf.gather_nd(indices, non_blank)\n",
    "non_blank_stime = tf.gather_nd(tf.repeat(tf.expand_dims(stime, axis=-1), tf.shape(indices)[-1], axis=-1), non_blank)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malaya_speech.subword.decode(subwords, non_blank_transcript.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords._id_to_subword(596 - 1), subwords._id_to_subword(206 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_blank_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_blank_stime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beam_width = tf.placeholder(tf.int32, None, name = 'beam_width')\n",
    "# decoded_beam = transducer_model.beam_decoder(padded_features, padded_lens, \n",
    "#                                              beam_width = beam_width, training = False)\n",
    "# decoded_beam = tf.identity(decoded_beam, name = 'beam_decoder')\n",
    "# decoded_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = transducer_model.encoder(padded_features, training = False)\n",
    "encoded = tf.identity(encoded, name = 'encoded')\n",
    "encoded_placeholder = tf.placeholder(tf.float32, [config['dmodel']], name = 'encoded_placeholder')\n",
    "predicted_placeholder = tf.placeholder(tf.int32, None, name = 'predicted_placeholder')\n",
    "t = transducer_model.predict_net.get_initial_state().shape\n",
    "states_placeholder = tf.placeholder(tf.float32, [int(i) for i in t], name = 'states_placeholder')\n",
    "\n",
    "ytu, new_states = transducer_model.decoder_inference(\n",
    "    encoded=encoded_placeholder,\n",
    "    predicted=predicted_placeholder,\n",
    "    states=states_placeholder,\n",
    "    training = True\n",
    ")\n",
    "\n",
    "ytu = tf.identity(ytu, name = 'ytu')\n",
    "new_states = tf.identity(new_states, name = 'new_states')\n",
    "ytu, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = transducer_model.predict_net.get_initial_state()\n",
    "initial_states = tf.identity(initial_states, name = 'initial_states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_list)\n",
    "saver.restore(sess, 'asr-small-conformer-output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = sess.run([non_blank_transcript, non_blank_stime, non_blank_etime], feed_dict = {X: padded, X_len: lens})\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "BeamHypothesis = collections.namedtuple(\n",
    "    'BeamHypothesis', ('score', 'prediction', 'states')\n",
    ")\n",
    "\n",
    "\n",
    "def transducer(\n",
    "    enc,\n",
    "    total,\n",
    "    initial_states,\n",
    "    encoded_placeholder,\n",
    "    predicted_placeholder,\n",
    "    states_placeholder,\n",
    "    ytu,\n",
    "    new_states,\n",
    "    sess,\n",
    "    beam_width = 10,\n",
    "    norm_score = True,\n",
    "):\n",
    "    kept_hyps = [\n",
    "        BeamHypothesis(score = 0.0, prediction = [0], states = initial_states)\n",
    "    ]\n",
    "    B = kept_hyps\n",
    "    for i in range(total):\n",
    "        A = B\n",
    "        B = []\n",
    "        while True:\n",
    "            y_hat = max(A, key = lambda x: x.score)\n",
    "            A.remove(y_hat)\n",
    "            ytu_, new_states_ = sess.run(\n",
    "                [ytu, new_states],\n",
    "                feed_dict = {\n",
    "                    encoded_placeholder: enc[i],\n",
    "                    predicted_placeholder: y_hat.prediction[-1],\n",
    "                    states_placeholder: y_hat.states,\n",
    "                },\n",
    "            )\n",
    "            for k in range(ytu_.shape[0]):\n",
    "                beam_hyp = BeamHypothesis(\n",
    "                    score = (y_hat.score + float(ytu_[k])),\n",
    "                    prediction = y_hat.prediction,\n",
    "                    states = y_hat.states,\n",
    "                )\n",
    "                if k == 0:\n",
    "                    B.append(beam_hyp)\n",
    "                else:\n",
    "                    beam_hyp = BeamHypothesis(\n",
    "                        score = beam_hyp.score,\n",
    "                        prediction = (beam_hyp.prediction + [int(k)]),\n",
    "                        states = new_states_,\n",
    "                    )\n",
    "                    A.append(beam_hyp)\n",
    "            if len(B) > beam_width:\n",
    "                break\n",
    "    if norm_score:\n",
    "        kept_hyps = sorted(\n",
    "            B, key = lambda x: x.score / len(x.prediction), reverse = True\n",
    "        )[:beam_width]\n",
    "    else:\n",
    "        kept_hyps = sorted(B, key = lambda x: x.score, reverse = True)[\n",
    "            :beam_width\n",
    "        ]\n",
    "    return kept_hyps[0].prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "r = sess.run(decoded, feed_dict = {X: padded, X_len: lens})\n",
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "encoded_, padded_lens_  = sess.run([encoded, padded_lens], feed_dict = {X: padded, X_len: lens})\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor\n",
    "s = sess.run(initial_states)\n",
    "\n",
    "for i in range(len(encoded_)):\n",
    "    r = transducer(\n",
    "        enc = encoded_[i],\n",
    "        total = padded_lens_[i],\n",
    "        initial_states = s,\n",
    "        encoded_placeholder = encoded_placeholder,\n",
    "        predicted_placeholder = predicted_placeholder,\n",
    "        states_placeholder = states_placeholder,\n",
    "        ytu = ytu,\n",
    "        new_states = new_states,\n",
    "        sess = sess,\n",
    "        beam_width = 1,\n",
    "    )\n",
    "\n",
    "    print(malaya_speech.subword.decode(subwords, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'output/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'gather' in n.op.lower()\n",
    "        or 'placeholder' in n.name\n",
    "        or 'encoded' in n.name\n",
    "        or 'decoder' in n.name\n",
    "        or 'ytu' in n.name\n",
    "        or 'new_states' in n.name\n",
    "        or 'padded_' in n.name\n",
    "        or 'initial_states' in n.name)\n",
    "        and 'adam' not in n.name\n",
    "        and 'global_step' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "        and 'ReadVariableOp' not in n.name\n",
    "        and 'Gather' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_graph('output', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "                \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('output/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = [\n",
    "    'X_placeholder',\n",
    "    'X_len_placeholder',\n",
    "    'encoded_placeholder',\n",
    "    'predicted_placeholder',\n",
    "    'states_placeholder',\n",
    "]\n",
    "output_nodes = [\n",
    "    'greedy_decoder',\n",
    "    'encoded',\n",
    "    'ytu',\n",
    "    'new_states',\n",
    "    'padded_features',\n",
    "    'padded_lens',\n",
    "    'initial_states'\n",
    "]\n",
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_, padded_lens_, s  = test_sess.run([outputs['encoded'], outputs['padded_lens'], outputs['initial_states']], \n",
    "                                        feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                     inputs['X_len_placeholder']: lens})\n",
    "\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "r = transducer(\n",
    "    enc = encoded_[i],\n",
    "    total = padded_lens_[i],\n",
    "    initial_states = s,\n",
    "    encoded_placeholder = inputs['encoded_placeholder'],\n",
    "    predicted_placeholder = inputs['predicted_placeholder'],\n",
    "    states_placeholder = inputs['states_placeholder'],\n",
    "    ytu = outputs['ytu'],\n",
    "    new_states = outputs['new_states'],\n",
    "    sess = test_sess,\n",
    "    beam_width = 1,\n",
    ")\n",
    "\n",
    "malaya_speech.subword.decode(subwords, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.graph_transforms import TransformGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ['add_default_attributes',\n",
    "             'remove_nodes(op=Identity, op=CheckNumerics, op=Dropout)',\n",
    "             'fold_batch_norms',\n",
    "             'fold_old_batch_norms',\n",
    "             'quantize_weights(fallback_min=-10, fallback_max=10)',\n",
    "             'strip_unused_nodes',\n",
    "             'sort_by_execution_order']\n",
    "\n",
    "pb = 'output/frozen_model.pb'\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.FastGFile(pb, 'rb') as f:\n",
    "    input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "transformed_graph_def = TransformGraph(input_graph_def, \n",
    "                                           input_nodes,\n",
    "                                           output_nodes, transforms)\n",
    "    \n",
    "with tf.gfile.GFile(f'{pb}.quantized', 'wb') as f:\n",
    "    f.write(transformed_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('output/frozen_model.pb.quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}\n",
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_, padded_lens_, s  = test_sess.run([outputs['encoded'], outputs['padded_lens'], outputs['initial_states']], \n",
    "                                        feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                     inputs['X_len_placeholder']: lens})\n",
    "\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "r = transducer(\n",
    "    enc = encoded_[i],\n",
    "    total = padded_lens_[i],\n",
    "    initial_states = s,\n",
    "    encoded_placeholder = inputs['encoded_placeholder'],\n",
    "    predicted_placeholder = inputs['predicted_placeholder'],\n",
    "    states_placeholder = inputs['states_placeholder'],\n",
    "    ytu = outputs['ytu'],\n",
    "    new_states = outputs['new_states'],\n",
    "    sess = test_sess,\n",
    "    beam_width = 1,\n",
    ")\n",
    "\n",
    "malaya_speech.subword.decode(subwords, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf output asr-small-conformer-output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
