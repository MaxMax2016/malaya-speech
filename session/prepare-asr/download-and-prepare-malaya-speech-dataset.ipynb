{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Malaya-Speech Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our model able to understand Bahasa and local english slang (Manglish)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "Run command below to download all data,\n",
    "\n",
    "```bash\n",
    "wget https://f000.backblazeb2.com/file/malay-dataset/speech/semisupervised-manglish.tar.gz\n",
    "tar -xf semisupervised-manglish.tar.gz\n",
    "\n",
    "wget https://cdn.commonvoice.mozilla.org/cv-corpus-5.1-2020-06-22/id.tar.gz\n",
    "tar -zxf id.tar.gz\n",
    "\n",
    "wget https://f000.backblazeb2.com/file/malay-dataset/speech/semisupervised-malay.tar.gz\n",
    "tar -xf semisupervised-malay.tar.gz\n",
    "\n",
    "wget https://f000.backblazeb2.com/file/malay-dataset/streaming.zip -O wikipedia-asr.zip\n",
    "unzip wikipedia-asr.zip\n",
    "\n",
    "wget https://f000.backblazeb2.com/file/malay-dataset/speech/iium/iium.json\n",
    "mkdir iium\n",
    "wget https://f000.backblazeb2.com/file/malay-dataset/speech/iium/streaming.zip -O iium-asr.zip\n",
    "unzip iium-asr.zip -d iium\n",
    "```\n",
    "\n",
    "Total samples length,\n",
    "\n",
    "1. Malay, ~93 hours, semisupervised.\n",
    "2. Manglish, ~107 hours, semisupervised.\n",
    "3. Wikipedia malay, ~3.4 hours, supervised.\n",
    "4. IIUM confession malay, ~2.4 hours, supervised.\n",
    "3. Indonesian, ~4 hours, supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install malaya-speech -U --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import malaya_speech\n",
    "import malaya_speech.train as train\n",
    "from glob import glob\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7490, 10)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cv-corpus-5.1-2020-06-22/id/validated.tsv', sep = '\\t')\n",
    "df = df[(df['sentence'].str.len() > 5) & (df['sentence'].str.count(' ') > 0)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7490"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_commonvoice = []\n",
    "for i in range(len(df)):\n",
    "    p = f\"cv-corpus-5.1-2020-06-22/id/clips/{df['path'].iloc[i]}\"\n",
    "    id_commonvoice.append((p, df['sentence'].iloc[i]))\n",
    "\n",
    "len(id_commonvoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57895, 65277)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "malay = glob('semisupervised-malay/output-wav/*.wav')\n",
    "manglish = glob('semisupervised-manglish/output-wav/*.wav')\n",
    "len(malay), len(manglish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"'\", \"-\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"*\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\", \"%\", \"$\", \"\\\"\", \"/\", \"&\", \":\", \"+\"]\n",
    "def preprocessing_text(string):\n",
    "        \n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c for c in string if c in vocabs])\n",
    "    return re.sub(r'[ ]+', ' ', string).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57895/57895 [00:01<00:00, 35213.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "malays = []\n",
    "for i in tqdm(malay):\n",
    "    try:\n",
    "        p = i.replace('output-wav','output-text')\n",
    "        with open(f'{p}.txt') as fopen:\n",
    "            text = fopen.read()\n",
    "        malays.append((i, text))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65277/65277 [00:01<00:00, 34909.82it/s]\n"
     ]
    }
   ],
   "source": [
    "manglishs = []\n",
    "for i in tqdm(manglish):\n",
    "    try:\n",
    "        p = i.replace('output-wav','output-text')\n",
    "        with open(f'{p}.txt') as fopen:\n",
    "            text = fopen.read()\n",
    "        manglishs.append((i, text))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2251/2251 [00:00<00:00, 606032.37it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1803"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('iium.json') as fopen:\n",
    "    iium = json.load(fopen)\n",
    "\n",
    "iiums = []\n",
    "wavs = glob('iium/streaming/*.wav')\n",
    "for i in tqdm(wavs):\n",
    "    try:\n",
    "        index = int(i.split('/')[-1].split('.wav')[0])\n",
    "        iiums.append((i, iium[index]))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "len(iiums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2887/2887 [00:00<00:00, 376159.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2887"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia = []\n",
    "wavs = glob('streaming/*wav')\n",
    "for i in tqdm(wavs):\n",
    "    text = os.path.split(i)[1].replace('.wav', '')\n",
    "    wikipedia.append((i, text))\n",
    "    \n",
    "len(wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = id_commonvoice + malays + manglishs + iiums + wikipedia\n",
    "audios, texts = zip(*audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def mp3_to_wav(file, sr = 16000):\n",
    "    audio = AudioSegment.from_file(file)\n",
    "    audio = audio.set_frame_rate(sr).set_channels(1)\n",
    "    sample = np.array(audio.get_array_of_samples())\n",
    "    return malaya_speech.astype.int_to_float(sample), sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = [preprocessing_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('malaya-speech-transcript.json', 'w') as fopen:\n",
    "    json.dump(cleaned_texts, fopen)\n",
    "    \n",
    "with open('malaya-speech-transcript.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(cleaned_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<EOS>',\n",
       " ' ',\n",
       " 'a',\n",
       " 'e',\n",
       " 'n',\n",
       " 'i',\n",
       " 't',\n",
       " 'u',\n",
       " 's',\n",
       " 'o',\n",
       " 'k',\n",
       " 'r',\n",
       " 'l',\n",
       " 'h',\n",
       " 'd',\n",
       " 'm',\n",
       " 'g',\n",
       " 'y',\n",
       " 'b',\n",
       " 'p',\n",
       " 'w',\n",
       " 'c',\n",
       " 'f',\n",
       " 'j',\n",
       " 'v',\n",
       " \"'\",\n",
       " '-',\n",
       " 'z',\n",
       " '0',\n",
       " '1',\n",
       " 'x',\n",
       " '2',\n",
       " 'q',\n",
       " '*',\n",
       " '5',\n",
       " '3',\n",
       " '4',\n",
       " '6',\n",
       " '9',\n",
       " '8',\n",
       " '7',\n",
       " '%',\n",
       " '\"',\n",
       " '$',\n",
       " '/',\n",
       " '&',\n",
       " ':',\n",
       " '+']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars = malaya_speech.char.generate_vocab(cleaned_texts)\n",
    "unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('malaya-speech-sst-vocab.json', 'w') as fopen:\n",
    "    json.dump(unique_chars, fopen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change into TFRecord\n",
    "\n",
    "This is not necessary step, we recommend to use yield iterator to train the model, but we also can save our data into TFRecord to speed up data pipelines. To do that, we need to create a yield iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(maxlen = 16):\n",
    "    for i in tqdm(range(len(audios))):\n",
    "        try:\n",
    "            if '.mp3' in audios[i]:\n",
    "                wav_data, sr = mp3_to_wav(audios[i])\n",
    "            else:\n",
    "                wav_data, sr = malaya_speech.load(audios[i])\n",
    "                \n",
    "            if (len(wav_data) / sr) > maxlen or len(cleaned_texts[i]) < 5:\n",
    "                print(f'skipped {audios[i]}')\n",
    "                continue\n",
    "\n",
    "            yield {\n",
    "                'waveforms': wav_data.tolist(),\n",
    "                'waveform_lens': [len(wav_data)],\n",
    "                'targets': malaya_speech.char.encode(cleaned_texts[i], add_eos = False,\n",
    "                                                     lookup = unique_chars),\n",
    "                'raw_transcript': [cleaned_texts[i]],\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "generator = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.system('rm bahasa-asr/data/*')\n",
    "DATA_DIR = os.path.expanduser('bahasa-asr/data')\n",
    "tf.gfile.MakeDirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define shards\n",
    "\n",
    "Like we defined below,\n",
    "\n",
    "```python\n",
    "shards = [{'split': 'train', 'shards': 999}, {'split': 'dev', 'shards': 1}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "shards = [{'split': 'train', 'shards': 999}, {'split': 'dev', 'shards': 1}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to TFRecord\n",
    "\n",
    "Just pass yield iterator to malaya_speech.train_prepare_dataset,\n",
    "\n",
    "```python\n",
    "def prepare_dataset(\n",
    "    generator,\n",
    "    data_dir: str,\n",
    "    shards: List[Dict],\n",
    "    prefix: str = 'dataset',\n",
    "    shuffle: bool = True,\n",
    "    already_shuffled: bool = False,\n",
    "):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.prepare_dataset(generator, DATA_DIR, shards, prefix = 'bahasa-asr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
