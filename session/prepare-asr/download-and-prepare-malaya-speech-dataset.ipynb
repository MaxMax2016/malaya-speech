{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Malaya-Speech Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our model able to understand Bahasa and local english slang (Manglish)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "Run command below to download all data,\n",
    "\n",
    "```bash\n",
    "wget https://cdn.commonvoice.mozilla.org/cv-corpus-5.1-2020-06-22/id.tar.gz\n",
    "tar -zxf id.tar.gz\n",
    "\n",
    "wget https://f000.backblazeb2.com/file/malay-dataset/speech/semisupervised-malay.tar.gz\n",
    "tar -xf semisupervised-malay.tar.gz\n",
    "\n",
    "https://f000.backblazeb2.com/file/malay-dataset/speech/semisupervised-malay-part2.tar.gz\n",
    "tar -xf semisupervised-malay-part2.tar.gz\n",
    "\n",
    "https://f000.backblazeb2.com/file/malay-dataset/speech/semisupervised-malay-part3.tar.gz\n",
    "tar -xf semisupervised-malay-part3.tar.gz\n",
    "\n",
    "wget https://f000.backblazeb2.com/file/malay-dataset/streaming.zip -O wikipedia-asr.zip\n",
    "unzip wikipedia-asr.zip\n",
    "```\n",
    "\n",
    "Total samples length,\n",
    "\n",
    "1. Malay, ~222 hours, semisupervised.\n",
    "2. Wikipedia malay, ~3.4 hours, supervised.\n",
    "3. Indonesian, ~4 hours, supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install malaya-speech -U --no-deps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya_speech/train/optimizer.py:34: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya_speech/train/optimizer.py:35: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya_speech/train/optimizer.py:36: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya_speech/train/optimizer.py:38: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/malaya_speech/train/optimizer.py:39: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import malaya_speech\n",
    "import malaya_speech.train as train\n",
    "from glob import glob\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7490, 10)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('cv-corpus-5.1-2020-06-22/id/validated.tsv', sep = '\\t')\n",
    "df = df[(df['sentence'].str.len() > 5) & (df['sentence'].str.count(' ') > 0)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7490"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_commonvoice = []\n",
    "for i in range(len(df)):\n",
    "    p = f\"cv-corpus-5.1-2020-06-22/id/clips/{df['path'].iloc[i]}\"\n",
    "    id_commonvoice.append((p, df['sentence'].iloc[i]))\n",
    "\n",
    "len(id_commonvoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136520"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "malay = glob('semisupervised-malay/output-wav/*.wav')\n",
    "malay.extend(glob('../youtube/malay/output-wav/*.wav'))\n",
    "malay.extend(glob('../youtube/malay2/output-wav/*.wav'))\n",
    "len(malay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"'\", \"-\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"*\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\", \"%\", \"$\", \"\\\"\", \"/\", \"&\", \":\", \"+\"]\n",
    "def preprocessing_text(string):\n",
    "        \n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c for c in string if c in vocabs])\n",
    "    return re.sub(r'[ ]+', ' ', string).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136520/136520 [00:11<00:00, 11678.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "malays = []\n",
    "for i in tqdm(malay):\n",
    "    try:\n",
    "        p = i.replace('output-wav','output-text')\n",
    "        with open(f'{p}.txt') as fopen:\n",
    "            text = fopen.read()\n",
    "        malays.append((i, text))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2887/2887 [00:00<00:00, 364672.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2887"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia = []\n",
    "wavs = glob('streaming/*wav')\n",
    "for i in tqdm(wavs):\n",
    "    text = os.path.split(i)[1].replace('.wav', '')\n",
    "    wikipedia.append((i, text))\n",
    "    \n",
    "len(wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = id_commonvoice + malays + wikipedia\n",
    "audios, texts = zip(*audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "import numpy as np\n",
    "\n",
    "def mp3_to_wav(file, sr = 16000):\n",
    "    audio = AudioSegment.from_file(file)\n",
    "    audio = audio.set_frame_rate(sr).set_channels(1)\n",
    "    sample = np.array(audio.get_array_of_samples())\n",
    "    return malaya_speech.astype.int_to_float(sample), sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = [preprocessing_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('malaya-speech-transcript.json', 'w') as fopen:\n",
    "    json.dump(cleaned_texts, fopen)\n",
    "    \n",
    "with open('malaya-speech-transcript.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(cleaned_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<PAD>',\n",
       " '<EOS>',\n",
       " 'a',\n",
       " ' ',\n",
       " 'n',\n",
       " 'i',\n",
       " 'e',\n",
       " 'k',\n",
       " 't',\n",
       " 'u',\n",
       " 'd',\n",
       " 'm',\n",
       " 'l',\n",
       " 's',\n",
       " 'r',\n",
       " 'g',\n",
       " 'b',\n",
       " 'p',\n",
       " 'h',\n",
       " 'o',\n",
       " 'y',\n",
       " 'j',\n",
       " 'c',\n",
       " 'w',\n",
       " '-',\n",
       " 'f',\n",
       " 'v',\n",
       " '0',\n",
       " 'z',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '5',\n",
       " 'q',\n",
       " '4',\n",
       " 'x',\n",
       " '6',\n",
       " '9',\n",
       " '7',\n",
       " \"'\",\n",
       " '8',\n",
       " '*',\n",
       " '\"',\n",
       " '$',\n",
       " '&',\n",
       " ':']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_chars = malaya_speech.char.generate_vocab(cleaned_texts)\n",
    "unique_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('malaya-speech-sst-vocab.json', 'w') as fopen:\n",
    "    json.dump(unique_chars, fopen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change into TFRecord\n",
    "\n",
    "This is not necessary step, we recommend to use yield iterator to train the model, but we also can save our data into TFRecord to speed up data pipelines. To do that, we need to create a yield iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(maxlen = 18):\n",
    "    for i in tqdm(range(len(audios))):\n",
    "        try:\n",
    "            if '.mp3' in audios[i]:\n",
    "                wav_data, sr = mp3_to_wav(audios[i])\n",
    "            else:\n",
    "                wav_data, sr = malaya_speech.load(audios[i])\n",
    "                \n",
    "            if (len(wav_data) / sr) > maxlen or len(cleaned_texts[i]) < 5:\n",
    "                print(f'skipped {audios[i]}')\n",
    "                continue\n",
    "\n",
    "            yield {\n",
    "                'waveforms': wav_data.tolist(),\n",
    "                'waveform_lens': [len(wav_data)],\n",
    "                'targets': malaya_speech.char.encode(cleaned_texts[i], add_eos = False,\n",
    "                                                     lookup = unique_chars),\n",
    "                'raw_transcript': [cleaned_texts[i]],\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "generator = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.system('rm bahasa-asr/data/*')\n",
    "DATA_DIR = os.path.expanduser('bahasa-asr/data')\n",
    "tf.gfile.MakeDirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define shards\n",
    "\n",
    "Like we defined below,\n",
    "\n",
    "```python\n",
    "shards = [{'split': 'train', 'shards': 999}, {'split': 'dev', 'shards': 1}]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "shards = [{'split': 'train', 'shards': 999}, {'split': 'dev', 'shards': 1}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save to TFRecord\n",
    "\n",
    "Just pass yield iterator to malaya_speech.train_prepare_dataset,\n",
    "\n",
    "```python\n",
    "def prepare_dataset(\n",
    "    generator,\n",
    "    data_dir: str,\n",
    "    shards: List[Dict],\n",
    "    prefix: str = 'dataset',\n",
    "    shuffle: bool = True,\n",
    "    already_shuffled: bool = False,\n",
    "):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.prepare_dataset(generator, DATA_DIR, shards, prefix = 'bahasa-asr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
