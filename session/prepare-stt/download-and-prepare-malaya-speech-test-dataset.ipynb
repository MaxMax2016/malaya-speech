{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Malaya-Speech Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set to convert into tfrecord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "\n",
    "Run command below to download all data,\n",
    "\n",
    "```bash\n",
    "wget https://f000.backblazeb2.com/file/malaya-speech-model/data/audio-iium.zip\n",
    "wget https://f000.backblazeb2.com/file/malaya-speech-model/collections/shuffled-iium.json\n",
    "unzip audio-iium.zip -d iium\n",
    "\n",
    "wget https://f000.backblazeb2.com/file/malaya-speech-model/data/audio-wattpad.zip\n",
    "wget https://f000.backblazeb2.com/file/malaya-speech-model/collections/transcript-wattpad.json\n",
    "unzip audio-wattpad.zip -d wattpad\n",
    "\n",
    "wget https://f000.backblazeb2.com/file/malaya-speech-model/data/testset-audiobook.tar.gz\n",
    "tar -zxf text-audiobook.tar.gz\n",
    "tar -xf testset-audiobook.tar.gz\n",
    "```\n",
    "\n",
    "Total samples length,\n",
    "\n",
    "1. Strong semisupervised malay audiobook, ~30 mins, strong semisupervised.\n",
    "2. iium, ~6 mins, supervised.\n",
    "3. wattpad, ~10 mins, supervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import malaya_speech\n",
    "import malaya_speech.train as train\n",
    "from glob import glob\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"'\", \"-\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"*\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\", \"%\", \"$\", \"\\\"\", \"/\", \"&\", \":\", \"+\"]\n",
    "def preprocessing_text(string):\n",
    "        \n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c for c in string if c in vocabs])\n",
    "    return re.sub(r'[ ]+', ' ', string).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 146/146 [00:00<00:00, 289262.34it/s]\n"
     ]
    }
   ],
   "source": [
    "wattpad = []\n",
    "wavs = glob('wattpad/audio-wattpad/*wav')\n",
    "\n",
    "with open('transcript-wattpad.json') as fopen:\n",
    "    transcript = json.load(fopen)\n",
    "    \n",
    "for i in tqdm(wavs):\n",
    "    index = i.split('/')[-1].replace('.wav','')\n",
    "    text = transcript[int(index)]\n",
    "    wattpad.append((i, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97/97 [00:00<00:00, 299372.69it/s]\n"
     ]
    }
   ],
   "source": [
    "iium = []\n",
    "wavs = glob('iium/audio-iium/*wav')\n",
    "\n",
    "with open('shuffled-iium.json') as fopen:\n",
    "    transcript = json.load(fopen)\n",
    "    \n",
    "for i in tqdm(wavs):\n",
    "    index = i.split('/')[-1].replace('.wav','')\n",
    "    text = transcript[int(index)]\n",
    "    iium.append((i, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [00:00<00:00, 27473.61it/s]\n"
     ]
    }
   ],
   "source": [
    "audiobook = []\n",
    "wavs = glob('test-set/*wav')\n",
    "for i in tqdm(wavs):\n",
    "    t = '/'.join(i.split('<>')[1:])\n",
    "    t = t.split('.wav')[0]\n",
    "    t = t.replace('output-wav', 'output-text')\n",
    "    with open(f'text-audiobook/{t}.wav.txt') as fopen:\n",
    "        text = fopen.read()\n",
    "    audiobook.append((i, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = wattpad + iium + audiobook\n",
    "audios, texts = zip(*audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts = [preprocessing_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('malaya-speech-sst-vocab.json') as fopen:\n",
    "    unique_chars = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(maxlen = 18, min_length_text = 0):\n",
    "    for i in tqdm(range(len(audios))):\n",
    "        try:\n",
    "            wav_data, sr = malaya_speech.load(audios[i])\n",
    "                \n",
    "            if (len(wav_data) / sr) > maxlen:\n",
    "                print(f'skipped audio too long {audios[i]}')\n",
    "                continue\n",
    "                \n",
    "            if len(cleaned_texts[i]) < min_length_text:\n",
    "                print(f'skipped text too short {audios[i]}')\n",
    "                continue    \n",
    "\n",
    "            yield {\n",
    "                'waveforms': wav_data.tolist(),\n",
    "                'waveform_lens': [len(wav_data)],\n",
    "                'targets': malaya_speech.char.encode(cleaned_texts[i], add_eos = False,\n",
    "                                                     lookup = unique_chars),\n",
    "                'raw_transcript': [cleaned_texts[i]],\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "generator = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "os.system('rm bahasa-asr-test/data/*')\n",
    "DATA_DIR = os.path.expanduser('bahasa-asr-test/data')\n",
    "tf.gfile.MakeDirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "shards = [{'split': 'dev', 'shards': 100}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/543 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Generating case 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 543/543 [00:12<00:00, 41.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Generated 543 Examples\n",
      "INFO:tensorflow:Shuffling data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Data shuffled.\n"
     ]
    }
   ],
   "source": [
    "train.prepare_dataset(generator, DATA_DIR, shards, prefix = 'bahasa-asr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
