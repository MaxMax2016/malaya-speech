{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/huseinzol05/language-model-bahasa-manglish-combined/resolve/main/model.klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/tf2/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2ForCTC,\n",
    "    Wav2Vec2Processor,\n",
    "    is_apex_available,\n",
    "    set_seed,\n",
    "    AutoModelForCTC,\n",
    "    TFWav2Vec2ForCTC,\n",
    "    TFWav2Vec2PreTrainedModel,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import json\n",
    "\n",
    "CTC_VOCAB = [''] + list(string.ascii_lowercase + string.digits) + [' ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {v: k for k, v in enumerate(CTC_VOCAB)}\n",
    "vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
    "del vocab_dict[\" \"]\n",
    "vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
    "vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
    "\n",
    "with open(\"ctc-vocab.json\", \"w\") as vocab_file:\n",
    "    json.dump(vocab_dict, vocab_file)\n",
    "\n",
    "tokenizer = Wav2Vec2CTCTokenizer(\n",
    "    \"ctc-vocab.json\",\n",
    "    unk_token=\"[UNK]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    word_delimiter_token=\"|\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(765, 3579, 614)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "malay = sorted(glob('malay-test/*.wav'), key = lambda x: int(x.split('/')[1].replace('.wav', '')))\n",
    "singlish = sorted(glob('singlish-test/*.wav'), key = lambda x: int(x.split('/')[1].replace('.wav', '')))\n",
    "mandarin = sorted(glob('mandarin-test/*.wav'), key = lambda x: int(x.split('/')[1].replace('.wav', '')))\n",
    "len(malay), len(singlish), len(mandarin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(765, 3579, 614)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('malay-test.json') as fopen:\n",
    "    malay_label = json.load(fopen)\n",
    "with open('singlish-test.json') as fopen:\n",
    "    singlish_label = json.load(fopen)\n",
    "with open('mandarin-test.json') as fopen:\n",
    "    mandarin_label = json.load(fopen)\n",
    "    \n",
    "len(malay_label), len(singlish_label), len(mandarin_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('singlish-test/3460.wav',\n",
       "  'the mother tongue language collections in libraries will also be enhanced'),\n",
       " ('singlish-test/44.wav',\n",
       "  'doing an ankle rotation with the tree because he can'),\n",
       " ('singlish-test/2807.wav',\n",
       "  'a massive field of trees designed in the shape of a giant q r code'),\n",
       " ('malay-test/247.wav',\n",
       "  'agar bisa segera keluar dari ruangan maut pak dadi mulai keluar kelas'),\n",
       " ('singlish-test/1700.wav', 'it smelt like petroleum and that was disturbing'),\n",
       " ('mandarin-test/289.wav', 'nan dao shi wo pu tong hua bu biao zhun'),\n",
       " ('mandarin-test/435.wav', 'jiao wo jin tian xia wu san dian qu ji chang'),\n",
       " ('singlish-test/857.wav',\n",
       "  'apart from new ideas singapore also needs a customise model to meets its social needs'),\n",
       " ('malay-test/154.wav', 'selepas lebih kurang tiga'),\n",
       " ('singlish-test/1370.wav', 'after all google search can only get you so far')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "audio = malay + singlish + mandarin\n",
    "labels = malay_label + singlish_label + mandarin_label\n",
    "audio, labels = shuffle(audio, labels)\n",
    "test_set = list(zip(audio, labels))\n",
    "test_set[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "def norm_audio(x):\n",
    "    return (x - x.mean()) / np.sqrt(x.var() + 1e-7)\n",
    "\n",
    "def sequence_1d(\n",
    "    seq, maxlen=None, padding: str = 'post', pad_int=0, return_len=False\n",
    "):\n",
    "    if padding not in ['post', 'pre']:\n",
    "        raise ValueError('padding only supported [`post`, `pre`]')\n",
    "\n",
    "    if not maxlen:\n",
    "        maxlen = max([len(s) for s in seq])\n",
    "\n",
    "    padded_seqs, length = [], []\n",
    "    for s in seq:\n",
    "        if isinstance(s, np.ndarray):\n",
    "            s = s.tolist()\n",
    "        if padding == 'post':\n",
    "            padded_seqs.append(s + [pad_int] * (maxlen - len(s)))\n",
    "        if padding == 'pre':\n",
    "            padded_seqs.append([pad_int] * (maxlen - len(s)) + s)\n",
    "        length.append(len(s))\n",
    "    if return_len:\n",
    "        return np.array(padded_seqs), length\n",
    "    return np.array(padded_seqs)\n",
    "\n",
    "def batching(audios):\n",
    "    audios = [sf.read(a)[0] for a in audios]\n",
    "    batch, lens = sequence_1d(audios,return_len=True)\n",
    "    attentions = [[1] * l for l in lens]\n",
    "    attentions = sequence_1d(attentions)\n",
    "    normed_input_values = []\n",
    "\n",
    "    for vector, length in zip(batch, attentions.sum(-1)):\n",
    "        normed_slice = (vector - vector[:length].mean()) / np.sqrt(vector[:length].var() + 1e-7)\n",
    "        if length < normed_slice.shape[0]:\n",
    "            normed_slice[length:] = 0.0\n",
    "\n",
    "        normed_input_values.append(normed_slice)\n",
    "\n",
    "    normed_input_values = np.array(normed_input_values)\n",
    "    return normed_input_values.astype(np.float32), attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCTC.from_pretrained(\n",
    "    './wav2vec2-mixed/checkpoint-60000',\n",
    "    ctc_loss_reduction=\"mean\",\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    vocab_size=len(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch_x = audio[:batch_size]\n",
    "normed_input_values, attentions = batching(batch_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_pt = model(torch.from_numpy(normed_input_values.astype(np.float32)), \n",
    "             attention_mask = torch.from_numpy(attentions))\n",
    "o_pt = o_pt.logits.detach().numpy()\n",
    "o_pt = log_softmax(o_pt, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['te madatown language collections in librarys will also be enhanced',\n",
       " 'doing an imped anchor rotation with the tree because he can',\n",
       " 'a massive field of trees designed in the shape of a giant q r cod',\n",
       " 'agra bisa segera keluar dari keroangan maut pak dadi melai keluar kelas']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_ids = np.argmax(o_pt, axis = -1)\n",
    "tokenizer.batch_decode(pred_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vocab = list(vocab_dict.keys())\n",
    "unique_vocab[-3] = ' ' \n",
    "unique_vocab[-2] = '?'\n",
    "unique_vocab[-1] = '_'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "from scipy.special import log_softmax\n",
    "import kenlm\n",
    "\n",
    "kenlm_model = kenlm.Model('model.klm')\n",
    "decoder = build_ctcdecoder(\n",
    "    unique_vocab,\n",
    "    kenlm_model,\n",
    "    alpha=0.2,\n",
    "    beta=1.0,\n",
    "    ctc_token_idx=tokenizer.pad_token_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 the madatown language collections in librarys will also be enhanced\n",
      "1 doing an impede anchor rotation with the tree because he can\n",
      "2 a massive field of trees designed in the shape of a giant q r cod\n",
      "3 agra bisa segera keluar dari ruangan maut pak dadi mulai keluar kelas\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(o_pt_)):\n",
    "    out = decoder.decode_beams(o_pt_[k], prune_history=True)\n",
    "    d_lm2, lm_state, timesteps, logit_score, lm_score = out[0]\n",
    "    print(k, d_lm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the mother tongue language collections in libraries will also be enhanced',\n",
       " 'doing an ankle rotation with the tree because he can',\n",
       " 'a massive field of trees designed in the shape of a giant q r code',\n",
       " 'agar bisa segera keluar dari ruangan maut pak dadi mulai keluar kelas']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cer(actual, hyp):\n",
    "    \"\"\"\n",
    "    Calculate CER using `python-Levenshtein`.\n",
    "    \"\"\"\n",
    "    import Levenshtein as Lev\n",
    "\n",
    "    actual = actual.replace(' ', '')\n",
    "    hyp = hyp.replace(' ', '')\n",
    "    return Lev.distance(actual, hyp) / len(actual)\n",
    "\n",
    "\n",
    "def calculate_wer(actual, hyp):\n",
    "    \"\"\"\n",
    "    Calculate WER using `python-Levenshtein`.\n",
    "    \"\"\"\n",
    "    import Levenshtein as Lev\n",
    "\n",
    "    b = set(actual.split() + hyp.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    w1 = [chr(word2char[w]) for w in actual.split()]\n",
    "    w2 = [chr(word2char[w]) for w in hyp.split()]\n",
    "\n",
    "    return Lev.distance(''.join(w1), ''.join(w2)) / len(actual.split())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 962/1240 [6:15:18<2:28:48, 32.12s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "wer, cer = [], []\n",
    "wer_lm, cer_lm = [], []\n",
    "\n",
    "for i in tqdm(range(0, len(audio), batch_size)):\n",
    "    batch_x = audio[i: i + batch_size]\n",
    "    batch_y = labels[i: i + batch_size]\n",
    "    normed_input_values, attentions = batching(batch_x)\n",
    "    o_pt = model(torch.from_numpy(normed_input_values.astype(np.float32)), \n",
    "             attention_mask = torch.from_numpy(attentions))\n",
    "    o_pt = o_pt.logits.detach().numpy()\n",
    "    o_pt = log_softmax(o_pt, axis = -1)\n",
    "    pred_ids = np.argmax(o_pt, axis = -1)\n",
    "    pred = tokenizer.batch_decode(pred_ids)\n",
    "    for k in range(len(o_pt)):\n",
    "        out = decoder.decode_beams(o_pt[k], prune_history=True)\n",
    "        d_lm2, lm_state, timesteps, logit_score, lm_score = out[0]\n",
    "        \n",
    "        wer.append(calculate_wer(batch_y[k], pred[k]))\n",
    "        cer.append(calculate_cer(batch_y[k], pred[k]))\n",
    "        \n",
    "        wer_lm.append(calculate_wer(batch_y[k], d_lm2))\n",
    "        cer_lm.append(calculate_cer(batch_y[k], d_lm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14151468058308714,\n",
       " 0.048555454439612775,\n",
       " 0.09809135311921899,\n",
       " 0.03977501945111893)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(wer), np.mean(cer), np.mean(wer_lm), np.mean(cer_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4958, 4958)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audio), len(wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_malay = [no for no, i in enumerate(audio) if 'malay-test/' in i]\n",
    "index_singlish = [no for no, i in enumerate(audio) if 'singlish-test/' in i]\n",
    "index_mandarin = [no for no, i in enumerate(audio) if 'mandarin-test/' in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.23714922876687583,\n",
       " 0.05372605571018908,\n",
       " 0.1294898148329521,\n",
       " 0.03508559320616622)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(wer)[index_malay]), np.mean(np.array(cer)[index_malay]), np.mean(np.array(wer_lm)[index_malay]), np.mean(np.array(cer_lm)[index_malay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.12941144843784677,\n",
       " 0.04883661835898531,\n",
       " 0.09411106530063956,\n",
       " 0.04119293317615638)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(wer)[index_singlish]), np.mean(np.array(cer)[index_singlish]), np.mean(np.array(wer_lm)[index_singlish]), np.mean(np.array(cer_lm)[index_singlish])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09291050873816364,\n",
       " 0.04047435404966954,\n",
       " 0.08217217867571727,\n",
       " 0.037352703254831865)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(wer)[index_mandarin]), np.mean(np.array(cer)[index_mandarin]), np.mean(np.array(wer_lm)[index_mandarin]), np.mean(np.array(cer_lm)[index_mandarin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
