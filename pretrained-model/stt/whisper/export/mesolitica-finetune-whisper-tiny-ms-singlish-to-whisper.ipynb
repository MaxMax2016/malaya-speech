{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bayartsogt-ya/whisper-multiple-hf-datasets/blob/main/src/multiple_datasets/hub_default_utils.py\n",
    "\n",
    "from copy import deepcopy\n",
    "import torch\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "\n",
    "WHISPER_MAPPING = {\n",
    "    \"layers\": \"blocks\",\n",
    "    \"fc1\": \"mlp.0\",\n",
    "    \"fc2\": \"mlp.2\",\n",
    "    \"final_layer_norm\": \"mlp_ln\",\n",
    "    \"layers\": \"blocks\",\n",
    "    \".self_attn.q_proj\": \".attn.query\",\n",
    "    \".self_attn.k_proj\": \".attn.key\",\n",
    "    \".self_attn.v_proj\": \".attn.value\",\n",
    "    \".self_attn_layer_norm\": \".attn_ln\",\n",
    "    \".self_attn.out_proj\": \".attn.out\",\n",
    "    \".encoder_attn.q_proj\": \".cross_attn.query\",\n",
    "    \".encoder_attn.k_proj\": \".cross_attn.key\",\n",
    "    \".encoder_attn.v_proj\": \".cross_attn.value\",\n",
    "    \".encoder_attn_layer_norm\": \".cross_attn_ln\",\n",
    "    \".encoder_attn.out_proj\": \".cross_attn.out\",\n",
    "    \"decoder.layer_norm.\": \"decoder.ln.\",\n",
    "    \"encoder.layer_norm.\": \"encoder.ln_post.\",\n",
    "    \"embed_tokens\": \"token_embedding\",\n",
    "    \"encoder.embed_positions.weight\": \"encoder.positional_embedding\",\n",
    "    \"decoder.embed_positions.weight\": \"decoder.positional_embedding\",\n",
    "    \"layer_norm\": \"ln_post\",\n",
    "}\n",
    "\n",
    "\n",
    "def rename_keys(s_dict):\n",
    "    keys = list(s_dict.keys())\n",
    "    for key in keys:\n",
    "        new_key = key\n",
    "        for k, v in WHISPER_MAPPING.items():\n",
    "            if k in key:\n",
    "                new_key = new_key.replace(k, v)\n",
    "\n",
    "        print(f\"{key} -> {new_key}\")\n",
    "\n",
    "        s_dict[new_key] = s_dict.pop(key)\n",
    "    return s_dict\n",
    "\n",
    "\n",
    "def convert_hf_whisper(hf_model_name_or_path: str, whisper_state_path: str):\n",
    "    transformer_model = WhisperForConditionalGeneration.from_pretrained(hf_model_name_or_path)\n",
    "    config = transformer_model.config\n",
    "\n",
    "    # first build dims\n",
    "    dims = {\n",
    "        'n_mels': config.num_mel_bins,\n",
    "        'n_vocab': config.vocab_size,\n",
    "        'n_audio_ctx': config.max_source_positions,\n",
    "        'n_audio_state': config.d_model,\n",
    "        'n_audio_head': config.encoder_attention_heads,\n",
    "        'n_audio_layer': config.encoder_layers,\n",
    "        'n_text_ctx': config.max_target_positions,\n",
    "        'n_text_state': config.d_model,\n",
    "        'n_text_head': config.decoder_attention_heads,\n",
    "        'n_text_layer': config.decoder_layers\n",
    "    }\n",
    "\n",
    "    state_dict = deepcopy(transformer_model.model.state_dict())\n",
    "    state_dict = rename_keys(state_dict)\n",
    "\n",
    "    torch.save({\"dims\": dims, \"model_state_dict\": state_dict}, whisper_state_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd0abfa7b3c64a65ac095fa307012496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.96k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc23c69324974d7c9fe4132c98347abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/151M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.conv1.weight -> encoder.conv1.weight\n",
      "encoder.conv1.bias -> encoder.conv1.bias\n",
      "encoder.conv2.weight -> encoder.conv2.weight\n",
      "encoder.conv2.bias -> encoder.conv2.bias\n",
      "encoder.embed_positions.weight -> encoder.positional_embedding\n",
      "encoder.layers.0.self_attn.k_proj.weight -> encoder.blocks.0.attn.key.weight\n",
      "encoder.layers.0.self_attn.v_proj.weight -> encoder.blocks.0.attn.value.weight\n",
      "encoder.layers.0.self_attn.v_proj.bias -> encoder.blocks.0.attn.value.bias\n",
      "encoder.layers.0.self_attn.q_proj.weight -> encoder.blocks.0.attn.query.weight\n",
      "encoder.layers.0.self_attn.q_proj.bias -> encoder.blocks.0.attn.query.bias\n",
      "encoder.layers.0.self_attn.out_proj.weight -> encoder.blocks.0.attn.out.weight\n",
      "encoder.layers.0.self_attn.out_proj.bias -> encoder.blocks.0.attn.out.bias\n",
      "encoder.layers.0.self_attn_layer_norm.weight -> encoder.blocks.0.attn_ln.weight\n",
      "encoder.layers.0.self_attn_layer_norm.bias -> encoder.blocks.0.attn_ln.bias\n",
      "encoder.layers.0.fc1.weight -> encoder.blocks.0.mlp.0.weight\n",
      "encoder.layers.0.fc1.bias -> encoder.blocks.0.mlp.0.bias\n",
      "encoder.layers.0.fc2.weight -> encoder.blocks.0.mlp.2.weight\n",
      "encoder.layers.0.fc2.bias -> encoder.blocks.0.mlp.2.bias\n",
      "encoder.layers.0.final_layer_norm.weight -> encoder.blocks.0.mlp_ln.weight\n",
      "encoder.layers.0.final_layer_norm.bias -> encoder.blocks.0.mlp_ln.bias\n",
      "encoder.layers.1.self_attn.k_proj.weight -> encoder.blocks.1.attn.key.weight\n",
      "encoder.layers.1.self_attn.v_proj.weight -> encoder.blocks.1.attn.value.weight\n",
      "encoder.layers.1.self_attn.v_proj.bias -> encoder.blocks.1.attn.value.bias\n",
      "encoder.layers.1.self_attn.q_proj.weight -> encoder.blocks.1.attn.query.weight\n",
      "encoder.layers.1.self_attn.q_proj.bias -> encoder.blocks.1.attn.query.bias\n",
      "encoder.layers.1.self_attn.out_proj.weight -> encoder.blocks.1.attn.out.weight\n",
      "encoder.layers.1.self_attn.out_proj.bias -> encoder.blocks.1.attn.out.bias\n",
      "encoder.layers.1.self_attn_layer_norm.weight -> encoder.blocks.1.attn_ln.weight\n",
      "encoder.layers.1.self_attn_layer_norm.bias -> encoder.blocks.1.attn_ln.bias\n",
      "encoder.layers.1.fc1.weight -> encoder.blocks.1.mlp.0.weight\n",
      "encoder.layers.1.fc1.bias -> encoder.blocks.1.mlp.0.bias\n",
      "encoder.layers.1.fc2.weight -> encoder.blocks.1.mlp.2.weight\n",
      "encoder.layers.1.fc2.bias -> encoder.blocks.1.mlp.2.bias\n",
      "encoder.layers.1.final_layer_norm.weight -> encoder.blocks.1.mlp_ln.weight\n",
      "encoder.layers.1.final_layer_norm.bias -> encoder.blocks.1.mlp_ln.bias\n",
      "encoder.layers.2.self_attn.k_proj.weight -> encoder.blocks.2.attn.key.weight\n",
      "encoder.layers.2.self_attn.v_proj.weight -> encoder.blocks.2.attn.value.weight\n",
      "encoder.layers.2.self_attn.v_proj.bias -> encoder.blocks.2.attn.value.bias\n",
      "encoder.layers.2.self_attn.q_proj.weight -> encoder.blocks.2.attn.query.weight\n",
      "encoder.layers.2.self_attn.q_proj.bias -> encoder.blocks.2.attn.query.bias\n",
      "encoder.layers.2.self_attn.out_proj.weight -> encoder.blocks.2.attn.out.weight\n",
      "encoder.layers.2.self_attn.out_proj.bias -> encoder.blocks.2.attn.out.bias\n",
      "encoder.layers.2.self_attn_layer_norm.weight -> encoder.blocks.2.attn_ln.weight\n",
      "encoder.layers.2.self_attn_layer_norm.bias -> encoder.blocks.2.attn_ln.bias\n",
      "encoder.layers.2.fc1.weight -> encoder.blocks.2.mlp.0.weight\n",
      "encoder.layers.2.fc1.bias -> encoder.blocks.2.mlp.0.bias\n",
      "encoder.layers.2.fc2.weight -> encoder.blocks.2.mlp.2.weight\n",
      "encoder.layers.2.fc2.bias -> encoder.blocks.2.mlp.2.bias\n",
      "encoder.layers.2.final_layer_norm.weight -> encoder.blocks.2.mlp_ln.weight\n",
      "encoder.layers.2.final_layer_norm.bias -> encoder.blocks.2.mlp_ln.bias\n",
      "encoder.layers.3.self_attn.k_proj.weight -> encoder.blocks.3.attn.key.weight\n",
      "encoder.layers.3.self_attn.v_proj.weight -> encoder.blocks.3.attn.value.weight\n",
      "encoder.layers.3.self_attn.v_proj.bias -> encoder.blocks.3.attn.value.bias\n",
      "encoder.layers.3.self_attn.q_proj.weight -> encoder.blocks.3.attn.query.weight\n",
      "encoder.layers.3.self_attn.q_proj.bias -> encoder.blocks.3.attn.query.bias\n",
      "encoder.layers.3.self_attn.out_proj.weight -> encoder.blocks.3.attn.out.weight\n",
      "encoder.layers.3.self_attn.out_proj.bias -> encoder.blocks.3.attn.out.bias\n",
      "encoder.layers.3.self_attn_layer_norm.weight -> encoder.blocks.3.attn_ln.weight\n",
      "encoder.layers.3.self_attn_layer_norm.bias -> encoder.blocks.3.attn_ln.bias\n",
      "encoder.layers.3.fc1.weight -> encoder.blocks.3.mlp.0.weight\n",
      "encoder.layers.3.fc1.bias -> encoder.blocks.3.mlp.0.bias\n",
      "encoder.layers.3.fc2.weight -> encoder.blocks.3.mlp.2.weight\n",
      "encoder.layers.3.fc2.bias -> encoder.blocks.3.mlp.2.bias\n",
      "encoder.layers.3.final_layer_norm.weight -> encoder.blocks.3.mlp_ln.weight\n",
      "encoder.layers.3.final_layer_norm.bias -> encoder.blocks.3.mlp_ln.bias\n",
      "encoder.layer_norm.weight -> encoder.ln_post.weight\n",
      "encoder.layer_norm.bias -> encoder.ln_post.bias\n",
      "decoder.embed_tokens.weight -> decoder.token_embedding.weight\n",
      "decoder.embed_positions.weight -> decoder.positional_embedding\n",
      "decoder.layers.0.self_attn.k_proj.weight -> decoder.blocks.0.attn.key.weight\n",
      "decoder.layers.0.self_attn.v_proj.weight -> decoder.blocks.0.attn.value.weight\n",
      "decoder.layers.0.self_attn.v_proj.bias -> decoder.blocks.0.attn.value.bias\n",
      "decoder.layers.0.self_attn.q_proj.weight -> decoder.blocks.0.attn.query.weight\n",
      "decoder.layers.0.self_attn.q_proj.bias -> decoder.blocks.0.attn.query.bias\n",
      "decoder.layers.0.self_attn.out_proj.weight -> decoder.blocks.0.attn.out.weight\n",
      "decoder.layers.0.self_attn.out_proj.bias -> decoder.blocks.0.attn.out.bias\n",
      "decoder.layers.0.self_attn_layer_norm.weight -> decoder.blocks.0.attn_ln.weight\n",
      "decoder.layers.0.self_attn_layer_norm.bias -> decoder.blocks.0.attn_ln.bias\n",
      "decoder.layers.0.encoder_attn.k_proj.weight -> decoder.blocks.0.cross_attn.key.weight\n",
      "decoder.layers.0.encoder_attn.v_proj.weight -> decoder.blocks.0.cross_attn.value.weight\n",
      "decoder.layers.0.encoder_attn.v_proj.bias -> decoder.blocks.0.cross_attn.value.bias\n",
      "decoder.layers.0.encoder_attn.q_proj.weight -> decoder.blocks.0.cross_attn.query.weight\n",
      "decoder.layers.0.encoder_attn.q_proj.bias -> decoder.blocks.0.cross_attn.query.bias\n",
      "decoder.layers.0.encoder_attn.out_proj.weight -> decoder.blocks.0.cross_attn.out.weight\n",
      "decoder.layers.0.encoder_attn.out_proj.bias -> decoder.blocks.0.cross_attn.out.bias\n",
      "decoder.layers.0.encoder_attn_layer_norm.weight -> decoder.blocks.0.cross_attn_ln.weight\n",
      "decoder.layers.0.encoder_attn_layer_norm.bias -> decoder.blocks.0.cross_attn_ln.bias\n",
      "decoder.layers.0.fc1.weight -> decoder.blocks.0.mlp.0.weight\n",
      "decoder.layers.0.fc1.bias -> decoder.blocks.0.mlp.0.bias\n",
      "decoder.layers.0.fc2.weight -> decoder.blocks.0.mlp.2.weight\n",
      "decoder.layers.0.fc2.bias -> decoder.blocks.0.mlp.2.bias\n",
      "decoder.layers.0.final_layer_norm.weight -> decoder.blocks.0.mlp_ln.weight\n",
      "decoder.layers.0.final_layer_norm.bias -> decoder.blocks.0.mlp_ln.bias\n",
      "decoder.layers.1.self_attn.k_proj.weight -> decoder.blocks.1.attn.key.weight\n",
      "decoder.layers.1.self_attn.v_proj.weight -> decoder.blocks.1.attn.value.weight\n",
      "decoder.layers.1.self_attn.v_proj.bias -> decoder.blocks.1.attn.value.bias\n",
      "decoder.layers.1.self_attn.q_proj.weight -> decoder.blocks.1.attn.query.weight\n",
      "decoder.layers.1.self_attn.q_proj.bias -> decoder.blocks.1.attn.query.bias\n",
      "decoder.layers.1.self_attn.out_proj.weight -> decoder.blocks.1.attn.out.weight\n",
      "decoder.layers.1.self_attn.out_proj.bias -> decoder.blocks.1.attn.out.bias\n",
      "decoder.layers.1.self_attn_layer_norm.weight -> decoder.blocks.1.attn_ln.weight\n",
      "decoder.layers.1.self_attn_layer_norm.bias -> decoder.blocks.1.attn_ln.bias\n",
      "decoder.layers.1.encoder_attn.k_proj.weight -> decoder.blocks.1.cross_attn.key.weight\n",
      "decoder.layers.1.encoder_attn.v_proj.weight -> decoder.blocks.1.cross_attn.value.weight\n",
      "decoder.layers.1.encoder_attn.v_proj.bias -> decoder.blocks.1.cross_attn.value.bias\n",
      "decoder.layers.1.encoder_attn.q_proj.weight -> decoder.blocks.1.cross_attn.query.weight\n",
      "decoder.layers.1.encoder_attn.q_proj.bias -> decoder.blocks.1.cross_attn.query.bias\n",
      "decoder.layers.1.encoder_attn.out_proj.weight -> decoder.blocks.1.cross_attn.out.weight\n",
      "decoder.layers.1.encoder_attn.out_proj.bias -> decoder.blocks.1.cross_attn.out.bias\n",
      "decoder.layers.1.encoder_attn_layer_norm.weight -> decoder.blocks.1.cross_attn_ln.weight\n",
      "decoder.layers.1.encoder_attn_layer_norm.bias -> decoder.blocks.1.cross_attn_ln.bias\n",
      "decoder.layers.1.fc1.weight -> decoder.blocks.1.mlp.0.weight\n",
      "decoder.layers.1.fc1.bias -> decoder.blocks.1.mlp.0.bias\n",
      "decoder.layers.1.fc2.weight -> decoder.blocks.1.mlp.2.weight\n",
      "decoder.layers.1.fc2.bias -> decoder.blocks.1.mlp.2.bias\n",
      "decoder.layers.1.final_layer_norm.weight -> decoder.blocks.1.mlp_ln.weight\n",
      "decoder.layers.1.final_layer_norm.bias -> decoder.blocks.1.mlp_ln.bias\n",
      "decoder.layers.2.self_attn.k_proj.weight -> decoder.blocks.2.attn.key.weight\n",
      "decoder.layers.2.self_attn.v_proj.weight -> decoder.blocks.2.attn.value.weight\n",
      "decoder.layers.2.self_attn.v_proj.bias -> decoder.blocks.2.attn.value.bias\n",
      "decoder.layers.2.self_attn.q_proj.weight -> decoder.blocks.2.attn.query.weight\n",
      "decoder.layers.2.self_attn.q_proj.bias -> decoder.blocks.2.attn.query.bias\n",
      "decoder.layers.2.self_attn.out_proj.weight -> decoder.blocks.2.attn.out.weight\n",
      "decoder.layers.2.self_attn.out_proj.bias -> decoder.blocks.2.attn.out.bias\n",
      "decoder.layers.2.self_attn_layer_norm.weight -> decoder.blocks.2.attn_ln.weight\n",
      "decoder.layers.2.self_attn_layer_norm.bias -> decoder.blocks.2.attn_ln.bias\n",
      "decoder.layers.2.encoder_attn.k_proj.weight -> decoder.blocks.2.cross_attn.key.weight\n",
      "decoder.layers.2.encoder_attn.v_proj.weight -> decoder.blocks.2.cross_attn.value.weight\n",
      "decoder.layers.2.encoder_attn.v_proj.bias -> decoder.blocks.2.cross_attn.value.bias\n",
      "decoder.layers.2.encoder_attn.q_proj.weight -> decoder.blocks.2.cross_attn.query.weight\n",
      "decoder.layers.2.encoder_attn.q_proj.bias -> decoder.blocks.2.cross_attn.query.bias\n",
      "decoder.layers.2.encoder_attn.out_proj.weight -> decoder.blocks.2.cross_attn.out.weight\n",
      "decoder.layers.2.encoder_attn.out_proj.bias -> decoder.blocks.2.cross_attn.out.bias\n",
      "decoder.layers.2.encoder_attn_layer_norm.weight -> decoder.blocks.2.cross_attn_ln.weight\n",
      "decoder.layers.2.encoder_attn_layer_norm.bias -> decoder.blocks.2.cross_attn_ln.bias\n",
      "decoder.layers.2.fc1.weight -> decoder.blocks.2.mlp.0.weight\n",
      "decoder.layers.2.fc1.bias -> decoder.blocks.2.mlp.0.bias\n",
      "decoder.layers.2.fc2.weight -> decoder.blocks.2.mlp.2.weight\n",
      "decoder.layers.2.fc2.bias -> decoder.blocks.2.mlp.2.bias\n",
      "decoder.layers.2.final_layer_norm.weight -> decoder.blocks.2.mlp_ln.weight\n",
      "decoder.layers.2.final_layer_norm.bias -> decoder.blocks.2.mlp_ln.bias\n",
      "decoder.layers.3.self_attn.k_proj.weight -> decoder.blocks.3.attn.key.weight\n",
      "decoder.layers.3.self_attn.v_proj.weight -> decoder.blocks.3.attn.value.weight\n",
      "decoder.layers.3.self_attn.v_proj.bias -> decoder.blocks.3.attn.value.bias\n",
      "decoder.layers.3.self_attn.q_proj.weight -> decoder.blocks.3.attn.query.weight\n",
      "decoder.layers.3.self_attn.q_proj.bias -> decoder.blocks.3.attn.query.bias\n",
      "decoder.layers.3.self_attn.out_proj.weight -> decoder.blocks.3.attn.out.weight\n",
      "decoder.layers.3.self_attn.out_proj.bias -> decoder.blocks.3.attn.out.bias\n",
      "decoder.layers.3.self_attn_layer_norm.weight -> decoder.blocks.3.attn_ln.weight\n",
      "decoder.layers.3.self_attn_layer_norm.bias -> decoder.blocks.3.attn_ln.bias\n",
      "decoder.layers.3.encoder_attn.k_proj.weight -> decoder.blocks.3.cross_attn.key.weight\n",
      "decoder.layers.3.encoder_attn.v_proj.weight -> decoder.blocks.3.cross_attn.value.weight\n",
      "decoder.layers.3.encoder_attn.v_proj.bias -> decoder.blocks.3.cross_attn.value.bias\n",
      "decoder.layers.3.encoder_attn.q_proj.weight -> decoder.blocks.3.cross_attn.query.weight\n",
      "decoder.layers.3.encoder_attn.q_proj.bias -> decoder.blocks.3.cross_attn.query.bias\n",
      "decoder.layers.3.encoder_attn.out_proj.weight -> decoder.blocks.3.cross_attn.out.weight\n",
      "decoder.layers.3.encoder_attn.out_proj.bias -> decoder.blocks.3.cross_attn.out.bias\n",
      "decoder.layers.3.encoder_attn_layer_norm.weight -> decoder.blocks.3.cross_attn_ln.weight\n",
      "decoder.layers.3.encoder_attn_layer_norm.bias -> decoder.blocks.3.cross_attn_ln.bias\n",
      "decoder.layers.3.fc1.weight -> decoder.blocks.3.mlp.0.weight\n",
      "decoder.layers.3.fc1.bias -> decoder.blocks.3.mlp.0.bias\n",
      "decoder.layers.3.fc2.weight -> decoder.blocks.3.mlp.2.weight\n",
      "decoder.layers.3.fc2.bias -> decoder.blocks.3.mlp.2.bias\n",
      "decoder.layers.3.final_layer_norm.weight -> decoder.blocks.3.mlp_ln.weight\n",
      "decoder.layers.3.final_layer_norm.bias -> decoder.blocks.3.mlp_ln.bias\n",
      "decoder.layer_norm.weight -> decoder.ln.weight\n",
      "decoder.layer_norm.bias -> decoder.ln.bias\n"
     ]
    }
   ],
   "source": [
    "convert_hf_whisper('mesolitica/finetune-whisper-tiny-ms-singlish', 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model('model.pt', device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: en\n",
      "nama saya husain bin zulkifli\n"
     ]
    }
   ],
   "source": [
    "audio = whisper.load_audio(\"husein-zolkepli.wav\")\n",
    "audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "# make log-Mel spectrogram and move to the same device as the model\n",
    "mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "# detect the spoken language\n",
    "_, probs = model.detect_language(mel)\n",
    "print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "# decode the audio\n",
    "options = whisper.DecodingOptions(fp16=False)\n",
    "result = whisper.decode(model, mel, options)\n",
    "\n",
    "# print the recognized text\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya_boilerplate.huggingface import upload_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:101: FutureWarning: `name` and `organization` input arguments are deprecated and will be removed in v0.10. Pass `repo_id` instead.\n",
      "  warnings.warn(\n",
      "<class 'requests.exceptions.HTTPError'> (Request ID: Root=1-63d772d9-6ad4bd6255fee0b9614acc6c)\n",
      "\n",
      "You already created this model repo - You already created this model repo\n"
     ]
    }
   ],
   "source": [
    "files_mapping = {'model.pt': 'model.pt'}\n",
    "upload_dict(model = 'finetune-whisper-tiny-ms-singlish', \n",
    "            files_mapping = files_mapping, username = 'mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
