{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "from collections import namedtuple\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "import torchaudio\n",
    "from pytorch_lightning import LightningModule\n",
    "from torchaudio.models import emformer_rnnt_model, Hypothesis, RNNTBeamSearch\n",
    "from torchaudio.models import Conformer, RNNT\n",
    "from torchaudio.models.rnnt import _Joiner, _Predictor, _TimeReduction, _Transcriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emformer_rnnt_base(num_symbols: int) -> RNNT:\n",
    "    r\"\"\"Builds basic version of Emformer-based :class:`~torchaudio.models.RNNT`.\n",
    "    Args:\n",
    "        num_symbols (int): The size of target token lexicon.\n",
    "    Returns:\n",
    "        RNNT:\n",
    "            Emformer RNN-T model.\n",
    "    \"\"\"\n",
    "    return emformer_rnnt_model(\n",
    "        input_dim=80,\n",
    "        encoding_dim=1024,\n",
    "        num_symbols=num_symbols,\n",
    "        segment_length=16,\n",
    "        right_context_length=4,\n",
    "        time_reduction_input_dim=128,\n",
    "        time_reduction_stride=4,\n",
    "        transformer_num_heads=8,\n",
    "        transformer_ffn_dim=1024,\n",
    "        transformer_num_layers=16,\n",
    "        transformer_dropout=0.1,\n",
    "        transformer_activation=\"gelu\",\n",
    "        transformer_left_context_length=30,\n",
    "        transformer_max_memory_size=0,\n",
    "        transformer_weight_init_scale_strategy=\"depthwise\",\n",
    "        transformer_tanh_on_mem=True,\n",
    "        symbol_embedding_dim=512,\n",
    "        num_lstm_layers=2,\n",
    "        lstm_layer_norm=True,\n",
    "        lstm_layer_norm_epsilon=1e-3,\n",
    "        lstm_dropout=0.3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch = namedtuple(\"Batch\", [\"features\", \"feature_lengths\", \"targets\", \"target_lengths\"])\n",
    "\n",
    "def post_process_hypos(\n",
    "    hypos: List[Hypothesis], sp_model: spm.SentencePieceProcessor\n",
    ") -> List[Tuple[str, float, List[int], List[int]]]:\n",
    "    tokens_idx = 0\n",
    "    score_idx = 3\n",
    "    post_process_remove_list = [\n",
    "        sp_model.unk_id(),\n",
    "        sp_model.eos_id(),\n",
    "        sp_model.pad_id(),\n",
    "    ]\n",
    "    filtered_hypo_tokens = [\n",
    "        [token_index for token_index in h[tokens_idx][1:] if token_index not in post_process_remove_list] for h in hypos\n",
    "    ]\n",
    "    hypos_str = [sp_model.decode(s) for s in filtered_hypo_tokens]\n",
    "    hypos_ids = [h[tokens_idx][1:] for h in hypos]\n",
    "    hypos_score = [[math.exp(h[score_idx])] for h in hypos]\n",
    "\n",
    "    nbest_batch = list(zip(hypos_str, hypos_score, hypos_ids))\n",
    "\n",
    "    return nbest_batch\n",
    "\n",
    "\n",
    "class ConformerRNNTModule(LightningModule):\n",
    "    def __init__(self, sp_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.sp_model = sp_model\n",
    "        spm_vocab_size = self.sp_model.get_piece_size()\n",
    "        self.blank_idx = spm_vocab_size\n",
    "\n",
    "        # ``conformer_rnnt_base`` hardcodes a specific Conformer RNN-T configuration.\n",
    "        # For greater customizability, please refer to ``conformer_rnnt_model``.\n",
    "        self.model = emformer_rnnt_base(num_symbols=1024)\n",
    "\n",
    "    def forward(self, batch: Batch):\n",
    "        decoder = RNNTBeamSearch(self.model, self.blank_idx)\n",
    "        hypotheses = decoder(batch.features.to(self.device), batch.feature_lengths.to(self.device), 20)\n",
    "        return post_process_hypos(hypotheses, self.sp_model)[0][0]\n",
    "\n",
    "    def training_step(self, batch: Batch, batch_idx = None):\n",
    "        \"\"\"Custom training step.\n",
    "        By default, DDP does the following on each train step:\n",
    "        - For each GPU, compute loss and gradient on shard of training data.\n",
    "        - Sync and average gradients across all GPUs. The final gradient\n",
    "          is (sum of gradients across all GPUs) / N, where N is the world\n",
    "          size (total number of GPUs).\n",
    "        - Update parameters on each GPU.\n",
    "        Here, we do the following:\n",
    "        - For k-th GPU, compute loss and scale it by (N / B_total), where B_total is\n",
    "          the sum of batch sizes across all GPUs. Compute gradient from scaled loss.\n",
    "        - Sync and average gradients across all GPUs. The final gradient\n",
    "          is (sum of gradients across all GPUs) / B_total.\n",
    "        - Update parameters on each GPU.\n",
    "        Doing so allows us to account for the variability in batch sizes that\n",
    "        variable-length sequential data yield.\n",
    "        \"\"\"\n",
    "        loss = self._step(batch, batch_idx, \"train\")\n",
    "        batch_size = batch.features.size(0)\n",
    "        batch_sizes = self.all_gather(batch_size)\n",
    "        self.log(\"Gathered batch size\", batch_sizes.sum(), on_step=True, on_epoch=True)\n",
    "        loss *= batch_sizes.size(0) / batch_sizes.sum()  # world size / batch size\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._step(batch, batch_idx, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        return self._step(batch, batch_idx, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pyaudio` is not available, `malaya_speech.streaming.pyaudio` is not able to use.\n"
     ]
    }
   ],
   "source": [
    "import malaya_speech\n",
    "from malaya_speech.utils import torch_featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, _ = malaya_speech.load('speech/example-speaker/husein-zolkepli.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_model = spm.SentencePieceProcessor(model_file='/home/husein/malaya-speech/malay-tts.model')\n",
    "global_stats = torch_featurization.GlobalStatsNormalization('malay-stats.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConformerRNNTModule.load_from_checkpoint('emformer-base-32/model-epoch=19-step=2040000.ckpt',\n",
    "                                                 sp_model=sp_model).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model.state_dict(), 'emformer-base.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([568, 80])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel = torch_featurization.melspectrogram(y)\n",
    "mel = torch_featurization.piecewise_linear_log(mel)\n",
    "mel = global_stats(mel)\n",
    "mel = torch.nn.functional.pad(mel, pad=(0,0,0,4))\n",
    "mel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'testing nama saya husin bin zulkafli'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = RNNTBeamSearch(model.model, model.blank_idx)\n",
    "hypotheses = decoder(mel, torch.Tensor((len(mel),)), 20)\n",
    "post_process_hypos(hypotheses, model.sp_model)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/husein/ssd1/speech-bahasa/malay-asr-test.json') as fopen:\n",
    "    test_set = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cer(actual, hyp):\n",
    "    \"\"\"\n",
    "    Calculate CER using `python-Levenshtein`.\n",
    "    \"\"\"\n",
    "    import Levenshtein as Lev\n",
    "\n",
    "    actual = actual.replace(' ', '')\n",
    "    hyp = hyp.replace(' ', '')\n",
    "    return Lev.distance(actual, hyp) / len(actual)\n",
    "\n",
    "\n",
    "def calculate_wer(actual, hyp):\n",
    "    \"\"\"\n",
    "    Calculate WER using `python-Levenshtein`.\n",
    "    \"\"\"\n",
    "    import Levenshtein as Lev\n",
    "\n",
    "    b = set(actual.split() + hyp.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    w1 = [chr(word2char[w]) for w in actual.split()]\n",
    "    w2 = [chr(word2char[w]) for w in hyp.split()]\n",
    "\n",
    "    return Lev.distance(''.join(w1), ''.join(w2)) / len(actual.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 739/739 [13:40<00:00,  1.11s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "wer, cer = [], []\n",
    "\n",
    "for i in tqdm(range(len(test_set['X']))):\n",
    "    batch_y = [test_set['Y'][i]]\n",
    "    y = malaya_speech.load(test_set['X'][i])[0]\n",
    "    mel = torch_featurization.melspectrogram(y)\n",
    "    mel = torch_featurization.piecewise_linear_log(mel)\n",
    "    mel = global_stats(mel)\n",
    "    \n",
    "    hypotheses = decoder(mel, torch.Tensor((len(mel),)), 20)\n",
    "    pred = post_process_hypos(hypotheses, model.sp_model)[0][0]\n",
    "    \n",
    "    wer.append(calculate_wer(test_set['Y'][i], pred))\n",
    "    cer.append(calculate_cer(test_set['Y'][i], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.18303839134234529, 0.07738533622881417)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.mean(wer), np.mean(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/husein/malaya-speech/postprocess-malaya-malay-test-set.json') as fopen:\n",
    "    malaya_malay = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 765/765 [05:33<00:00,  2.29it/s]\n"
     ]
    }
   ],
   "source": [
    "wer, cer = [], []\n",
    "\n",
    "for i in tqdm(range(len(malaya_malay))):\n",
    "    if not malaya_malay[i]['accept']:\n",
    "        continue\n",
    "    \n",
    "    y = malaya_speech.load(f'/home/husein/malaya-speech/malay-test/{i}.wav')[0]\n",
    "    mel = torch_featurization.melspectrogram(y)\n",
    "    mel = torch_featurization.piecewise_linear_log(mel)\n",
    "    mel = global_stats(mel)\n",
    "    \n",
    "    hypotheses = decoder(mel, torch.Tensor((len(mel),)), 20)\n",
    "    pred = post_process_hypos(hypotheses, model.sp_model)[0][0]\n",
    "    \n",
    "    wer.append(calculate_wer(malaya_malay[i]['cleaned'], pred))\n",
    "    cer.append(calculate_cer(malaya_malay[i]['cleaned'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1757624237861392, 0.062339190005373434)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(wer), np.mean(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya_boilerplate.huggingface import upload_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:101: FutureWarning: `name` and `organization` input arguments are deprecated and will be removed in v0.10. Pass `repo_id` instead.\n",
      "  warnings.warn(\n",
      "<class 'requests.exceptions.HTTPError'> (Request ID: Root=1-63edd481-327bf3d869f5fb1c6a02b535)\n",
      "\n",
      "You already created this model repo - You already created this model repo\n"
     ]
    }
   ],
   "source": [
    "files_mapping = {'emformer-base.pt': 'model.pt',\n",
    "                 '/home/husein/malaya-speech/malay-tts.model': 'malay-stt.model',\n",
    "                'malay-stats.json': 'malay-stats.json'}\n",
    "upload_dict(model = 'emformer-base', files_mapping = files_mapping, username = 'mesolitica')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
