{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Config, Wav2Vec2ForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:368: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = Wav2Vec2Config.from_pretrained('facebook/wav2vec2-base')\n",
    "config.intermediate_size = 2048\n",
    "config.hidden_size = 512\n",
    "config.num_hidden_layers = 6\n",
    "config.num_attention_heads = 8\n",
    "config.save_pretrained('./wav2vec2-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_7_1000000.pt\t\tcheckpoint.best_loss_3.0450.pt\r\n",
      "checkpoint.best_loss_3.0372.pt\tcheckpoint_best.pt\r\n",
      "checkpoint.best_loss_3.0422.pt\tcheckpoint_last.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/husein/outputs/2022-10-08/02-41-28/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_8_1020000.pt  checkpoint_last.pt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/husein/outputs/2022-10-11/11-47-25/checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  train_inner  valid\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/husein/outputs/2022-10-08/23-28-04/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = '/home/husein/outputs/2022-10-08/23-28-04/checkpoints/checkpoint.best_loss_3.2552.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file wav2vec2-small/config.json\n",
      "/home/husein/.local/lib/python3.8/site-packages/transformers/configuration_utils.py:368: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  warnings.warn(\n",
      "Model config Wav2Vec2Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "2022-10-11 13:26:21 | INFO | __main__ |  was initialized from mask_emb.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | Feat extract conv layer 0 was initialized from feature_extractor.conv_layers.0.0.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | Feat extract layer norm weight of layer 0 was initialized from feature_extractor.conv_layers.0.2.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | Feat extract layer norm weight of layer 0 was initialized from feature_extractor.conv_layers.0.2.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | Feat extract conv layer 1 was initialized from feature_extractor.conv_layers.1.0.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | Feat extract conv layer 2 was initialized from feature_extractor.conv_layers.2.0.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | Feat extract conv layer 3 was initialized from feature_extractor.conv_layers.3.0.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | Feat extract conv layer 4 was initialized from feature_extractor.conv_layers.4.0.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | Feat extract conv layer 5 was initialized from feature_extractor.conv_layers.5.0.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | Feat extract conv layer 6 was initialized from feature_extractor.conv_layers.6.0.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ |  was initialized from quantizer.vars.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | quantizer.weight_proj.weight was initialized from quantizer.weight_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | quantizer.weight_proj.bias was initialized from quantizer.weight_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | project_q.weight was initialized from project_q.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | project_q.bias was initialized from project_q.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.pos_conv_embed.conv.bias was initialized from encoder.pos_conv.0.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.pos_conv_embed.conv.weight_g was initialized from encoder.pos_conv.0.weight_g.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.pos_conv_embed.conv.weight_v was initialized from encoder.pos_conv.0.weight_v.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.k_proj.weight was initialized from encoder.layers.0.self_attn.k_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.k_proj.bias was initialized from encoder.layers.0.self_attn.k_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.v_proj.weight was initialized from encoder.layers.0.self_attn.v_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.v_proj.bias was initialized from encoder.layers.0.self_attn.v_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.q_proj.weight was initialized from encoder.layers.0.self_attn.q_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.q_proj.bias was initialized from encoder.layers.0.self_attn.q_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.out_proj.weight was initialized from encoder.layers.0.self_attn.out_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.attention.out_proj.bias was initialized from encoder.layers.0.self_attn.out_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.layer_norm.weight was initialized from encoder.layers.0.self_attn_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.layer_norm.bias was initialized from encoder.layers.0.self_attn_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.weight was initialized from encoder.layers.0.fc1.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.feed_forward.intermediate_dense.bias was initialized from encoder.layers.0.fc1.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.feed_forward.output_dense.weight was initialized from encoder.layers.0.fc2.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.feed_forward.output_dense.bias was initialized from encoder.layers.0.fc2.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.final_layer_norm.weight was initialized from encoder.layers.0.final_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.0.final_layer_norm.bias was initialized from encoder.layers.0.final_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.k_proj.weight was initialized from encoder.layers.1.self_attn.k_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.k_proj.bias was initialized from encoder.layers.1.self_attn.k_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.v_proj.weight was initialized from encoder.layers.1.self_attn.v_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.v_proj.bias was initialized from encoder.layers.1.self_attn.v_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.q_proj.weight was initialized from encoder.layers.1.self_attn.q_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.q_proj.bias was initialized from encoder.layers.1.self_attn.q_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.out_proj.weight was initialized from encoder.layers.1.self_attn.out_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.attention.out_proj.bias was initialized from encoder.layers.1.self_attn.out_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.layer_norm.weight was initialized from encoder.layers.1.self_attn_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.layer_norm.bias was initialized from encoder.layers.1.self_attn_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.weight was initialized from encoder.layers.1.fc1.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.feed_forward.intermediate_dense.bias was initialized from encoder.layers.1.fc1.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.feed_forward.output_dense.weight was initialized from encoder.layers.1.fc2.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.feed_forward.output_dense.bias was initialized from encoder.layers.1.fc2.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.final_layer_norm.weight was initialized from encoder.layers.1.final_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.1.final_layer_norm.bias was initialized from encoder.layers.1.final_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.k_proj.weight was initialized from encoder.layers.2.self_attn.k_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.k_proj.bias was initialized from encoder.layers.2.self_attn.k_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.v_proj.weight was initialized from encoder.layers.2.self_attn.v_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.v_proj.bias was initialized from encoder.layers.2.self_attn.v_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.q_proj.weight was initialized from encoder.layers.2.self_attn.q_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.q_proj.bias was initialized from encoder.layers.2.self_attn.q_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.out_proj.weight was initialized from encoder.layers.2.self_attn.out_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.attention.out_proj.bias was initialized from encoder.layers.2.self_attn.out_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.layer_norm.weight was initialized from encoder.layers.2.self_attn_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.layer_norm.bias was initialized from encoder.layers.2.self_attn_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.weight was initialized from encoder.layers.2.fc1.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.feed_forward.intermediate_dense.bias was initialized from encoder.layers.2.fc1.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.feed_forward.output_dense.weight was initialized from encoder.layers.2.fc2.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.feed_forward.output_dense.bias was initialized from encoder.layers.2.fc2.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.final_layer_norm.weight was initialized from encoder.layers.2.final_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.2.final_layer_norm.bias was initialized from encoder.layers.2.final_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.k_proj.weight was initialized from encoder.layers.3.self_attn.k_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.k_proj.bias was initialized from encoder.layers.3.self_attn.k_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.v_proj.weight was initialized from encoder.layers.3.self_attn.v_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.v_proj.bias was initialized from encoder.layers.3.self_attn.v_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.q_proj.weight was initialized from encoder.layers.3.self_attn.q_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.q_proj.bias was initialized from encoder.layers.3.self_attn.q_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.out_proj.weight was initialized from encoder.layers.3.self_attn.out_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.attention.out_proj.bias was initialized from encoder.layers.3.self_attn.out_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.layer_norm.weight was initialized from encoder.layers.3.self_attn_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.layer_norm.bias was initialized from encoder.layers.3.self_attn_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.weight was initialized from encoder.layers.3.fc1.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.feed_forward.intermediate_dense.bias was initialized from encoder.layers.3.fc1.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.feed_forward.output_dense.weight was initialized from encoder.layers.3.fc2.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.feed_forward.output_dense.bias was initialized from encoder.layers.3.fc2.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.final_layer_norm.weight was initialized from encoder.layers.3.final_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.3.final_layer_norm.bias was initialized from encoder.layers.3.final_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.k_proj.weight was initialized from encoder.layers.4.self_attn.k_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.k_proj.bias was initialized from encoder.layers.4.self_attn.k_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.v_proj.weight was initialized from encoder.layers.4.self_attn.v_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.v_proj.bias was initialized from encoder.layers.4.self_attn.v_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.q_proj.weight was initialized from encoder.layers.4.self_attn.q_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.q_proj.bias was initialized from encoder.layers.4.self_attn.q_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.out_proj.weight was initialized from encoder.layers.4.self_attn.out_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.attention.out_proj.bias was initialized from encoder.layers.4.self_attn.out_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.layer_norm.weight was initialized from encoder.layers.4.self_attn_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.layer_norm.bias was initialized from encoder.layers.4.self_attn_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.weight was initialized from encoder.layers.4.fc1.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.feed_forward.intermediate_dense.bias was initialized from encoder.layers.4.fc1.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.feed_forward.output_dense.weight was initialized from encoder.layers.4.fc2.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.feed_forward.output_dense.bias was initialized from encoder.layers.4.fc2.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.final_layer_norm.weight was initialized from encoder.layers.4.final_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.4.final_layer_norm.bias was initialized from encoder.layers.4.final_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.k_proj.weight was initialized from encoder.layers.5.self_attn.k_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.k_proj.bias was initialized from encoder.layers.5.self_attn.k_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.v_proj.weight was initialized from encoder.layers.5.self_attn.v_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.v_proj.bias was initialized from encoder.layers.5.self_attn.v_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.q_proj.weight was initialized from encoder.layers.5.self_attn.q_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.q_proj.bias was initialized from encoder.layers.5.self_attn.q_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.out_proj.weight was initialized from encoder.layers.5.self_attn.out_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.attention.out_proj.bias was initialized from encoder.layers.5.self_attn.out_proj.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.layer_norm.weight was initialized from encoder.layers.5.self_attn_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.layer_norm.bias was initialized from encoder.layers.5.self_attn_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.weight was initialized from encoder.layers.5.fc1.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.feed_forward.intermediate_dense.bias was initialized from encoder.layers.5.fc1.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.feed_forward.output_dense.weight was initialized from encoder.layers.5.fc2.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.feed_forward.output_dense.bias was initialized from encoder.layers.5.fc2.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.final_layer_norm.weight was initialized from encoder.layers.5.final_layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layers.5.final_layer_norm.bias was initialized from encoder.layers.5.final_layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layer_norm.weight was initialized from encoder.layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.encoder.layer_norm.bias was initialized from encoder.layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.feature_projection.layer_norm.weight was initialized from layer_norm.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | wav2vec2.feature_projection.layer_norm.bias was initialized from layer_norm.bias.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | project_hid.weight was initialized from final_proj.weight.\n",
      "2022-10-11 13:26:21 | INFO | __main__ | project_hid.bias was initialized from final_proj.bias.\n",
      "2022-10-11 13:26:21 | WARNING | __main__ | Unused weights: []\n",
      "Configuration saved in wav2vec2-small/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved in wav2vec2-small/pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m transformers.models.wav2vec2.convert_wav2vec2_original_pytorch_checkpoint_to_pytorch --pytorch_dump_folder wav2vec2-small --checkpoint_path {ckpt} --config_path wav2vec2-small/config.json --not_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForPreTraining.from_pretrained('./wav2vec2-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35794ce009234b16b110c4302281f439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/99.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/mesolitica/wav2vec2-small-ms\n",
      "   14f411b..c3a773c  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/mesolitica/wav2vec2-small-ms/commit/c3a773c04956fde180a83113a6252bfa5f6f5333'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('wav2vec2-small-ms', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!cd wav2vec2-small-ms && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 7c7db8d] add tensorboard\n",
      " 3 files changed, 6 insertions(+), 6 deletions(-)\n",
      "Uploading LFS objects: 100% (3/3), 9.2 MB | 1.7 MB/s, done.                     \n",
      "Enumerating objects: 17, done.\n",
      "Counting objects: 100% (17/17), done.\n",
      "Delta compression using up to 20 threads\n",
      "Compressing objects: 100% (9/9), done.\n",
      "Writing objects: 100% (9/9), 970 bytes | 970.00 KiB/s, done.\n",
      "Total 9 (delta 1), reused 0 (delta 0)\n",
      "remote: Scanning LFS files for validity, may be slow...\u001b[K\n",
      "remote: LFS file scan complete.\u001b[K\n",
      "To https://huggingface.co/mesolitica/wav2vec2-small-ms\n",
      "   c3a773c..7c7db8d  main -> main\n"
     ]
    }
   ],
   "source": [
    "!cp -r /home/husein/outputs/2022-10-08/23-28-04/tensorboard wav2vec2-small-ms\n",
    "!cd wav2vec2-small-ms && git add . && git commit -m 'add tensorboard' && git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
