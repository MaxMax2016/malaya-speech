{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['dumping-cleaned-news.txt', 'filtered-dumping-wiki.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "pure_texts = []\n",
    "for f in files:\n",
    "    \n",
    "    with open(f'/home/husein/pure-text/{f}') as fopen:\n",
    "        txts = list(filter(None, fopen.read().split('\\n')))\n",
    "\n",
    "    pure_texts.extend(random.sample(txts, 200000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = '/home/husein/speech-bahasa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7490, 10)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{base_directory}/cv-corpus-5.1-2020-06-22/id/validated.tsv', sep = '\\t')\n",
    "df = df[(df['sentence'].str.len() > 5) & (df['sentence'].str.count(' ') > 0)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7490"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_commonvoice = []\n",
    "for i in range(len(df)):\n",
    "    p = f\"{base_directory}/cv-corpus-5.1-2020-06-22/id/clips/{df['path'].iloc[i]}\"\n",
    "    t = df['sentence'].iloc[i]\n",
    "    if len(t) < 5:\n",
    "        continue\n",
    "    id_commonvoice.append((p, t))\n",
    "\n",
    "len(id_commonvoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "377474"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malay = glob(f'{base_directory}/part*/semisupervised/output-wav/*.wav')\n",
    "len(malay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 377474/377474 [01:11<00:00, 5300.51it/s]\n"
     ]
    }
   ],
   "source": [
    "malays = []\n",
    "for i in tqdm(malay):\n",
    "    try:\n",
    "        p = i.replace('output-wav','output-text')\n",
    "        with open(f'{p}.txt') as fopen:\n",
    "            text = fopen.read()\n",
    "        if len(text) < 4:\n",
    "            continue\n",
    "        malays.append((i, text))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2887/2887 [00:00<00:00, 392765.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2887"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wikipedia = []\n",
    "wavs = glob(f'{base_directory}/streaming/*wav')\n",
    "for i in tqdm(wavs):\n",
    "    text = os.path.split(i)[1].replace('.wav', '')\n",
    "    wikipedia.append((i, text))\n",
    "    \n",
    "len(wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2151/2151 [00:00<00:00, 99362.85it/s]\n"
     ]
    }
   ],
   "source": [
    "news = []\n",
    "wavs = glob(f'{base_directory}/news/audio/*wav')\n",
    "\n",
    "with open(f'{base_directory}/transcript-news.json') as fopen:\n",
    "    transcript_news = json.load(fopen)\n",
    "    \n",
    "for i in tqdm(wavs):\n",
    "    index = i.split('/')[-1].replace('.wav','')\n",
    "    text = transcript_news[int(index)]\n",
    "    news.append((i, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64341/64341 [00:05<00:00, 12620.71it/s]\n"
     ]
    }
   ],
   "source": [
    "audiobook = []\n",
    "wavs = glob(f'{base_directory}/combined/*wav')\n",
    "for i in tqdm(wavs):\n",
    "    t = '/'.join(i.split('<>')[1:])\n",
    "    t = t.split('.wav')[0]\n",
    "    t = t.replace('output-wav', 'output-text')\n",
    "    with open(f'{base_directory}/text-audiobook/{t}.wav.txt') as fopen:\n",
    "        text = fopen.read()\n",
    "    audiobook.append((i, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4294/4294 [00:00<00:00, 783671.63it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{base_directory}/haqkiem/metadata.csv', header = None, sep = '|')\n",
    "txts = df.values.tolist()\n",
    "haqkiem = []\n",
    "for f in tqdm(txts):\n",
    "    text = f[1]\n",
    "    text = text.split('.,,')[0]\n",
    "    f = f[0]\n",
    "    r = f'{base_directory}/haqkiem/{f}.wav'\n",
    "    haqkiem.append((r, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = id_commonvoice + malays + wikipedia + news + audiobook + haqkiem\n",
    "audios, texts = zip(*audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"'\", \"-\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"*\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\", \"%\", \"$\", \"\\\"\", \"/\", \"&\", \":\", \"+\"]\n",
    "def preprocessing_text(string):\n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c for c in string if c in vocabs])\n",
    "    return re.sub(r'[ ]+', ' ', string).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pure_texts + list(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 569356/858066 [00:13<00:06, 42572.14it/s]\n",
      "\n",
      "\n",
      "100%|██████████| 858066/858066 [00:20<00:00, 42207.46it/s]A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "processed_text = [preprocessing_text(t) for t in tqdm(combined)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cleaned-rnn-lm.json', 'w') as fopen:\n",
    "    json.dump(processed_text, fopen)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
