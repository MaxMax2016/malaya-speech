{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "125e9d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9743e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 05:42:11.040774: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "/home/husein/tf-nvidia/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import malaya_speech.config\n",
    "from malaya_speech.train.model import squeezeformer, transducer\n",
    "import malaya_speech\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09553a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords = malaya_speech.subword.load('transducer.subword.subwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "906f5571",
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = malaya_speech.tf_featurization.STTFeaturizer(\n",
    "    normalize_per_feature = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff50fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.compat.v1.placeholder(tf.float32, [None, None], name = 'X_placeholder')\n",
    "X_len = tf.compat.v1.placeholder(tf.int32, [None], name = 'X_len_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6324be5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'ExpandDims:0' shape=(?, ?, 80, 1) dtype=float32>,\n",
       " <tf.Tensor 'TensorArrayStack_2/TensorArrayGatherV3:0' shape=(?,) dtype=int32>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = tf.shape(X)[0]\n",
    "features = tf.TensorArray(dtype = tf.float32, size = batch_size, dynamic_size = True, infer_shape = False)\n",
    "features_len = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "\n",
    "init_state = (0, features, features_len)\n",
    "\n",
    "def condition(i, features, features_len):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, features, features_len):\n",
    "    f = featurizer(X[i, :X_len[i]])\n",
    "    f_len = tf.shape(f)[0]\n",
    "    return i + 1, features.write(i, f), features_len.write(i, f_len)\n",
    "\n",
    "_, features, features_len = tf.while_loop(condition, body, init_state)\n",
    "features_len = features_len.stack()\n",
    "padded_features = tf.TensorArray(dtype = tf.float32, size = batch_size)\n",
    "padded_lens = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "maxlen = tf.reduce_max(features_len)\n",
    "\n",
    "init_state = (0, padded_features, padded_lens)\n",
    "\n",
    "def condition(i, padded_features, padded_lens):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, padded_features, padded_lens):\n",
    "    f = features.read(i)\n",
    "    len_f = tf.shape(f)[0]\n",
    "    f = tf.pad(f, [[0, maxlen - tf.shape(f)[0]], [0,0]])\n",
    "    return i + 1, padded_features.write(i, f), padded_lens.write(i, len_f)\n",
    "\n",
    "_, padded_features, padded_lens = tf.while_loop(condition, body, init_state)\n",
    "padded_features = padded_features.stack()\n",
    "padded_lens = padded_lens.stack()\n",
    "padded_lens.set_shape((None,))\n",
    "padded_features.set_shape((None, None, 80))\n",
    "padded_features = tf.expand_dims(padded_features, -1)\n",
    "padded_features, padded_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66d509a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = tf.identity(padded_features, name = 'padded_features')\n",
    "padded_lens = tf.identity(padded_lens, name = 'padded_lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3519c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/squeezeformer/subsampling.py:40: The name tf.keras.initializers.RandomUniform is deprecated. Please use tf.compat.v1.keras.initializers.RandomUniform instead.\n",
      "\n",
      "INFO:tensorflow:Subsampling with DS conv\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n",
      "['M', 's', 'C', 's']\n"
     ]
    }
   ],
   "source": [
    "config = malaya_speech.config.squeezeformer_m_encoder_config\n",
    "config['encoder_dropout'] = 0.0\n",
    "model = squeezeformer.Model(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d31b91be",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_config = malaya_speech.config.conformer_base_decoder_config\n",
    "decoder_config['embed_dropout'] = 0.0\n",
    "transducer_model = transducer.rnn.Model(\n",
    "    model, vocabulary_size = subwords.vocab_size, **decoder_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342da830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
    "z = tf.zeros((tf.shape(p)[0], 1),dtype=tf.int32)\n",
    "c = tf.concat([z, p], axis = 1)\n",
    "p_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac9f0315",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac0fa023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transducer/transducer_joint/transducer_joint_vocab/BiasAdd:0' shape=(?, ?, ?, 1030) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = transducer_model([padded_features, padded_lens, c, p_len], training = training)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45b61378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_410014/2303108271.py:1: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_410014/2303108271.py:2: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 05:42:27.690477: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2022-08-17 05:42:27.731978: E tensorflow/stream_executor/cuda/cuda_driver.cc:282] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-08-17 05:42:27.732007: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: husein-MS-7D31\n",
      "2022-08-17 05:42:27.732010: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: husein-MS-7D31\n",
      "2022-08-17 05:42:27.732082: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 470.141.3\n",
      "2022-08-17 05:42:27.732097: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 470.141.3\n",
      "2022-08-17 05:42:27.732100: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 470.141.3\n",
      "2022-08-17 05:42:27.737849: I tensorflow/core/platform/profile_utils/cpu_utils.cc:109] CPU Frequency: 2112000000 Hz\n",
      "2022-08-17 05:42:27.738251: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x720fd90 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-08-17 05:42:27.738263: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a2f2870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_410014/134513056.py:1: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_410014/134513056.py:1: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_410014/134513056.py:2: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from asr-m-squeezeformer-transducer/model.ckpt-800000\n"
     ]
    }
   ],
   "source": [
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_list)\n",
    "saver.restore(sess, 'asr-m-squeezeformer-transducer/model.ckpt-800000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbf8d819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'padded_features:0' shape=(?, ?, 80, 1) dtype=float32>,\n",
       " <tf.Tensor 'padded_lens:0' shape=(?,) dtype=int32>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_features, padded_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "464c4029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'greedy_decoder:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = transducer_model.greedy_decoder(padded_features, padded_lens, training = training)\n",
    "decoded = tf.identity(decoded[0], name = 'greedy_decoder')\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5a0feced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'greedy_decoder:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66c3addf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'conformer_1/conformer_encoder/conformer_encoder_block_19/conformer_encoder_block_19_layer3/conformer_encoder_block_19_layer3_ln/batchnorm/add_1:0' shape=(?, ?, 324) dtype=float32>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = transducer_model.encoder(padded_features, padded_lens, training = training)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b06208",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = tf.identity(encoded, name = 'encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5e06d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_410014/500527222.py:1: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'ytu:0' shape=(1030,) dtype=float32>,\n",
       " <tf.Tensor 'new_states:0' shape=(1, 2, 1, 640) dtype=float32>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_placeholder = tf.placeholder(tf.float32, [config['encoder_dmodel']], name = 'encoded_placeholder')\n",
    "predicted_placeholder = tf.placeholder(tf.int32, None, name = 'predicted_placeholder')\n",
    "t = transducer_model.predict_net.get_initial_state().shape\n",
    "states_placeholder = tf.placeholder(tf.float32, [int(i) for i in t], name = 'states_placeholder')\n",
    "\n",
    "ytu, new_states = transducer_model.decoder_inference(\n",
    "    encoded=encoded_placeholder,\n",
    "    predicted=predicted_placeholder,\n",
    "    states=states_placeholder,\n",
    "    training = training\n",
    ")\n",
    "\n",
    "ytu = tf.identity(ytu, name = 'ytu')\n",
    "new_states = tf.identity(new_states, name = 'new_states')\n",
    "ytu, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "099c1778",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = transducer_model.predict_net.get_initial_state()\n",
    "initial_states = tf.identity(initial_states, name = 'initial_states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b72a381",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'speech/record/savewav_2020-11-26_22-36-06_294832.wav',\n",
    "    'speech/record/savewav_2020-11-26_22-40-56_929661.wav',\n",
    "    'speech/record/675.wav',\n",
    "    'speech/record/664.wav',\n",
    "    'speech/example-speaker/husein-zolkepli.wav',\n",
    "    'speech/example-speaker/mas-aisyah.wav',\n",
    "    'speech/example-speaker/khalil-nooh.wav',\n",
    "    'speech/example-speaker/shafiqah-idayu.wav',\n",
    "    'speech/khutbah/wadi-annuar.wav',\n",
    "]\n",
    "\n",
    "front_pad = 200\n",
    "back_pad = 2000\n",
    "inputs = [malaya_speech.load(f)[0] for f in files]\n",
    "padded, lens = malaya_speech.padding.sequence_1d(inputs, return_len = True)\n",
    "back = np.zeros(shape = (len(inputs), back_pad))\n",
    "front = np.zeros(shape = (len(inputs), front_pad))\n",
    "padded = np.concatenate([front, padded, back], axis = -1)\n",
    "lens = [l + front_pad + back_pad for l in lens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "195432d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "BeamHypothesis = collections.namedtuple(\n",
    "    'BeamHypothesis', ('score', 'prediction', 'states')\n",
    ")\n",
    "\n",
    "\n",
    "def transducer(\n",
    "    enc,\n",
    "    total,\n",
    "    initial_states,\n",
    "    encoded_placeholder,\n",
    "    predicted_placeholder,\n",
    "    states_placeholder,\n",
    "    ytu,\n",
    "    new_states,\n",
    "    sess,\n",
    "    beam_width = 10,\n",
    "    norm_score = True,\n",
    "):\n",
    "    kept_hyps = [\n",
    "        BeamHypothesis(score = 0.0, prediction = [0], states = initial_states)\n",
    "    ]\n",
    "    B = kept_hyps\n",
    "    for i in range(total):\n",
    "        A = B\n",
    "        B = []\n",
    "        while True:\n",
    "            y_hat = max(A, key = lambda x: x.score)\n",
    "            A.remove(y_hat)\n",
    "            ytu_, new_states_ = sess.run(\n",
    "                [ytu, new_states],\n",
    "                feed_dict = {\n",
    "                    encoded_placeholder: enc[i],\n",
    "                    predicted_placeholder: y_hat.prediction[-1],\n",
    "                    states_placeholder: y_hat.states,\n",
    "                },\n",
    "            )\n",
    "            for k in range(ytu_.shape[0]):\n",
    "                beam_hyp = BeamHypothesis(\n",
    "                    score = (y_hat.score + float(ytu_[k])),\n",
    "                    prediction = y_hat.prediction,\n",
    "                    states = y_hat.states,\n",
    "                )\n",
    "                if k == 0:\n",
    "                    B.append(beam_hyp)\n",
    "                else:\n",
    "                    beam_hyp = BeamHypothesis(\n",
    "                        score = beam_hyp.score,\n",
    "                        prediction = (beam_hyp.prediction + [int(k)]),\n",
    "                        states = new_states_,\n",
    "                    )\n",
    "                    A.append(beam_hyp)\n",
    "            if len(B) > beam_width:\n",
    "                break\n",
    "    if norm_score:\n",
    "        kept_hyps = sorted(\n",
    "            B, key = lambda x: x.score / len(x.prediction), reverse = True\n",
    "        )[:beam_width]\n",
    "    else:\n",
    "        kept_hyps = sorted(B, key = lambda x: x.score, reverse = True)[\n",
    "            :beam_width\n",
    "        ]\n",
    "    return kept_hyps[0].prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "086b8032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 05:42:51.966770: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 236079360 exceeds 10% of system memory.\n",
      "2022-08-17 05:42:52.081442: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 242459568 exceeds 10% of system memory.\n",
      "2022-08-17 05:42:52.154178: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 59019840 exceeds 10% of system memory.\n",
      "2022-08-17 05:42:52.178204: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 59019840 exceeds 10% of system memory.\n",
      "2022-08-17 05:42:52.644016: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 376257312 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helo nama saya mesin saya tak suka mandi ketat saya masam\n",
      "helo nama saya husin saya suka mandi saya mandi setiap hari\n",
      "ini dan melalui kenyataan mesej itu mastura menegaskan\n",
      "pilihan tepat apabila dia kini lebih berani dan\n",
      "testing nama saya husin bin zulkari\n",
      "sebut perkataan uncle\n",
      "tolong sebut antikata\n",
      "nama saya syafiqa hidayah\n",
      "jadi dalam perjalanan ini dunia yang susah ini ketika nabi mengajar muaz bin jabal tadi ni allahumma in\n",
      "CPU times: user 28.7 s, sys: 3.93 s, total: 32.7 s\n",
      "Wall time: 5.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = sess.run(decoded, feed_dict = {X: padded, X_len: lens})\n",
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2222c7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helo nama saya mesin saya tak suka mandi ketat saya masam\n",
      "helo nama saya husin saya suka mandi saya mandi setiap hari\n",
      "ini dan melalui kenyataan mesej itu mastura menegaskan\n",
      "pilihan tepat apabila dia kini lebih berani dan\n",
      "testing nama saya husin bin zulkapli\n",
      "sebut perkataan uncle\n",
      "tolong sebut antikata\n",
      "nama saya syafiqa hidayah\n",
      "jadi dalam perjalanan ini dunia yang susah ini ketika nabi mengajar muaz bin jabal tadi ni allahu\n",
      "CPU times: user 39.6 s, sys: 2.02 s, total: 41.6 s\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "encoded_, padded_lens_  = sess.run([encoded, padded_lens], feed_dict = {X: padded, X_len: lens})\n",
    "padded_lens_ = padded_lens_ // model.encoder.conv_subsampling.time_reduction_factor\n",
    "s = sess.run(initial_states)\n",
    "\n",
    "for i in range(len(encoded_)):\n",
    "    r = transducer(\n",
    "        enc = encoded_[i],\n",
    "        total = padded_lens_[i],\n",
    "        initial_states = s,\n",
    "        encoded_placeholder = encoded_placeholder,\n",
    "        predicted_placeholder = predicted_placeholder,\n",
    "        states_placeholder = states_placeholder,\n",
    "        ytu = ytu,\n",
    "        new_states = new_states,\n",
    "        sess = sess,\n",
    "        beam_width = 1,\n",
    "    )\n",
    "\n",
    "    print(malaya_speech.subword.decode(subwords, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "abaa2e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hypothesis(index=<tf.Tensor 'while_3/Exit_1:0' shape=() dtype=int32>, prediction=<tf.Tensor 'TensorArrayStack_3/TensorArrayGatherV3:0' shape=(?,) dtype=int32>, states=<tf.Tensor 'while_3/Exit_3:0' shape=(1, 2, 1, 640) dtype=float32>, alignment=<tf.Tensor 'TensorArrayStack_4/TensorArrayGatherV3:0' shape=(?, 1030) dtype=float32>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = padded_lens // model.encoder.conv_subsampling.time_reduction_factor\n",
    "encoded = transducer_model.encoder(padded_features, padded_lens, training = training)\n",
    "g = transducer_model._perform_greedy(encoded[0], l[0],\n",
    "                                tf.constant(0, dtype = tf.int32),\n",
    "                                transducer_model.predict_net.get_initial_state())\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "93f27d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = g.prediction\n",
    "minus_one = -1 * tf.ones_like(indices, dtype=tf.int32)\n",
    "blank_like = 0 * tf.ones_like(indices, dtype=tf.int32)\n",
    "indices = tf.where(indices == minus_one, blank_like, indices)\n",
    "num_samples = tf.cast(X_len[0], dtype=tf.float32)\n",
    "total_time_reduction_factor = featurizer.frame_step\n",
    "stime = tf.range(0, num_samples, delta=total_time_reduction_factor, dtype=tf.float32)\n",
    "stime /= tf.cast(featurizer.sample_rate, dtype=tf.float32)\n",
    "stime = stime[::tf.shape(stime)[0] // tf.shape(indices)[0]]\n",
    "stime.set_shape((None,))\n",
    "non_blank = tf.where(tf.not_equal(indices, 0))\n",
    "non_blank_transcript = tf.gather_nd(indices, non_blank)\n",
    "non_blank_stime = tf.gather_nd(stime, non_blank)\n",
    "non_blank_transcript = tf.identity(non_blank_transcript, name = 'non_blank_transcript')\n",
    "non_blank_stime = tf.identity(non_blank_stime, name = 'non_blank_stime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "436539f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 1.29 s, total: 14.4 s\n",
      "Wall time: 4.42 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = sess.run([non_blank_transcript, non_blank_stime], feed_dict = {X: padded, X_len: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70055f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words, indices = [], []\n",
    "for no, ids in enumerate(r[0]):\n",
    "    w = subwords._id_to_subword(ids - 1)\n",
    "    if type(w) == bytes:\n",
    "        w = w.decode()\n",
    "    words.extend([w, None])\n",
    "    indices.extend([no, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cced5c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "from malaya_speech.utils import text_encoder\n",
    "\n",
    "def _trim_underscore_and_tell(token):\n",
    "    if token.endswith('_'):\n",
    "        return token[:-1], True\n",
    "    return token, False\n",
    "\n",
    "def decode(ids):\n",
    "    ids = text_encoder.pad_decr(ids)\n",
    "    subword_ids = ids\n",
    "    del ids\n",
    "\n",
    "    subwords_ = []\n",
    "    prev_bytes = []\n",
    "    prev_ids = []\n",
    "    ids = []\n",
    "\n",
    "    def consume_prev_bytes():\n",
    "        if prev_bytes:\n",
    "            subwords_.extend(prev_bytes)\n",
    "            ids.extend(prev_ids)\n",
    "        return [], []\n",
    "\n",
    "    for no, subword_id in enumerate(subword_ids):\n",
    "        subword = subwords._id_to_subword(subword_id)\n",
    "        if isinstance(subword, six.binary_type):\n",
    "            # Byte-encoded\n",
    "            prev_bytes.append(subword.decode('utf-8', 'replace'))\n",
    "            if subword == b' ':\n",
    "                prev_ids.append(None)\n",
    "            else:\n",
    "                prev_ids.append(no)\n",
    "        else:\n",
    "            # If there were bytes previously, convert to unicode.\n",
    "            prev_bytes, prev_ids = consume_prev_bytes()\n",
    "            trimmed, add_space = _trim_underscore_and_tell(subword)\n",
    "            ids.append(no)\n",
    "            subwords_.append(trimmed)\n",
    "            if add_space:\n",
    "                subwords_.append(' ')\n",
    "                ids.append(None)\n",
    "    prev_bytes = consume_prev_bytes()\n",
    "\n",
    "    return subwords_, ids\n",
    "\n",
    "words, indices = decode(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "487f6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_indices(subwords, ids, l, reduction_factor = 160, sample_rate = 16000):\n",
    "    result, temp_l, temp_r = [], [], []\n",
    "    for i in range(len(subwords)):\n",
    "        if ids[i] is not None:\n",
    "            temp_l.append(subwords[i])\n",
    "            temp_r.append(l[ids[i]])\n",
    "        else:\n",
    "            data = {'text': ''.join(temp_l), \n",
    "                    'start': round(temp_r[0],4), \n",
    "                    'end': round(temp_r[-1] + (reduction_factor / sample_rate), 4)}\n",
    "            result.append(data)\n",
    "            temp_l, temp_r = [], []\n",
    "    \n",
    "    if len(temp_l):\n",
    "        data = {'text': ''.join(temp_l), \n",
    "                'start': round(temp_r[0],4), \n",
    "                'end': round(temp_r[-1] + (reduction_factor / sample_rate), 4)}\n",
    "        result.append(data)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37bba668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'helo', 'start': 0.04, 'end': 0.17},\n",
       " {'text': 'nama', 'start': 0.8, 'end': 1.01},\n",
       " {'text': 'saya', 'start': 1.08, 'end': 1.33},\n",
       " {'text': 'mesin', 'start': 1.48, 'end': 1.77},\n",
       " {'text': 'saya', 'start': 2.68, 'end': 2.77},\n",
       " {'text': 'tak', 'start': 2.84, 'end': 2.85},\n",
       " {'text': 'suka', 'start': 2.96, 'end': 3.17},\n",
       " {'text': 'mandi', 'start': 3.2, 'end': 3.41},\n",
       " {'text': 'ketat', 'start': 4.6, 'end': 4.77},\n",
       " {'text': 'saya', 'start': 4.88, 'end': 5.09},\n",
       " {'text': 'masam', 'start': 5.32, 'end': 5.61}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_indices(words, indices, r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4557e700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output-m-squeezeformer/model.ckpt'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'output-m-squeezeformer/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f6e373ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_410014/399315655.py:4: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['X_placeholder',\n",
       " 'X_len_placeholder',\n",
       " 'padded_features',\n",
       " 'padded_lens',\n",
       " 'conformer_encoder_block_0_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_0_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_0_layer1_ff_scale',\n",
       " 'conformer_encoder_block_0_layer1_ff_bias',\n",
       " 'conformer_encoder_block_0_layer3_conv_scale',\n",
       " 'conformer_encoder_block_0_layer3_conv_bias',\n",
       " 'conformer_encoder_block_0_layer3_ff_scale',\n",
       " 'conformer_encoder_block_0_layer3_ff_bias',\n",
       " 'conformer_encoder_block_1_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_1_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_1_layer1_ff_scale',\n",
       " 'conformer_encoder_block_1_layer1_ff_bias',\n",
       " 'conformer_encoder_block_1_layer3_conv_scale',\n",
       " 'conformer_encoder_block_1_layer3_conv_bias',\n",
       " 'conformer_encoder_block_1_layer3_ff_scale',\n",
       " 'conformer_encoder_block_1_layer3_ff_bias',\n",
       " 'conformer_encoder_block_2_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_2_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_2_layer1_ff_scale',\n",
       " 'conformer_encoder_block_2_layer1_ff_bias',\n",
       " 'conformer_encoder_block_2_layer3_conv_scale',\n",
       " 'conformer_encoder_block_2_layer3_conv_bias',\n",
       " 'conformer_encoder_block_2_layer3_ff_scale',\n",
       " 'conformer_encoder_block_2_layer3_ff_bias',\n",
       " 'conformer_encoder_block_3_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_3_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_3_layer1_ff_scale',\n",
       " 'conformer_encoder_block_3_layer1_ff_bias',\n",
       " 'conformer_encoder_block_3_layer3_conv_scale',\n",
       " 'conformer_encoder_block_3_layer3_conv_bias',\n",
       " 'conformer_encoder_block_3_layer3_ff_scale',\n",
       " 'conformer_encoder_block_3_layer3_ff_bias',\n",
       " 'conformer_encoder_block_4_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_4_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_4_layer1_ff_scale',\n",
       " 'conformer_encoder_block_4_layer1_ff_bias',\n",
       " 'conformer_encoder_block_4_layer3_conv_scale',\n",
       " 'conformer_encoder_block_4_layer3_conv_bias',\n",
       " 'conformer_encoder_block_4_layer3_ff_scale',\n",
       " 'conformer_encoder_block_4_layer3_ff_bias',\n",
       " 'conformer_encoder_block_5_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_5_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_5_layer1_ff_scale',\n",
       " 'conformer_encoder_block_5_layer1_ff_bias',\n",
       " 'conformer_encoder_block_5_layer3_conv_scale',\n",
       " 'conformer_encoder_block_5_layer3_conv_bias',\n",
       " 'conformer_encoder_block_5_layer3_ff_scale',\n",
       " 'conformer_encoder_block_5_layer3_ff_bias',\n",
       " 'conformer_encoder_block_6_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_6_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_6_layer1_ff_scale',\n",
       " 'conformer_encoder_block_6_layer1_ff_bias',\n",
       " 'conformer_encoder_block_6_layer3_conv_scale',\n",
       " 'conformer_encoder_block_6_layer3_conv_bias',\n",
       " 'conformer_encoder_block_6_layer3_ff_scale',\n",
       " 'conformer_encoder_block_6_layer3_ff_bias',\n",
       " 'conformer_encoder_block_7_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_7_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_7_layer1_ff_scale',\n",
       " 'conformer_encoder_block_7_layer1_ff_bias',\n",
       " 'conformer_encoder_block_7_layer3_conv_scale',\n",
       " 'conformer_encoder_block_7_layer3_conv_bias',\n",
       " 'conformer_encoder_block_7_layer3_ff_scale',\n",
       " 'conformer_encoder_block_7_layer3_ff_bias',\n",
       " 'conformer_encoder_block_8_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_8_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_8_layer1_ff_scale',\n",
       " 'conformer_encoder_block_8_layer1_ff_bias',\n",
       " 'conformer_encoder_block_8_layer3_conv_scale',\n",
       " 'conformer_encoder_block_8_layer3_conv_bias',\n",
       " 'conformer_encoder_block_8_layer3_ff_scale',\n",
       " 'conformer_encoder_block_8_layer3_ff_bias',\n",
       " 'conformer_encoder_block_9_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_9_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_9_layer1_ff_scale',\n",
       " 'conformer_encoder_block_9_layer1_ff_bias',\n",
       " 'conformer_encoder_block_9_layer3_conv_scale',\n",
       " 'conformer_encoder_block_9_layer3_conv_bias',\n",
       " 'conformer_encoder_block_9_layer3_ff_scale',\n",
       " 'conformer_encoder_block_9_layer3_ff_bias',\n",
       " 'conformer_encoder_block_10_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_10_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_10_layer1_ff_scale',\n",
       " 'conformer_encoder_block_10_layer1_ff_bias',\n",
       " 'conformer_encoder_block_10_layer3_conv_scale',\n",
       " 'conformer_encoder_block_10_layer3_conv_bias',\n",
       " 'conformer_encoder_block_10_layer3_ff_scale',\n",
       " 'conformer_encoder_block_10_layer3_ff_bias',\n",
       " 'conformer_encoder_block_11_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_11_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_11_layer1_ff_scale',\n",
       " 'conformer_encoder_block_11_layer1_ff_bias',\n",
       " 'conformer_encoder_block_11_layer3_conv_scale',\n",
       " 'conformer_encoder_block_11_layer3_conv_bias',\n",
       " 'conformer_encoder_block_11_layer3_ff_scale',\n",
       " 'conformer_encoder_block_11_layer3_ff_bias',\n",
       " 'conformer_encoder_block_12_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_12_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_12_layer1_ff_scale',\n",
       " 'conformer_encoder_block_12_layer1_ff_bias',\n",
       " 'conformer_encoder_block_12_layer3_conv_scale',\n",
       " 'conformer_encoder_block_12_layer3_conv_bias',\n",
       " 'conformer_encoder_block_12_layer3_ff_scale',\n",
       " 'conformer_encoder_block_12_layer3_ff_bias',\n",
       " 'conformer_encoder_block_13_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_13_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_13_layer1_ff_scale',\n",
       " 'conformer_encoder_block_13_layer1_ff_bias',\n",
       " 'conformer_encoder_block_13_layer3_conv_scale',\n",
       " 'conformer_encoder_block_13_layer3_conv_bias',\n",
       " 'conformer_encoder_block_13_layer3_ff_scale',\n",
       " 'conformer_encoder_block_13_layer3_ff_bias',\n",
       " 'conformer_encoder_block_14_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_14_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_14_layer1_ff_scale',\n",
       " 'conformer_encoder_block_14_layer1_ff_bias',\n",
       " 'conformer_encoder_block_14_layer3_conv_scale',\n",
       " 'conformer_encoder_block_14_layer3_conv_bias',\n",
       " 'conformer_encoder_block_14_layer3_ff_scale',\n",
       " 'conformer_encoder_block_14_layer3_ff_bias',\n",
       " 'conformer_encoder_block_15_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_15_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_15_layer1_ff_scale',\n",
       " 'conformer_encoder_block_15_layer1_ff_bias',\n",
       " 'conformer_encoder_block_15_layer3_conv_scale',\n",
       " 'conformer_encoder_block_15_layer3_conv_bias',\n",
       " 'conformer_encoder_block_15_layer3_ff_scale',\n",
       " 'conformer_encoder_block_15_layer3_ff_bias',\n",
       " 'conformer_encoder_block_16_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_16_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_16_layer1_ff_scale',\n",
       " 'conformer_encoder_block_16_layer1_ff_bias',\n",
       " 'conformer_encoder_block_16_layer3_conv_scale',\n",
       " 'conformer_encoder_block_16_layer3_conv_bias',\n",
       " 'conformer_encoder_block_16_layer3_ff_scale',\n",
       " 'conformer_encoder_block_16_layer3_ff_bias',\n",
       " 'conformer_encoder_block_17_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_17_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_17_layer1_ff_scale',\n",
       " 'conformer_encoder_block_17_layer1_ff_bias',\n",
       " 'conformer_encoder_block_17_layer3_conv_scale',\n",
       " 'conformer_encoder_block_17_layer3_conv_bias',\n",
       " 'conformer_encoder_block_17_layer3_ff_scale',\n",
       " 'conformer_encoder_block_17_layer3_ff_bias',\n",
       " 'conformer_encoder_block_18_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_18_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_18_layer1_ff_scale',\n",
       " 'conformer_encoder_block_18_layer1_ff_bias',\n",
       " 'conformer_encoder_block_18_layer3_conv_scale',\n",
       " 'conformer_encoder_block_18_layer3_conv_bias',\n",
       " 'conformer_encoder_block_18_layer3_ff_scale',\n",
       " 'conformer_encoder_block_18_layer3_ff_bias',\n",
       " 'conformer_encoder_block_19_layer1_mhsa_scale',\n",
       " 'conformer_encoder_block_19_layer1_mhsa_bias',\n",
       " 'conformer_encoder_block_19_layer1_ff_scale',\n",
       " 'conformer_encoder_block_19_layer1_ff_bias',\n",
       " 'conformer_encoder_block_19_layer3_conv_scale',\n",
       " 'conformer_encoder_block_19_layer3_conv_bias',\n",
       " 'conformer_encoder_block_19_layer3_ff_scale',\n",
       " 'conformer_encoder_block_19_layer3_ff_bias',\n",
       " 'transducer/transducer_prediction/transducer_prediction_embedding/embeddings',\n",
       " 'greedy_decoder',\n",
       " 'encoded',\n",
       " 'encoded_placeholder',\n",
       " 'predicted_placeholder',\n",
       " 'states_placeholder',\n",
       " 'ytu',\n",
       " 'new_states',\n",
       " 'initial_states',\n",
       " 'non_blank_transcript',\n",
       " 'non_blank_stime']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'gather' in n.op.lower()\n",
    "        or 'placeholder' in n.name\n",
    "        or 'encoded' in n.name\n",
    "        or 'decoder' in n.name\n",
    "        or 'ytu' in n.name\n",
    "        or 'new_states' in n.name\n",
    "        or 'padded_' in n.name\n",
    "        or 'initial_states' in n.name\n",
    "        or 'non_blank' in n.name)\n",
    "        and 'adam' not in n.name\n",
    "        and 'global_step' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "        and 'ReadVariableOp' not in n.name\n",
    "        and 'Gather' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "501d7d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e388745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_410014/3042836824.py:3: The name tf.gfile.Exists is deprecated. Please use tf.io.gfile.exists instead.\n",
      "\n",
      "WARNING:tensorflow:From /tmp/ipykernel_410014/3042836824.py:16: The name tf.train.import_meta_graph is deprecated. Please use tf.compat.v1.train.import_meta_graph instead.\n",
      "\n",
      "INFO:tensorflow:Restoring parameters from output-m-squeezeformer/model.ckpt\n",
      "INFO:tensorflow:Froze 887 variables.\n",
      "INFO:tensorflow:Converted 887 variables to const ops.\n",
      "WARNING:tensorflow:From /tmp/ipykernel_410014/3042836824.py:25: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "40496 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('output-m-squeezeformer', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c52a5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "                \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68f041fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_410014/1406032803.py:3: The name tf.GraphDef is deprecated. Please use tf.compat.v1.GraphDef instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = load_graph('output-m-squeezeformer/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa5d4973",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = [\n",
    "    'X_placeholder',\n",
    "    'X_len_placeholder',\n",
    "    'encoded_placeholder',\n",
    "    'predicted_placeholder',\n",
    "    'states_placeholder',\n",
    "]\n",
    "output_nodes = [\n",
    "    'greedy_decoder',\n",
    "    'encoded',\n",
    "    'ytu',\n",
    "    'new_states',\n",
    "    'padded_features',\n",
    "    'padded_lens',\n",
    "    'initial_states',\n",
    "    'non_blank_transcript',\n",
    "    'non_blank_stime'\n",
    "]\n",
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a0133cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess = tf.Session(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9b28c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "126a85c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helo nama saya mesin saya tak suka mandi ketat saya masam\n",
      "helo nama saya husin saya suka mandi saya mandi setiap hari\n",
      "ini dan melalui kenyataan mesej itu mastura menegaskan\n",
      "pilihan tepat apabila dia kini lebih berani dan\n",
      "testing nama saya husin bin zulkari\n",
      "sebut perkataan uncle\n",
      "tolong sebut antikata\n",
      "nama saya syafiqa hidayah\n",
      "jadi dalam perjalanan ini dunia yang susah ini ketika nabi mengajar muaz bin jabal tadi ni allahumma in\n"
     ]
    }
   ],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e8172a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_, padded_lens_, s  = test_sess.run([outputs['encoded'], outputs['padded_lens'], outputs['initial_states']], \n",
    "                                        feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                     inputs['X_len_placeholder']: lens})\n",
    "\n",
    "padded_lens_ = padded_lens_ // model.encoder.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f4cd2119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helo nama saya mesin saya tak suka mandi ketat saya masam'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "r = transducer(\n",
    "    enc = encoded_[i],\n",
    "    total = padded_lens_[i],\n",
    "    initial_states = s,\n",
    "    encoded_placeholder = inputs['encoded_placeholder'],\n",
    "    predicted_placeholder = inputs['predicted_placeholder'],\n",
    "    states_placeholder = inputs['states_placeholder'],\n",
    "    ytu = outputs['ytu'],\n",
    "    new_states = outputs['new_states'],\n",
    "    sess = test_sess,\n",
    "    beam_width = 1,\n",
    ")\n",
    "\n",
    "malaya_speech.subword.decode(subwords, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bbf54424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.graph_transforms import TransformGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c9bdf69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-17 05:43:46.536364: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying add_default_attributes\n",
      "2022-08-17 05:43:46.770995: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying remove_nodes\n",
      "2022-08-17 05:43:46.987655: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_features\n",
      "2022-08-17 05:43:46.987692: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_lens\n",
      "2022-08-17 05:43:47.009995: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for greedy_decoder\n",
      "2022-08-17 05:43:47.025194: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for encoded\n",
      "2022-08-17 05:43:47.025579: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for ytu\n",
      "2022-08-17 05:43:47.025599: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for new_states\n",
      "2022-08-17 05:43:47.025637: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for initial_states\n",
      "2022-08-17 05:43:47.044483: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_transcript\n",
      "2022-08-17 05:43:47.044517: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_stime\n",
      "2022-08-17 05:43:47.387590: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_features\n",
      "2022-08-17 05:43:47.387633: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_lens\n",
      "2022-08-17 05:43:47.409318: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for greedy_decoder\n",
      "2022-08-17 05:43:47.435848: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for encoded\n",
      "2022-08-17 05:43:47.436274: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for ytu\n",
      "2022-08-17 05:43:47.436295: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for new_states\n",
      "2022-08-17 05:43:47.436332: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for initial_states\n",
      "2022-08-17 05:43:47.454980: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_transcript\n",
      "2022-08-17 05:43:47.455022: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_stime\n",
      "2022-08-17 05:43:47.819727: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_features\n",
      "2022-08-17 05:43:47.819780: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_lens\n",
      "2022-08-17 05:43:47.843544: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for greedy_decoder\n",
      "2022-08-17 05:43:47.860528: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for encoded\n",
      "2022-08-17 05:43:47.860867: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for ytu\n",
      "2022-08-17 05:43:47.860888: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for new_states\n",
      "2022-08-17 05:43:47.860924: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for initial_states\n",
      "2022-08-17 05:43:47.876820: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_transcript\n",
      "2022-08-17 05:43:47.876846: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_stime\n",
      "2022-08-17 05:43:48.221574: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_features\n",
      "2022-08-17 05:43:48.221627: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_lens\n",
      "2022-08-17 05:43:48.242626: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for greedy_decoder\n",
      "2022-08-17 05:43:48.257650: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for encoded\n",
      "2022-08-17 05:43:48.257973: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for ytu\n",
      "2022-08-17 05:43:48.257993: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for new_states\n",
      "2022-08-17 05:43:48.258052: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for initial_states\n",
      "2022-08-17 05:43:48.272042: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_transcript\n",
      "2022-08-17 05:43:48.272067: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_stime\n",
      "2022-08-17 05:43:48.636823: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_features\n",
      "2022-08-17 05:43:48.636866: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for padded_lens\n",
      "2022-08-17 05:43:48.656842: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for greedy_decoder\n",
      "2022-08-17 05:43:48.671534: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for encoded\n",
      "2022-08-17 05:43:48.671843: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for ytu\n",
      "2022-08-17 05:43:48.671863: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for new_states\n",
      "2022-08-17 05:43:48.671897: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for initial_states\n",
      "2022-08-17 05:43:48.685800: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_transcript\n",
      "2022-08-17 05:43:48.685822: I tensorflow/tools/graph_transforms/remove_nodes.cc:78] Skipping replacement for non_blank_stime\n",
      "2022-08-17 05:43:49.895710: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying fold_batch_norms\n",
      "2022-08-17 05:43:50.463730: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying fold_old_batch_norms\n",
      "2022-08-17 05:43:52.041661: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying quantize_weights\n",
      "2022-08-17 05:43:53.168683: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying strip_unused_nodes\n",
      "2022-08-17 05:43:53.597096: I tensorflow/tools/graph_transforms/transform_graph.cc:318] Applying sort_by_execution_order\n"
     ]
    }
   ],
   "source": [
    "transforms = ['add_default_attributes',\n",
    "             'remove_nodes(op=Identity, op=CheckNumerics, op=Dropout)',\n",
    "             'fold_batch_norms',\n",
    "             'fold_old_batch_norms',\n",
    "             'quantize_weights(fallback_min=-10, fallback_max=10)',\n",
    "             'strip_unused_nodes',\n",
    "             'sort_by_execution_order']\n",
    "\n",
    "input_nodes = [\n",
    "    'X_placeholder',\n",
    "    'X_len_placeholder',\n",
    "    'encoded_placeholder',\n",
    "    'predicted_placeholder',\n",
    "    'states_placeholder',\n",
    "]\n",
    "output_nodes = [\n",
    "    'greedy_decoder',\n",
    "    'encoded',\n",
    "    'ytu',\n",
    "    'new_states',\n",
    "    'padded_features',\n",
    "    'padded_lens',\n",
    "    'initial_states',\n",
    "    'non_blank_transcript',\n",
    "    'non_blank_stime'\n",
    "]\n",
    "\n",
    "pb = 'output-m-squeezeformer/frozen_model.pb'\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.FastGFile(pb, 'rb') as f:\n",
    "    input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "transformed_graph_def = TransformGraph(input_graph_def, \n",
    "                                           input_nodes,\n",
    "                                           output_nodes, transforms)\n",
    "    \n",
    "with tf.gfile.GFile(f'{pb}.quantized', 'wb') as f:\n",
    "    f.write(transformed_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7bd57e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('output-m-squeezeformer/frozen_model.pb.quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69dae4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}\n",
    "test_sess = tf.Session(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f85783d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "450b273d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helo nama saya mesin saya tak suka mandi ketat saya masam\n",
      "helo nama saya husin saya suka mandi saya mandi setiap hari\n",
      "ini dan melalui kenyataan mesej itu mastura menegaskan\n",
      "pilihan tepat apabila dia kini lebih berani dan\n",
      "testing nama saya husin bin zulkari\n",
      "sebut perkataan uncle\n",
      "tolong sebut antikata\n",
      "nama saya syafiqa hidayah\n",
      "jadi dalam perjalanan ini dunia yang susah ini ketika nabi mengajar muaz bin jabal tadi ni allah mak\n"
     ]
    }
   ],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0f2e0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya_boilerplate.huggingface import upload_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b4a5c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output-m-squeezeformer/\n",
      "output-m-squeezeformer/frozen_model.pb.quantized\n",
      "output-m-squeezeformer/frozen_model.pb\n",
      "output-m-squeezeformer/model.ckpt.meta\n",
      "output-m-squeezeformer/checkpoint\n",
      "output-m-squeezeformer/model.ckpt.index\n",
      "output-m-squeezeformer/model.ckpt.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "!tar -cvf output-m-squeezeformer.tar output-m-squeezeformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a73a0963",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/tf-nvidia/lib/python3.8/site-packages/huggingface_hub/hf_api.py:91: FutureWarning: `name` and `organization` input arguments are deprecated and will be removed in v0.10. Pass `repo_id` instead.\n",
      "  warnings.warn(\n",
      "409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: NLnucOY8QqKCbzlje_s6w) - You already created this model repo\n"
     ]
    }
   ],
   "source": [
    "files_mapping = {'output-m-squeezeformer.tar': 'output-m-squeezeformer.tar'}\n",
    "upload_dict(model = 'pretrained-speech-to-text-transducer', files_mapping = files_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bfff0625",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_mapping = {'output-m-squeezeformer/frozen_model.pb': 'model.pb'}\n",
    "upload_dict(model = 'speech-to-text-transducer-m-squeezeformer', files_mapping = files_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a0995d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_mapping = {'output-m-squeezeformer/frozen_model.pb.quantized': 'model.pb'}\n",
    "upload_dict(model = 'speech-to-text-transducer-m-squeezeformer-quantized', files_mapping = files_mapping)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
