{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/pretrained/output-large-conformer-v4.tar.gz\n",
    "# !tar -zxf output-large-conformer-v4.tar.gz\n",
    "# !rm  output-large-conformer-v4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya_speech.train.model.conformer as conformer\n",
    "import malaya_speech.train.model.transducer as transducer\n",
    "import malaya_speech\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords = malaya_speech.subword.load('transducer.subword')\n",
    "vocabs = {i: subwords._id_to_subword(i) for i in range(subwords.vocab_size - 1)}\n",
    "vocabs[-1] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['say', 'a_', 'suk', 'a_', 'aya', b'm'], [33, 1, 468, 1, 138, 883])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = subwords.encode('saya suka ayam')\n",
    "[vocabs[e - 1] for e in encoded], encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "featurizer = malaya_speech.tf_featurization.STTFeaturizer(\n",
    "    normalize_per_feature = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.compat.v1.placeholder(tf.float32, [None, None], name = 'X_placeholder')\n",
    "X_len = tf.compat.v1.placeholder(tf.int32, [None], name = 'X_len_placeholder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'ExpandDims:0' shape=(?, ?, 80, 1) dtype=float32>,\n",
       " <tf.Tensor 'TensorArrayStack_2/TensorArrayGatherV3:0' shape=(?,) dtype=int32>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = tf.shape(X)[0]\n",
    "features = tf.TensorArray(dtype = tf.float32, size = batch_size, dynamic_size = True, infer_shape = False)\n",
    "features_len = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "\n",
    "init_state = (0, features, features_len)\n",
    "\n",
    "def condition(i, features, features_len):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, features, features_len):\n",
    "    f = featurizer(X[i, :X_len[i]])\n",
    "    f_len = tf.shape(f)[0]\n",
    "    return i + 1, features.write(i, f), features_len.write(i, f_len)\n",
    "\n",
    "_, features, features_len = tf.while_loop(condition, body, init_state)\n",
    "features_len = features_len.stack()\n",
    "padded_features = tf.TensorArray(dtype = tf.float32, size = batch_size)\n",
    "padded_lens = tf.TensorArray(dtype = tf.int32, size = batch_size)\n",
    "maxlen = tf.reduce_max(features_len)\n",
    "\n",
    "init_state = (0, padded_features, padded_lens)\n",
    "\n",
    "def condition(i, padded_features, padded_lens):\n",
    "    return i < batch_size\n",
    "\n",
    "def body(i, padded_features, padded_lens):\n",
    "    f = features.read(i)\n",
    "    len_f = tf.shape(f)[0]\n",
    "    f = tf.pad(f, [[0, maxlen - tf.shape(f)[0]], [0,0]])\n",
    "    return i + 1, padded_features.write(i, f), padded_lens.write(i, len_f)\n",
    "\n",
    "_, padded_features, padded_lens = tf.while_loop(condition, body, init_state)\n",
    "padded_features = padded_features.stack()\n",
    "padded_lens = padded_lens.stack()\n",
    "padded_lens.set_shape((None,))\n",
    "padded_features.set_shape((None, None, 80))\n",
    "padded_features = tf.expand_dims(padded_features, -1)\n",
    "padded_features, padded_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_features = tf.identity(padded_features, name = 'padded_features')\n",
    "padded_lens = tf.identity(padded_lens, name = 'padded_lens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = malaya_speech.config.conformer_large_encoder_config\n",
    "config['dropout'] = 0.0\n",
    "conformer_model = conformer.Model(**config)\n",
    "decoder_config = malaya_speech.config.conformer_large_decoder_config\n",
    "decoder_config['embed_dropout'] = 0.0\n",
    "transducer_model = transducer.rnn.Model(\n",
    "    conformer_model, vocabulary_size = subwords.vocab_size, **decoder_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = tf.compat.v1.placeholder(tf.int32, [None, None])\n",
    "z = tf.zeros((tf.shape(p)[0], 1),dtype=tf.int32)\n",
    "c = tf.concat([z, p], axis = 1)\n",
    "p_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/transducer/layer.py:37: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py:3994: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transducer/transducer_joint/transducer_joint_vocab/BiasAdd:0' shape=(?, ?, ?, 1030) dtype=float32>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = transducer_model([padded_features, c, p_len], training = training)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output-large-conformer/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_list)\n",
    "saver.restore(sess, 'output-large-conformer/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'while_2/Exit_1:0' shape=(?, ?) dtype=int32>,\n",
       " <tf.Tensor 'while_2/Exit_2:0' shape=(?, ?, 1030) dtype=float32>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = transducer_model.greedy_decoder(padded_features, padded_lens, training = training)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'greedy_decoder:0' shape=(?, ?) dtype=int32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_logits = tf.identity(decoded[1], name = 'greedy_decoder_logits')\n",
    "decoded = tf.identity(decoded[0], name = 'greedy_decoder')\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'ytu:0' shape=(1030,) dtype=float32>,\n",
       " <tf.Tensor 'new_states:0' shape=(1, 2, 1, 640) dtype=float32>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = transducer_model.encoder(padded_features, training = training)\n",
    "encoded = tf.identity(encoded, name = 'encoded')\n",
    "encoded_placeholder = tf.placeholder(tf.float32, [config['dmodel']], name = 'encoded_placeholder')\n",
    "predicted_placeholder = tf.placeholder(tf.int32, None, name = 'predicted_placeholder')\n",
    "t = transducer_model.predict_net.get_initial_state().shape\n",
    "states_placeholder = tf.placeholder(tf.float32, [int(i) for i in t], name = 'states_placeholder')\n",
    "\n",
    "ytu, new_states = transducer_model.decoder_inference(\n",
    "    encoded=encoded_placeholder,\n",
    "    predicted=predicted_placeholder,\n",
    "    states=states_placeholder,\n",
    "    training = training,\n",
    ")\n",
    "\n",
    "ytu = tf.identity(ytu, name = 'ytu')\n",
    "new_states = tf.identity(new_states, name = 'new_states')\n",
    "ytu, new_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_states = transducer_model.predict_net.get_initial_state()\n",
    "initial_states = tf.identity(initial_states, name = 'initial_states')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "# saver = tf.train.Saver(var_list = var_list)\n",
    "# saver.restore(sess, 'asr-small-conformer-transducer/model.ckpt-325000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    "    'speech/record/savewav_2020-11-26_22-36-06_294832.wav',\n",
    "    'speech/record/savewav_2020-11-26_22-40-56_929661.wav',\n",
    "    'speech/record/675.wav',\n",
    "    'speech/record/664.wav',\n",
    "    'speech/example-speaker/husein-zolkepli.wav',\n",
    "    'speech/example-speaker/mas-aisyah.wav',\n",
    "    'speech/example-speaker/khalil-nooh.wav',\n",
    "    'speech/example-speaker/shafiqah-idayu.wav',\n",
    "    'speech/khutbah/wadi-annuar.wav',\n",
    "]\n",
    "front_pad = 200\n",
    "back_pad = 2000\n",
    "inputs = [malaya_speech.load(f)[0] for f in files]\n",
    "padded, lens = malaya_speech.padding.sequence_1d(inputs, return_len = True)\n",
    "back = np.zeros(shape = (len(inputs), back_pad))\n",
    "front = np.zeros(shape = (len(inputs), front_pad))\n",
    "padded = np.concatenate([front, padded, back], axis = -1)\n",
    "lens = [l + front_pad + back_pad for l in lens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello nama saya send saya tak suka mandi ketat saya masam\n",
      "hello nama saya husin saya suka mandi saya mandi setiap hari\n",
      "ini dan melalui kenyataan mesej itu mastura menegaskan\n",
      "pilihan tepat apabila dia kini lebih berani dan\n",
      "testing nama saya husin bin zulkifli\n",
      "sebut perkataan angka\n",
      "tolong sebut antikata\n",
      "nama saya syafiqah idayu\n",
      "jadi dalam perjalanan ini dunia yang susah ini ketika nabi mengajar muadz bin jabal tadi ini allah maha ini\n",
      "CPU times: user 41.1 s, sys: 9.27 s, total: 50.4 s\n",
      "Wall time: 6.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = sess.run(decoded, feed_dict = {X: padded, X_len: lens})\n",
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.1 s, sys: 3.75 s, total: 52.8 s\n",
      "Wall time: 5.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "greedy, padded_lens_ = sess.run([decoded_logits, padded_lens], feed_dict = {X: padded, X_len: lens})\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ctc_decoders import Scorer\n",
    "# from ctc_decoders import ctc_beam_search_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_vocab = [subwords._id_to_subword(i) for i in range(subwords.vocab_size - 1)]\n",
    "# unique_vocab = [w.decode(encoding = 'ISO-8859-1') if isinstance(w, bytes) else w for w in unique_vocab]\n",
    "# unique_vocab = [w.replace('_', ' ') for w in unique_vocab]\n",
    "# len(unique_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scorer = Scorer(0.02, 1.0, 'out.trie.klm', unique_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = greedy[0,:padded_lens_[0]]\n",
    "# l = np.concatenate([l[:,1:], l[:,:1]], axis = 1)\n",
    "# l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.2 s, sys: 901 ms, total: 27.1 s\n",
      "Wall time: 4.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "encoded_, padded_lens_  = sess.run([encoded, padded_lens], feed_dict = {X: padded, X_len: lens})\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor\n",
    "s = sess.run(initial_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "BeamHypothesis = collections.namedtuple(\n",
    "    'BeamHypothesis', ('score', 'prediction', 'states')\n",
    ")\n",
    "\n",
    "def apply_temp(logits_BxN, temperature):\n",
    "    uniform_noise_BxN = np.random.uniform(size = logits_BxN.shape)\n",
    "    logits_BxN += -np.log(-np.log(uniform_noise_BxN)) * temperature\n",
    "    return logits_BxN\n",
    "\n",
    "def transducer(\n",
    "    enc,\n",
    "    total,\n",
    "    initial_states,\n",
    "    encoded_placeholder,\n",
    "    predicted_placeholder,\n",
    "    states_placeholder,\n",
    "    ytu,\n",
    "    new_states,\n",
    "    sess,\n",
    "    beam_width = 10,\n",
    "    temperature = 0.0,\n",
    "    score_norm = True,\n",
    "):\n",
    "    kept_hyps = [\n",
    "        BeamHypothesis(score = 0.0, prediction = [0], states = initial_states)\n",
    "    ]\n",
    "    B = kept_hyps\n",
    "    for i in range(total):\n",
    "        A = B\n",
    "        B = []\n",
    "        while True:\n",
    "            y_hat = max(A, key = lambda x: x.score)\n",
    "            A.remove(y_hat)\n",
    "            ytu_, new_states_ = sess.run(\n",
    "                [ytu, new_states],\n",
    "                feed_dict = {\n",
    "                    encoded_placeholder: enc[i],\n",
    "                    predicted_placeholder: y_hat.prediction[-1],\n",
    "                    states_placeholder: y_hat.states,\n",
    "                },\n",
    "            )\n",
    "            if temperature > 0:\n",
    "                ytu_ = apply_temp(ytu_, temperature = temperature)\n",
    "            top_k = ytu_[1:].argsort()[-beam_width:][::-1]\n",
    "            B.append(BeamHypothesis(\n",
    "                score = y_hat.score + ytu_[0],\n",
    "                prediction = y_hat.prediction,\n",
    "                states = y_hat.states,\n",
    "            ))\n",
    "            for k in top_k:\n",
    "                \n",
    "                beam_hyp = BeamHypothesis(\n",
    "                    score = y_hat.score + ytu_[k + 1],\n",
    "                    prediction = y_hat.prediction + [k + 1],\n",
    "                    states = new_states_,\n",
    "                )\n",
    "                A.append(beam_hyp)\n",
    "            \n",
    "            hyps_max = max(A, key=lambda x: x.score).score\n",
    "            kept_most_prob = sorted(\n",
    "                [hyp for hyp in B if hyp.score > hyps_max],\n",
    "                key=lambda x: x.score,\n",
    "            )\n",
    "            if len(kept_most_prob) >= beam_width:\n",
    "                B = kept_most_prob\n",
    "                break\n",
    "    \n",
    "    if score_norm:\n",
    "        B.sort(key=lambda x: x.score / len(x.prediction), reverse=True)\n",
    "    else:\n",
    "        B.sort(key=lambda x: x.score, reverse=True)\n",
    "    return B[0].prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# o = transducer(\n",
    "#     enc = encoded_[i],\n",
    "#     total = padded_lens_[i],\n",
    "#     initial_states = s,\n",
    "#     encoded_placeholder = encoded_placeholder,\n",
    "#     predicted_placeholder = predicted_placeholder,\n",
    "#     states_placeholder = states_placeholder,\n",
    "#     ytu = ytu,\n",
    "#     new_states = new_states,\n",
    "#     sess = sess,\n",
    "#     beam_width = 2,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# malaya_speech.subword.decode(subwords, o[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello nama saya mesin saya tak suka mandi ketat saya memasang\n",
      "helo nama saya husin saya suka mandi saya mandi setiap hari\n",
      "ini dan melalui kenyataan mesej itu mastura menegaskan\n",
      "pilihan tepat apabila dia kini lebih berani dan\n",
      "testing nama saya husin bin zulkifli\n",
      "sebut perkataan angka\n",
      "tolong sebut antikata\n",
      "nama saya syafiqah idayu\n",
      "jadi dalam perjalanan ini dunia yang susah ini ketika nabi mengajar muadz bin jabal tadi ini allah maha ini\n",
      "CPU times: user 2min 14s, sys: 14.9 s, total: 2min 29s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for i in range(len(encoded_)):\n",
    "    r = transducer(\n",
    "        enc = encoded_[i],\n",
    "        total = padded_lens_[i],\n",
    "        initial_states = s,\n",
    "        encoded_placeholder = encoded_placeholder,\n",
    "        predicted_placeholder = predicted_placeholder,\n",
    "        states_placeholder = states_placeholder,\n",
    "        ytu = ytu,\n",
    "        new_states = new_states,\n",
    "        sess = sess,\n",
    "        beam_width = 10,\n",
    "    )\n",
    "    print(malaya_speech.subword.decode(subwords, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# for i in range(len(encoded_)):\n",
    "#     r = transducer(\n",
    "#         enc = encoded_[i],\n",
    "#         total = padded_lens_[i],\n",
    "#         initial_states = s,\n",
    "#         encoded_placeholder = encoded_placeholder,\n",
    "#         predicted_placeholder = predicted_placeholder,\n",
    "#         states_placeholder = states_placeholder,\n",
    "#         ytu = ytu,\n",
    "#         new_states = new_states,\n",
    "#         sess = sess,\n",
    "#         beam_width = 10,\n",
    "#         temperature = 0.1\n",
    "#     )\n",
    "#     print(malaya_speech.subword.decode(subwords, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malaya-speech-model/language-model/redape-community/model.trie.klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kenlm\n",
    "\n",
    "kenlm_model = kenlm.Model('out.trie.klm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode.language_model import LanguageModel\n",
    "\n",
    "language_model = LanguageModel(kenlm_model, alpha = 0.01, beta = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "import heapq\n",
    "from typing import Any, Collection, Dict, Iterable, List, Optional, Tuple, Union\n",
    "\n",
    "LMState = Optional[Union[kenlm.State, List[kenlm.State]]]\n",
    "\n",
    "BeamHypothesis_LM = collections.namedtuple(\n",
    "    'BeamHypothesis_LM', ('score', 'prediction', 'states', 'text', 'next_word', 'word_part')\n",
    ")\n",
    "\n",
    "def _sort_and_trim_beams(beams: list, beam_width: int) -> list:\n",
    "    return heapq.nlargest(beam_width, beams, key=lambda x: x.score)\n",
    "\n",
    "def prune_space(string):\n",
    "    return re.sub(r'[ ]+', ' ', string).strip()\n",
    "\n",
    "def _merge_tokens(token_1, token_2) -> str:\n",
    "    if len(token_2) == 0:\n",
    "        text = token_1\n",
    "    elif len(token_1) == 0:\n",
    "        text = token_2\n",
    "    else:\n",
    "        text = token_1 + \" \" + token_2\n",
    "    return prune_space(text)\n",
    "\n",
    "def _prune_history(beams, lm_order: int):\n",
    "    \"\"\"Filter out beams that are the same over max_ngram history.\n",
    "    Since n-gram language models have a finite history when scoring a new token, we can use that\n",
    "    fact to prune beams that only differ early on (more than n tokens in the past) and keep only the\n",
    "    higher scoring ones. Note that this helps speed up the decoding process but comes at the cost of\n",
    "    some amount of beam diversity. If more than the top beam is used in the output it should\n",
    "    potentially be disabled.\n",
    "    \"\"\"\n",
    "    # let's keep at least 1 word of history\n",
    "    min_n_history = max(1, lm_order - 1)\n",
    "    seen_hashes = set()\n",
    "    filtered_beams = []\n",
    "    # for each beam after this, check if we need to add it\n",
    "    for beam in beams:\n",
    "        text = beam.text\n",
    "        next_word = beam.next_word\n",
    "        word_part = beam.word_part\n",
    "        last_char = beam.prediction[-1]\n",
    "        logit_score = beam.score\n",
    "        hash_idx = (tuple(text.split()[-min_n_history:]), word_part, last_char)\n",
    "        if hash_idx not in seen_hashes:\n",
    "            beam_hyp = BeamHypothesis_LM(\n",
    "                score = logit_score,\n",
    "                prediction = beam.prediction,\n",
    "                states = beam.states,\n",
    "                text = text,\n",
    "                next_word = next_word,\n",
    "                word_part = word_part\n",
    "            )\n",
    "            filtered_beams.append(beam_hyp)\n",
    "            seen_hashes.add(hash_idx)\n",
    "    return filtered_beams\n",
    "\n",
    "def get_lm_beams(beams, cached_lm_scores,\n",
    "        cached_partial_token_scores,\n",
    "        is_eos: bool = False):\n",
    "    \n",
    "    new_beams = []\n",
    "    for beam in beams:\n",
    "        text = beam.text\n",
    "        next_word = beam.next_word\n",
    "        word_part = beam.word_part\n",
    "        last_char = beam.prediction[-1]\n",
    "        logit_score = beam.score\n",
    "        new_text = _merge_tokens(text, next_word)\n",
    "        if new_text not in cached_lm_scores:\n",
    "            _, prev_raw_lm_score, start_state = cached_lm_scores[text]\n",
    "            score, end_state = language_model.score(start_state, next_word, is_last_word=is_eos)\n",
    "            raw_lm_score = prev_raw_lm_score + score\n",
    "            lm_hw_score = raw_lm_score\n",
    "            cached_lm_scores[new_text] = (lm_hw_score, raw_lm_score, end_state)\n",
    "        lm_score, _, _ = cached_lm_scores[new_text]\n",
    "        if len(word_part) > 0:\n",
    "            if word_part not in cached_partial_token_scores:\n",
    "                cached_partial_token_scores[word_part] = language_model.score_partial_token(\n",
    "                    word_part\n",
    "                )\n",
    "            lm_score += cached_partial_token_scores[word_part]\n",
    "        beam_hyp = BeamHypothesis_LM(\n",
    "            score = logit_score + lm_score,\n",
    "            prediction = beam.prediction,\n",
    "            states = beam.states,\n",
    "            text = new_text,\n",
    "            next_word = '',\n",
    "            word_part = word_part\n",
    "        )\n",
    "        new_beams.append(beam_hyp)\n",
    "\n",
    "    return new_beams\n",
    "            \n",
    "        \n",
    "def transducer_lm(\n",
    "    enc,\n",
    "    total,\n",
    "    initial_states,\n",
    "    encoded_placeholder,\n",
    "    predicted_placeholder,\n",
    "    states_placeholder,\n",
    "    ytu,\n",
    "    new_states,\n",
    "    sess,\n",
    "    beam_width = 10,\n",
    "    token_min_logp = -20.0,\n",
    "    beam_prune_logp = -1.0,\n",
    "    temperature = 0.0,\n",
    "):\n",
    "    kept_hyps = [\n",
    "        BeamHypothesis_LM(score = 0.0, prediction = [0], states = initial_states, text = '', next_word = '',\n",
    "                      word_part = '')\n",
    "    ]\n",
    "    start_state = kenlm.State()\n",
    "    cached_lm_scores: Dict[str, Tuple[float, float, LMState]] = {\n",
    "        '': (0.0, 0.0, language_model.get_start_state())\n",
    "    }\n",
    "    cached_p_lm_scores: Dict[str, float] = {}\n",
    "    B = kept_hyps\n",
    "    for i in range(total):\n",
    "        A = B\n",
    "        B = []\n",
    "        while True:\n",
    "            y_hat = max(A, key = lambda x: x.score)\n",
    "            A.remove(y_hat)\n",
    "            ytu_, new_states_ = sess.run(\n",
    "                [ytu, new_states],\n",
    "                feed_dict = {\n",
    "                    encoded_placeholder: enc[i],\n",
    "                    predicted_placeholder: y_hat.prediction[-1],\n",
    "                    states_placeholder: y_hat.states,\n",
    "                },\n",
    "            )\n",
    "            ytu_ = np.log(ytu_)\n",
    "            if temperature > 0:\n",
    "                ytu_ = apply_temp(ytu_, temperature=temperature)\n",
    "            # ytu_ = np.clip(ytu_, np.log(1e-15), 0)\n",
    "            B.append(BeamHypothesis_LM(\n",
    "                score=y_hat.score + ytu_[0],\n",
    "                prediction=y_hat.prediction,\n",
    "                states=y_hat.states,\n",
    "                text=y_hat.text,\n",
    "                next_word=y_hat.next_word,\n",
    "                word_part=y_hat.word_part,\n",
    "            ))\n",
    "            ytu_ = ytu_[1:]\n",
    "            max_idx = ytu_.argmax()\n",
    "            idx_list = set(np.where(ytu_ >= token_min_logp)[0]) | {max_idx}\n",
    "            for k in idx_list:\n",
    "                w = vocabs[k]\n",
    "                if isinstance(w, bytes):\n",
    "                    w = w.decode(encoding = 'ISO-8859-1')\n",
    "                w = w.replace('_', ' ')\n",
    "                s = y_hat.score + ytu_[k]\n",
    "                p = y_hat.prediction + [k + 1]\n",
    "            \n",
    "                if w[-1] == ' ':\n",
    "                    beam_hyp = BeamHypothesis_LM(\n",
    "                        score = s,\n",
    "                        prediction = p,\n",
    "                        states = new_states_,\n",
    "                        text = y_hat.text,\n",
    "                        next_word = y_hat.word_part + w,\n",
    "                        word_part = ''\n",
    "                    )\n",
    "                else:\n",
    "                    beam_hyp = BeamHypothesis_LM(\n",
    "                        score = s,\n",
    "                        prediction = p,\n",
    "                        states = new_states_,\n",
    "                        text = y_hat.text,\n",
    "                        next_word = y_hat.next_word,\n",
    "                        word_part = y_hat.word_part + w\n",
    "                    )\n",
    "                A.append(beam_hyp)\n",
    "            \n",
    "            A = get_lm_beams(A, cached_lm_scores, cached_p_lm_scores)\n",
    "            max_A = max(A, key = lambda x: x.score)\n",
    "            max_score = max_A.score\n",
    "            A = [b for b in A if b.score >= max_score + beam_prune_logp]\n",
    "            A = _sort_and_trim_beams(A, beam_width = beam_width)\n",
    "            A = _prune_history(A, lm_order=language_model.order)\n",
    "            \n",
    "            max_A = max(A, key=lambda x: x.score)\n",
    "            hyps_max = max_A.score\n",
    "            kept_most_prob = sorted(\n",
    "                [hyp for hyp in B if hyp.score > hyps_max],\n",
    "                key=lambda x: x.score,\n",
    "            )\n",
    "            if len(kept_most_prob) > 0:\n",
    "                B = kept_most_prob\n",
    "                break\n",
    "            \n",
    "\n",
    "    new_beams = []\n",
    "    for beam in B:\n",
    "        text = beam.text\n",
    "        next_word = beam.next_word\n",
    "        word_part = beam.word_part\n",
    "        last_char = beam.prediction[-1]\n",
    "        logit_score = beam.score\n",
    "        beam_hyp = BeamHypothesis_LM(\n",
    "            score = logit_score,\n",
    "            prediction = beam.prediction,\n",
    "            states = beam.states,\n",
    "            text = text,\n",
    "            next_word = word_part,\n",
    "            word_part = ''\n",
    "        )\n",
    "        new_beams.append(beam_hyp)\n",
    "    A = get_lm_beams(new_beams, cached_lm_scores, cached_p_lm_scores,is_eos=True)\n",
    "    max_A = max(A, key = lambda x: x.score)\n",
    "    max_score = max_A.score\n",
    "    A = [b for b in A if b.score >= max_score + beam_prune_logp]\n",
    "    A = _sort_and_trim_beams(A, beam_width = beam_width)\n",
    "    return A[0].prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_model = LanguageModel(kenlm_model, alpha = 0.03, beta = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 8\n",
    "o = transducer_lm(\n",
    "    enc = encoded_[i],\n",
    "    total = padded_lens_[i],\n",
    "    initial_states = s,\n",
    "    encoded_placeholder = encoded_placeholder,\n",
    "    predicted_placeholder = predicted_placeholder,\n",
    "    states_placeholder = states_placeholder,\n",
    "    ytu = ytu,\n",
    "    new_states = new_states,\n",
    "    sess = sess,\n",
    "    beam_width = 3,\n",
    ")\n",
    "malaya_speech.subword.decode(subwords, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "encoded_, padded_lens_  = sess.run([encoded, padded_lens], feed_dict = {X: padded, X_len: lens})\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor\n",
    "s = sess.run(initial_states)\n",
    "\n",
    "for i in range(len(encoded_)):\n",
    "    r = transducer(\n",
    "        enc = encoded_[i],\n",
    "        total = padded_lens_[i],\n",
    "        initial_states = s,\n",
    "        encoded_placeholder = encoded_placeholder,\n",
    "        predicted_placeholder = predicted_placeholder,\n",
    "        states_placeholder = states_placeholder,\n",
    "        ytu = ytu,\n",
    "        new_states = new_states,\n",
    "        sess = sess,\n",
    "        beam_width = 1,\n",
    "    )\n",
    "\n",
    "    print(malaya_speech.subword.decode(subwords, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Hypothesis(index=<tf.Tensor 'while_3/Exit_1:0' shape=() dtype=int32>, prediction=<tf.Tensor 'TensorArrayStack_3/TensorArrayGatherV3:0' shape=(?,) dtype=int32>, states=<tf.Tensor 'while_3/Exit_3:0' shape=(1, 2, 1, 640) dtype=float32>, alignment=<tf.Tensor 'TensorArrayStack_4/TensorArrayGatherV3:0' shape=(?, 1030) dtype=float32>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = padded_lens // transducer_model.encoder.conv_subsampling.time_reduction_factor\n",
    "encoded = transducer_model.encoder(padded_features, training = training)\n",
    "g = transducer_model._perform_greedy(encoded[0], l[0],\n",
    "                                tf.constant(0, dtype = tf.int32),\n",
    "                                transducer_model.predict_net.get_initial_state())\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = g.prediction\n",
    "minus_one = -1 * tf.ones_like(indices, dtype=tf.int32)\n",
    "blank_like = 0 * tf.ones_like(indices, dtype=tf.int32)\n",
    "indices = tf.where(indices == minus_one, blank_like, indices)\n",
    "num_samples = tf.cast(X_len[0], dtype=tf.float32)\n",
    "total_time_reduction_factor = featurizer.frame_step\n",
    "stime = tf.range(0, num_samples, delta=total_time_reduction_factor, dtype=tf.float32)\n",
    "stime /= tf.cast(featurizer.sample_rate, dtype=tf.float32)\n",
    "stime = stime[::tf.shape(stime)[0] // tf.shape(indices)[0]]\n",
    "stime.set_shape((None,))\n",
    "non_blank = tf.where(tf.not_equal(indices, 0))\n",
    "non_blank_transcript = tf.gather_nd(indices, non_blank)\n",
    "non_blank_stime = tf.gather_nd(stime, non_blank)\n",
    "non_blank_transcript = tf.identity(non_blank_transcript, name = 'non_blank_transcript')\n",
    "non_blank_stime = tf.identity(non_blank_stime, name = 'non_blank_stime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.1 s, sys: 1.5 s, total: 30.6 s\n",
      "Wall time: 5.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "r = sess.run([non_blank_transcript, non_blank_stime], feed_dict = {X: padded, X_len: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, indices = [], []\n",
    "for no, ids in enumerate(r[0]):\n",
    "    w = subwords._id_to_subword(ids - 1)\n",
    "    if type(w) == bytes:\n",
    "        w = w.decode()\n",
    "    words.extend([w, None])\n",
    "    indices.extend([no, None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "from malaya_speech.utils import text_encoder\n",
    "\n",
    "def _trim_underscore_and_tell(token):\n",
    "    if token.endswith('_'):\n",
    "        return token[:-1], True\n",
    "    return token, False\n",
    "\n",
    "def decode(ids):\n",
    "    ids = text_encoder.pad_decr(ids)\n",
    "    subword_ids = ids\n",
    "    del ids\n",
    "\n",
    "    subwords_ = []\n",
    "    prev_bytes = []\n",
    "    prev_ids = []\n",
    "    ids = []\n",
    "\n",
    "    def consume_prev_bytes():\n",
    "        if prev_bytes:\n",
    "            subwords_.extend(prev_bytes)\n",
    "            ids.extend(prev_ids)\n",
    "        return [], []\n",
    "\n",
    "    for no, subword_id in enumerate(subword_ids):\n",
    "        subword = subwords._id_to_subword(subword_id)\n",
    "        if isinstance(subword, six.binary_type):\n",
    "            # Byte-encoded\n",
    "            prev_bytes.append(subword.decode('utf-8', 'replace'))\n",
    "            if subword == b' ':\n",
    "                prev_ids.append(None)\n",
    "            else:\n",
    "                prev_ids.append(no)\n",
    "        else:\n",
    "            # If there were bytes previously, convert to unicode.\n",
    "            prev_bytes, prev_ids = consume_prev_bytes()\n",
    "            trimmed, add_space = _trim_underscore_and_tell(subword)\n",
    "            ids.append(no)\n",
    "            subwords_.append(trimmed)\n",
    "            if add_space:\n",
    "                subwords_.append(' ')\n",
    "                ids.append(None)\n",
    "    prev_bytes = consume_prev_bytes()\n",
    "\n",
    "    return subwords_, ids\n",
    "\n",
    "words, indices = decode(r[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_indices(subwords, ids, l, reduction_factor = 160, sample_rate = 16000):\n",
    "    result, temp_l, temp_r = [], [], []\n",
    "    for i in range(len(subwords)):\n",
    "        if ids[i] is not None:\n",
    "            temp_l.append(subwords[i])\n",
    "            temp_r.append(l[ids[i]])\n",
    "        else:\n",
    "            data = {'text': ''.join(temp_l), \n",
    "                    'start': round(temp_r[0],4), \n",
    "                    'end': round(temp_r[-1] + (reduction_factor / sample_rate), 4)}\n",
    "            result.append(data)\n",
    "            temp_l, temp_r = [], []\n",
    "    \n",
    "    if len(temp_l):\n",
    "        data = {'text': ''.join(temp_l), \n",
    "                'start': round(temp_r[0],4), \n",
    "                'end': round(temp_r[-1] + (reduction_factor / sample_rate), 4)}\n",
    "        result.append(data)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'hello', 'start': 0.0, 'end': 0.13},\n",
       " {'text': 'nama', 'start': 0.76, 'end': 0.97},\n",
       " {'text': 'saya', 'start': 1.0, 'end': 1.25},\n",
       " {'text': 'send', 'start': 1.6, 'end': 1.81},\n",
       " {'text': 'saya', 'start': 2.64, 'end': 2.73},\n",
       " {'text': 'tak', 'start': 2.8, 'end': 2.81},\n",
       " {'text': 'suka', 'start': 2.92, 'end': 3.09},\n",
       " {'text': 'mandi', 'start': 3.16, 'end': 3.37},\n",
       " {'text': 'ketat', 'start': 4.56, 'end': 4.73},\n",
       " {'text': 'saya', 'start': 4.84, 'end': 5.01},\n",
       " {'text': 'masam', 'start': 5.24, 'end': 5.53}]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_indices(words, indices, r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output-large-conformer-v2/model.ckpt'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'output-large-conformer-v2/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X_placeholder',\n",
       " 'X_len_placeholder',\n",
       " 'padded_features',\n",
       " 'padded_lens',\n",
       " 'transducer/transducer_prediction/transducer_prediction_embedding/embeddings',\n",
       " 'greedy_decoder_logits',\n",
       " 'greedy_decoder',\n",
       " 'encoded',\n",
       " 'encoded_placeholder',\n",
       " 'predicted_placeholder',\n",
       " 'states_placeholder',\n",
       " 'ytu',\n",
       " 'new_states',\n",
       " 'initial_states',\n",
       " 'non_blank_transcript',\n",
       " 'non_blank_stime']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'gather' in n.op.lower()\n",
    "        or 'placeholder' in n.name\n",
    "        or 'encoded' in n.name\n",
    "        or 'decoder' in n.name\n",
    "        or 'ytu' in n.name\n",
    "        or 'new_states' in n.name\n",
    "        or 'padded_' in n.name\n",
    "        or 'initial_states' in n.name\n",
    "        or 'non_blank' in n.name)\n",
    "        and 'adam' not in n.name\n",
    "        and 'global_step' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "        and 'ReadVariableOp' not in n.name\n",
    "        and 'Gather' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from output-large-conformer-v2/model.ckpt\n",
      "WARNING:tensorflow:From <ipython-input-46-9a7215a4e58a>:23: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 561 variables.\n",
      "INFO:tensorflow:Converted 561 variables to const ops.\n",
      "27434 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('output-large-conformer-v2', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "                \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('output-large-conformer/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = [\n",
    "    'X_placeholder',\n",
    "    'X_len_placeholder',\n",
    "    'encoded_placeholder',\n",
    "    'predicted_placeholder',\n",
    "    'states_placeholder',\n",
    "]\n",
    "output_nodes = [\n",
    "    'greedy_decoder',\n",
    "    'encoded',\n",
    "    'ytu',\n",
    "    'new_states',\n",
    "    'padded_features',\n",
    "    'padded_lens',\n",
    "    'initial_states',\n",
    "    'non_blank_transcript',\n",
    "    'non_blank_stime'\n",
    "]\n",
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess = tf.Session(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_, padded_lens_, s  = test_sess.run([outputs['encoded'], outputs['padded_lens'], outputs['initial_states']], \n",
    "                                        feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                     inputs['X_len_placeholder']: lens})\n",
    "\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "r = transducer(\n",
    "    enc = encoded_[i],\n",
    "    total = padded_lens_[i],\n",
    "    initial_states = s,\n",
    "    encoded_placeholder = inputs['encoded_placeholder'],\n",
    "    predicted_placeholder = inputs['predicted_placeholder'],\n",
    "    states_placeholder = inputs['states_placeholder'],\n",
    "    ytu = outputs['ytu'],\n",
    "    new_states = outputs['new_states'],\n",
    "    sess = test_sess,\n",
    "    beam_width = 1,\n",
    ")\n",
    "\n",
    "malaya_speech.subword.decode(subwords, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.graph_transforms import TransformGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-50-0f85278018f3>:31: FastGFile.__init__ (from tensorflow.python.platform.gfile) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.gfile.GFile.\n"
     ]
    }
   ],
   "source": [
    "transforms = ['add_default_attributes',\n",
    "             'remove_nodes(op=Identity, op=CheckNumerics, op=Dropout)',\n",
    "             'fold_batch_norms',\n",
    "             'fold_old_batch_norms',\n",
    "             'quantize_weights(fallback_min=-10, fallback_max=10)',\n",
    "             'strip_unused_nodes',\n",
    "             'sort_by_execution_order']\n",
    "\n",
    "input_nodes = [\n",
    "    'X_placeholder',\n",
    "    'X_len_placeholder',\n",
    "    'encoded_placeholder',\n",
    "    'predicted_placeholder',\n",
    "    'states_placeholder',\n",
    "]\n",
    "output_nodes = [\n",
    "    'greedy_decoder',\n",
    "    'encoded',\n",
    "    'ytu',\n",
    "    'new_states',\n",
    "    'padded_features',\n",
    "    'padded_lens',\n",
    "    'initial_states',\n",
    "    'non_blank_transcript',\n",
    "    'non_blank_stime'\n",
    "]\n",
    "\n",
    "pb = 'output-large-conformer-v2/frozen_model.pb'\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.FastGFile(pb, 'rb') as f:\n",
    "    input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "transformed_graph_def = TransformGraph(input_graph_def, \n",
    "                                           input_nodes,\n",
    "                                           output_nodes, transforms)\n",
    "    \n",
    "with tf.gfile.GFile(f'{pb}.quantized', 'wb') as f:\n",
    "    f.write(transformed_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = load_graph('output-base-conformer/frozen_model.pb.quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in input_nodes}\n",
    "outputs = {n: g.get_tensor_by_name(f'import/{n}:0') for n in output_nodes}\n",
    "test_sess = tf.Session(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test_sess.run(outputs['greedy_decoder'], feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                          inputs['X_len_placeholder']: lens})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in r:\n",
    "    print(malaya_speech.subword.decode(subwords, row[row > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_, padded_lens_, s  = test_sess.run([outputs['encoded'], outputs['padded_lens'], outputs['initial_states']], \n",
    "                                        feed_dict = {inputs['X_placeholder']: padded, \n",
    "                                                     inputs['X_len_placeholder']: lens})\n",
    "\n",
    "padded_lens_ = padded_lens_ // conformer_model.conv_subsampling.time_reduction_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 0\n",
    "# r = transducer(\n",
    "#     enc = encoded_[i],\n",
    "#     total = padded_lens_[i],\n",
    "#     initial_states = s,\n",
    "#     encoded_placeholder = inputs['encoded_placeholder'],\n",
    "#     predicted_placeholder = inputs['predicted_placeholder'],\n",
    "#     states_placeholder = inputs['states_placeholder'],\n",
    "#     ytu = outputs['ytu'],\n",
    "#     new_states = outputs['new_states'],\n",
    "#     sess = test_sess,\n",
    "#     beam_width = 1,\n",
    "# )\n",
    "\n",
    "# malaya_speech.subword.decode(subwords, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_application_key_id = os.environ['b2_application_key_id']\n",
    "b2_application_key = os.environ['b2_application_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from b2sdk.v1 import *\n",
    "info = InMemoryAccountInfo()\n",
    "b2_api = B2Api(info)\n",
    "application_key_id = b2_application_key_id\n",
    "application_key = b2_application_key\n",
    "b2_api.authorize_account(\"production\", application_key_id, application_key)\n",
    "file_info = {'how': 'good-file'}\n",
    "b2_bucket = b2_api.get_bucket_by_name('malaya-speech-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = 'output-large-conformer-v2'\n",
    "tar = 'output-large-conformer-v4-v2.tar.gz'\n",
    "os.system(f'tar -czvf {tar} {directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b2sdk.file_version.FileVersionInfo at 0x7f84070f76a0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outPutname = f'pretrained/{tar}'\n",
    "b2_bucket.upload_local_file(\n",
    "    local_file=tar,\n",
    "    file_name=outPutname,\n",
    "    file_infos=file_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm {tar}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b2sdk.file_version.FileVersionInfo at 0x7f84070b3320>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'output-large-conformer-v2/frozen_model.pb'\n",
    "outPutname = 'speech-to-text-transducer/large-conformer/model.pb'\n",
    "b2_bucket.upload_local_file(\n",
    "    local_file=file,\n",
    "    file_name=outPutname,\n",
    "    file_infos=file_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b2sdk.file_version.FileVersionInfo at 0x7f8407091198>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = 'output-large-conformer-v2/frozen_model.pb.quantized'\n",
    "outPutname = 'speech-to-text-transducer/large-conformer-quantized/model.pb'\n",
    "b2_bucket.upload_local_file(\n",
    "    local_file=file,\n",
    "    file_name=outPutname,\n",
    "    file_infos=file_info,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf output-large-conformer-v2 output-large-conformer-v4-v2.tar.gz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
