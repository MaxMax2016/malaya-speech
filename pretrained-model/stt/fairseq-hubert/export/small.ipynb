{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import HubertConfig, HubertPreTrainedModel, HubertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = HubertConfig.from_pretrained('facebook/hubert-base-ls960')\n",
    "config.num_attention_heads = 8\n",
    "config.hidden_size = 512\n",
    "config.intermediate_size = 2048\n",
    "config.num_hidden_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.save_pretrained('./hubert-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = '/home/husein/outputs/2022-11-06/20-10-17/checkpoints/checkpoint_5_700000.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading configuration file hubert-small/config.json\n",
      "Model config HubertConfig {\n",
      "  \"_name_or_path\": \"facebook/hubert-base-ls960\",\n",
      "  \"activation_dropout\": 0.1,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"HubertModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_proj_layer_norm\": true,\n",
      "  \"final_dropout\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 512,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 2048,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"model_type\": \"hubert\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"tokenizer_class\": \"Wav2Vec2CTCTokenizer\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32\n",
      "}\n",
      "\n",
      "2022-11-10 23:51:03 | INFO | fairseq.tasks.hubert_pretraining | current directory is /home/husein/dev/malaya-speech\n",
      "2022-11-10 23:51:03 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/home/husein/ssd1/speech-bahasa/wav2vec2-malay', 'fine_tuning': False, 'labels': ['km'], 'label_dir': '/home/husein/ssd1/mfcc-label', 'label_rate': 100.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2022-11-10 23:51:03 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 100.0, 'extractor_mode': default, 'encoder_layers': 6, 'encoder_embed_dim': 512, 'encoder_ffn_embed_dim': 2048, 'encoder_attention_heads': 8, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': True, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': False}\n",
      "2022-11-10 23:51:03 | INFO | __main__ |  was initialized from mask_emb.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | Feat extract conv layer 0 was initialized from feature_extractor.conv_layers.0.0.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | Feat extract layer norm weight of layer 0 was initialized from feature_extractor.conv_layers.0.2.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | Feat extract layer norm weight of layer 0 was initialized from feature_extractor.conv_layers.0.2.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | Feat extract conv layer 1 was initialized from feature_extractor.conv_layers.1.0.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | Feat extract conv layer 2 was initialized from feature_extractor.conv_layers.2.0.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | Feat extract conv layer 3 was initialized from feature_extractor.conv_layers.3.0.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | Feat extract conv layer 4 was initialized from feature_extractor.conv_layers.4.0.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | Feat extract conv layer 5 was initialized from feature_extractor.conv_layers.5.0.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | Feat extract conv layer 6 was initialized from feature_extractor.conv_layers.6.0.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.pos_conv_embed.conv.bias was initialized from encoder.pos_conv.0.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.pos_conv_embed.conv.weight_g was initialized from encoder.pos_conv.0.weight_g.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.pos_conv_embed.conv.weight_v was initialized from encoder.pos_conv.0.weight_v.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.attention.k_proj.weight was initialized from encoder.layers.0.self_attn.k_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.attention.k_proj.bias was initialized from encoder.layers.0.self_attn.k_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.attention.v_proj.weight was initialized from encoder.layers.0.self_attn.v_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.attention.v_proj.bias was initialized from encoder.layers.0.self_attn.v_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.attention.q_proj.weight was initialized from encoder.layers.0.self_attn.q_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.attention.q_proj.bias was initialized from encoder.layers.0.self_attn.q_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.attention.out_proj.weight was initialized from encoder.layers.0.self_attn.out_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.attention.out_proj.bias was initialized from encoder.layers.0.self_attn.out_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.layer_norm.weight was initialized from encoder.layers.0.self_attn_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.layer_norm.bias was initialized from encoder.layers.0.self_attn_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.feed_forward.intermediate_dense.weight was initialized from encoder.layers.0.fc1.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.feed_forward.intermediate_dense.bias was initialized from encoder.layers.0.fc1.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.feed_forward.output_dense.weight was initialized from encoder.layers.0.fc2.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.feed_forward.output_dense.bias was initialized from encoder.layers.0.fc2.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.final_layer_norm.weight was initialized from encoder.layers.0.final_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.0.final_layer_norm.bias was initialized from encoder.layers.0.final_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.attention.k_proj.weight was initialized from encoder.layers.1.self_attn.k_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.attention.k_proj.bias was initialized from encoder.layers.1.self_attn.k_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.attention.v_proj.weight was initialized from encoder.layers.1.self_attn.v_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.attention.v_proj.bias was initialized from encoder.layers.1.self_attn.v_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.attention.q_proj.weight was initialized from encoder.layers.1.self_attn.q_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.attention.q_proj.bias was initialized from encoder.layers.1.self_attn.q_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.attention.out_proj.weight was initialized from encoder.layers.1.self_attn.out_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.attention.out_proj.bias was initialized from encoder.layers.1.self_attn.out_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.layer_norm.weight was initialized from encoder.layers.1.self_attn_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.layer_norm.bias was initialized from encoder.layers.1.self_attn_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.feed_forward.intermediate_dense.weight was initialized from encoder.layers.1.fc1.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.feed_forward.intermediate_dense.bias was initialized from encoder.layers.1.fc1.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.feed_forward.output_dense.weight was initialized from encoder.layers.1.fc2.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.feed_forward.output_dense.bias was initialized from encoder.layers.1.fc2.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.final_layer_norm.weight was initialized from encoder.layers.1.final_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.1.final_layer_norm.bias was initialized from encoder.layers.1.final_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.attention.k_proj.weight was initialized from encoder.layers.2.self_attn.k_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.attention.k_proj.bias was initialized from encoder.layers.2.self_attn.k_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.attention.v_proj.weight was initialized from encoder.layers.2.self_attn.v_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.attention.v_proj.bias was initialized from encoder.layers.2.self_attn.v_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.attention.q_proj.weight was initialized from encoder.layers.2.self_attn.q_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.attention.q_proj.bias was initialized from encoder.layers.2.self_attn.q_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.attention.out_proj.weight was initialized from encoder.layers.2.self_attn.out_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.attention.out_proj.bias was initialized from encoder.layers.2.self_attn.out_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.layer_norm.weight was initialized from encoder.layers.2.self_attn_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.layer_norm.bias was initialized from encoder.layers.2.self_attn_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.feed_forward.intermediate_dense.weight was initialized from encoder.layers.2.fc1.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.feed_forward.intermediate_dense.bias was initialized from encoder.layers.2.fc1.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.feed_forward.output_dense.weight was initialized from encoder.layers.2.fc2.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.feed_forward.output_dense.bias was initialized from encoder.layers.2.fc2.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.final_layer_norm.weight was initialized from encoder.layers.2.final_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.2.final_layer_norm.bias was initialized from encoder.layers.2.final_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.attention.k_proj.weight was initialized from encoder.layers.3.self_attn.k_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.attention.k_proj.bias was initialized from encoder.layers.3.self_attn.k_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.attention.v_proj.weight was initialized from encoder.layers.3.self_attn.v_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.attention.v_proj.bias was initialized from encoder.layers.3.self_attn.v_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.attention.q_proj.weight was initialized from encoder.layers.3.self_attn.q_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.attention.q_proj.bias was initialized from encoder.layers.3.self_attn.q_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.attention.out_proj.weight was initialized from encoder.layers.3.self_attn.out_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.attention.out_proj.bias was initialized from encoder.layers.3.self_attn.out_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.layer_norm.weight was initialized from encoder.layers.3.self_attn_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.layer_norm.bias was initialized from encoder.layers.3.self_attn_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.feed_forward.intermediate_dense.weight was initialized from encoder.layers.3.fc1.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.feed_forward.intermediate_dense.bias was initialized from encoder.layers.3.fc1.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.feed_forward.output_dense.weight was initialized from encoder.layers.3.fc2.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.feed_forward.output_dense.bias was initialized from encoder.layers.3.fc2.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.final_layer_norm.weight was initialized from encoder.layers.3.final_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.3.final_layer_norm.bias was initialized from encoder.layers.3.final_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.attention.k_proj.weight was initialized from encoder.layers.4.self_attn.k_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.attention.k_proj.bias was initialized from encoder.layers.4.self_attn.k_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.attention.v_proj.weight was initialized from encoder.layers.4.self_attn.v_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.attention.v_proj.bias was initialized from encoder.layers.4.self_attn.v_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.attention.q_proj.weight was initialized from encoder.layers.4.self_attn.q_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.attention.q_proj.bias was initialized from encoder.layers.4.self_attn.q_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.attention.out_proj.weight was initialized from encoder.layers.4.self_attn.out_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.attention.out_proj.bias was initialized from encoder.layers.4.self_attn.out_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.layer_norm.weight was initialized from encoder.layers.4.self_attn_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.layer_norm.bias was initialized from encoder.layers.4.self_attn_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.feed_forward.intermediate_dense.weight was initialized from encoder.layers.4.fc1.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.feed_forward.intermediate_dense.bias was initialized from encoder.layers.4.fc1.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.feed_forward.output_dense.weight was initialized from encoder.layers.4.fc2.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.feed_forward.output_dense.bias was initialized from encoder.layers.4.fc2.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.final_layer_norm.weight was initialized from encoder.layers.4.final_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.4.final_layer_norm.bias was initialized from encoder.layers.4.final_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.attention.k_proj.weight was initialized from encoder.layers.5.self_attn.k_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.attention.k_proj.bias was initialized from encoder.layers.5.self_attn.k_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.attention.v_proj.weight was initialized from encoder.layers.5.self_attn.v_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.attention.v_proj.bias was initialized from encoder.layers.5.self_attn.v_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.attention.q_proj.weight was initialized from encoder.layers.5.self_attn.q_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.attention.q_proj.bias was initialized from encoder.layers.5.self_attn.q_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.attention.out_proj.weight was initialized from encoder.layers.5.self_attn.out_proj.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.attention.out_proj.bias was initialized from encoder.layers.5.self_attn.out_proj.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.layer_norm.weight was initialized from encoder.layers.5.self_attn_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.layer_norm.bias was initialized from encoder.layers.5.self_attn_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.feed_forward.intermediate_dense.weight was initialized from encoder.layers.5.fc1.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.feed_forward.intermediate_dense.bias was initialized from encoder.layers.5.fc1.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.feed_forward.output_dense.weight was initialized from encoder.layers.5.fc2.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.feed_forward.output_dense.bias was initialized from encoder.layers.5.fc2.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.final_layer_norm.weight was initialized from encoder.layers.5.final_layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layers.5.final_layer_norm.bias was initialized from encoder.layers.5.final_layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layer_norm.weight was initialized from encoder.layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | encoder.layer_norm.bias was initialized from encoder.layer_norm.bias.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | feature_projection.layer_norm.weight was initialized from layer_norm.weight.\n",
      "2022-11-10 23:51:03 | INFO | __main__ | feature_projection.layer_norm.bias was initialized from layer_norm.bias.\n",
      "2022-11-10 23:51:03 | WARNING | __main__ | Unused weights: ['label_embs_concat', 'final_proj.weight', 'final_proj.bias']\n",
      "Configuration saved in hubert-small/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved in hubert-small/pytorch_model.bin\r\n"
     ]
    }
   ],
   "source": [
    "!python3 -m transformers.models.hubert.convert_hubert_original_pytorch_checkpoint_to_pytorch --pytorch_dump_folder hubert-small --checkpoint_path {ckpt} --config_path hubert-small/config.json --not_finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HubertModel.from_pretrained('./hubert-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py:38: FutureWarning: Deprecated positional argument(s) used in 'create_repo': pass token='hubert-small-ms' as keyword args. From version 0.12 passing these as positional arguments will result in an error,\n",
      "  warnings.warn(\n",
      "/home/husein/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:102: FutureWarning: `name` and `organization` input arguments are deprecated and will be removed in v0.10. Pass `repo_id` instead.\n",
      "  warnings.warn(\n",
      "/home/husein/.local/lib/python3.8/site-packages/huggingface_hub/hf_api.py:681: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/mesolitica/hubert-small-ms into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146b2fdc910a4b1e9a9d928681835168",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/97.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity, may be slow...        \n",
      "remote: LFS file scan complete.        \n",
      "To https://huggingface.co/mesolitica/hubert-small-ms\n",
      "   04e5a97..6645a2c  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/mesolitica/hubert-small-ms/commit/6645a2ca400ee19a58f674c74bc26c998e3d55dc'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('hubert-small-ms', organization='mesolitica')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\r\n"
     ]
    }
   ],
   "source": [
    "!cd hubert-small-ms && git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  train_inner  valid\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/husein/outputs/2022-11-06/20-10-17/tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 733190e] add tensorboard\n",
      " 3 files changed, 9 insertions(+)\n",
      " create mode 100644 tensorboard/train/events.out.tfevents.1667752549.husein-MS-7D31.9099.2\n",
      " create mode 100644 tensorboard/train_inner/events.out.tfevents.1667736704.husein-MS-7D31.9099.0\n",
      " create mode 100644 tensorboard/valid/events.out.tfevents.1667737799.husein-MS-7D31.9099.1\n",
      "Uploading LFS objects: 100% (3/3), 3.8 MB | 317 KB/s, done.                     \n",
      "Enumerating objects: 10, done.\n",
      "Counting objects: 100% (10/10), done.\n",
      "Delta compression using up to 20 threads\n",
      "Compressing objects: 100% (9/9), done.\n",
      "Writing objects: 100% (9/9), 981 bytes | 981.00 KiB/s, done.\n",
      "Total 9 (delta 1), reused 0 (delta 0)\n",
      "remote: Scanning LFS files for validity, may be slow...\u001b[K\n",
      "remote: LFS file scan complete.\u001b[K\n",
      "To https://huggingface.co/mesolitica/hubert-small-ms\n",
      "   6645a2c..733190e  main -> main\n"
     ]
    }
   ],
   "source": [
    "!cp -r /home/husein/outputs/2022-11-06/20-10-17/tensorboard hubert-small-ms\n",
    "!cd hubert-small-ms && git add . && git commit -m 'add tensorboard' && git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
