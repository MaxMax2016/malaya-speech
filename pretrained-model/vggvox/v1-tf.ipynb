{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, GlobalAveragePooling2D, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, ZeroPadding2D, MaxPooling2D, AveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Lambda, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_pool(\n",
    "    inp_tensor,\n",
    "    layer_idx,\n",
    "    conv_filters,\n",
    "    conv_kernel_size,\n",
    "    conv_strides,\n",
    "    conv_pad,\n",
    "    pool = '',\n",
    "    pool_size = (2, 2),\n",
    "    pool_strides = None,\n",
    "    conv_layer_prefix = 'conv',\n",
    "):\n",
    "    x = ZeroPadding2D(padding = conv_pad, name = 'pad{}'.format(layer_idx))(\n",
    "        inp_tensor\n",
    "    )\n",
    "    x = Conv2D(\n",
    "        filters = conv_filters,\n",
    "        kernel_size = conv_kernel_size,\n",
    "        strides = conv_strides,\n",
    "        padding = 'valid',\n",
    "        name = '{}{}'.format(conv_layer_prefix, layer_idx),\n",
    "    )(x)\n",
    "    x = BatchNormalization(\n",
    "        epsilon = 1e-5, momentum = 1.0, name = 'bn{}'.format(layer_idx)\n",
    "    )(x)\n",
    "    x = Activation('relu', name = 'relu{}'.format(layer_idx))(x)\n",
    "    if pool == 'max':\n",
    "        x = MaxPooling2D(\n",
    "            pool_size = pool_size,\n",
    "            strides = pool_strides,\n",
    "            name = 'mpool{}'.format(layer_idx),\n",
    "        )(x)\n",
    "    elif pool == 'avg':\n",
    "        x = AveragePooling2D(\n",
    "            pool_size = pool_size,\n",
    "            strides = pool_strides,\n",
    "            name = 'apool{}'.format(layer_idx),\n",
    "        )(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Block of layers: Conv --> BatchNorm --> ReLU --> Dynamic average pool (fc6 -> apool6 only)\n",
    "def conv_bn_dynamic_apool(\n",
    "    inp_tensor,\n",
    "    layer_idx,\n",
    "    conv_filters,\n",
    "    conv_kernel_size,\n",
    "    conv_strides,\n",
    "    conv_pad,\n",
    "    conv_layer_prefix = 'conv',\n",
    "):\n",
    "    x = ZeroPadding2D(padding = conv_pad, name = 'pad{}'.format(layer_idx))(\n",
    "        inp_tensor\n",
    "    )\n",
    "    x = Conv2D(\n",
    "        filters = conv_filters,\n",
    "        kernel_size = conv_kernel_size,\n",
    "        strides = conv_strides,\n",
    "        padding = 'valid',\n",
    "        name = '{}{}'.format(conv_layer_prefix, layer_idx),\n",
    "    )(x)\n",
    "    x = BatchNormalization(\n",
    "        epsilon = 1e-5, momentum = 1.0, name = 'bn{}'.format(layer_idx)\n",
    "    )(x)\n",
    "    x = Activation('relu', name = 'relu{}'.format(layer_idx))(x)\n",
    "    x = GlobalAveragePooling2D(name = 'gapool{}'.format(layer_idx))(x)\n",
    "    x = Reshape((1, 1, conv_filters), name = 'reshape{}'.format(layer_idx))(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet1D(Model):\n",
    "    def __init__(self, params=None, is_training=False):\n",
    "        super(Resnet1D, self).__init__()\n",
    "    \n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        inp = inputs['features_input']\n",
    "        x = conv_bn_pool(\n",
    "            inp,\n",
    "            layer_idx = 1,\n",
    "            conv_filters = 96,\n",
    "            conv_kernel_size = (7, 7),\n",
    "            conv_strides = (2, 2),\n",
    "            conv_pad = (1, 1),\n",
    "            pool = 'max',\n",
    "            pool_size = (3, 3),\n",
    "            pool_strides = (2, 2),\n",
    "        )\n",
    "        x = conv_bn_pool(\n",
    "            x,\n",
    "            layer_idx = 2,\n",
    "            conv_filters = 256,\n",
    "            conv_kernel_size = (5, 5),\n",
    "            conv_strides = (2, 2),\n",
    "            conv_pad = (1, 1),\n",
    "            pool = 'max',\n",
    "            pool_size = (3, 3),\n",
    "            pool_strides = (2, 2),\n",
    "        )\n",
    "        x = conv_bn_pool(\n",
    "            x,\n",
    "            layer_idx = 3,\n",
    "            conv_filters = 384,\n",
    "            conv_kernel_size = (3, 3),\n",
    "            conv_strides = (1, 1),\n",
    "            conv_pad = (1, 1),\n",
    "        )\n",
    "        x = conv_bn_pool(\n",
    "            x,\n",
    "            layer_idx = 4,\n",
    "            conv_filters = 256,\n",
    "            conv_kernel_size = (3, 3),\n",
    "            conv_strides = (1, 1),\n",
    "            conv_pad = (1, 1),\n",
    "        )\n",
    "        x = conv_bn_pool(\n",
    "            x,\n",
    "            layer_idx = 5,\n",
    "            conv_filters = 256,\n",
    "            conv_kernel_size = (3, 3),\n",
    "            conv_strides = (1, 1),\n",
    "            conv_pad = (1, 1),\n",
    "            pool = 'max',\n",
    "            pool_size = (5, 3),\n",
    "            pool_strides = (3, 2),\n",
    "        )\n",
    "        x = conv_bn_dynamic_apool(\n",
    "            x,\n",
    "            layer_idx = 6,\n",
    "            conv_filters = 4096,\n",
    "            conv_kernel_size = (9, 1),\n",
    "            conv_strides = (1, 1),\n",
    "            conv_pad = (0, 0),\n",
    "            conv_layer_prefix = 'fc',\n",
    "        )\n",
    "        x = conv_bn_pool(\n",
    "            x,\n",
    "            layer_idx = 7,\n",
    "            conv_filters = 1024,\n",
    "            conv_kernel_size = (1, 1),\n",
    "            conv_strides = (1, 1),\n",
    "            conv_pad = (0, 0),\n",
    "            conv_layer_prefix = 'fc',\n",
    "        )\n",
    "        x = Lambda(lambda y: K.l2_normalize(y, axis = 3), name = 'norm')(x)\n",
    "        x = Conv2D(\n",
    "            filters = 1024,\n",
    "            kernel_size = (1, 1),\n",
    "            strides = (1, 1),\n",
    "            padding = 'valid',\n",
    "            name = 'fc8',\n",
    "        )(x)\n",
    "        return x\n",
    "    \n",
    "class Model:\n",
    "    def __init__(self, is_training = False):\n",
    "        self.X = tf.placeholder(tf.float32, [None, 512, None, 1])\n",
    "        self.model = Resnet1D(is_training = is_training)\n",
    "        inputs = {'features_input': self.X}\n",
    "        self.logits = self.model.call(inputs)\n",
    "        self.logits = self.logits[:,0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'v1/vggvox.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Restoring parameters from v1/vggvox.ckpt\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_lists)\n",
    "saver.restore(sess, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal processing\n",
    "SAMPLE_RATE = 16000\n",
    "PREEMPHASIS_ALPHA = 0.97\n",
    "FRAME_LEN = 0.025\n",
    "FRAME_STEP = 0.01\n",
    "NUM_FFT = 512\n",
    "BUCKET_STEP = 1\n",
    "MAX_SEC = 10\n",
    "\n",
    "# Model\n",
    "WEIGHTS_FILE = \"data/model/weights.h5\"\n",
    "COST_METRIC = \"cosine\"  # euclidean or cosine\n",
    "INPUT_SHAPE=(NUM_FFT,None,1)\n",
    "\n",
    "# IO\n",
    "ENROLL_LIST_FILE = \"cfg/enroll_list.csv\"\n",
    "TEST_LIST_FILE = \"cfg/test_list.csv\"\n",
    "RESULT_FILE = \"res/results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from scipy.signal import lfilter, butter\n",
    "\n",
    "import sigproc\n",
    "\n",
    "\n",
    "def load_wav(filename, sample_rate):\n",
    "    audio, sr = librosa.load(filename, sr = sample_rate, mono = True)\n",
    "    audio = audio.flatten()\n",
    "    return audio\n",
    "\n",
    "\n",
    "def normalize_frames(m, epsilon = 1e-12):\n",
    "    return np.array([(v - np.mean(v)) / max(np.std(v), epsilon) for v in m])\n",
    "\n",
    "\n",
    "# https://github.com/christianvazquez7/ivector/blob/master/MSRIT/rm_dc_n_dither.m\n",
    "def remove_dc_and_dither(sin, sample_rate):\n",
    "    if sample_rate == 16e3:\n",
    "        alpha = 0.99\n",
    "    elif sample_rate == 8e3:\n",
    "        alpha = 0.999\n",
    "    else:\n",
    "        print('Sample rate must be 16kHz or 8kHz only')\n",
    "        exit(1)\n",
    "    sin = lfilter([1, -1], [1, -alpha], sin)\n",
    "    dither = (\n",
    "        np.random.random_sample(len(sin))\n",
    "        + np.random.random_sample(len(sin))\n",
    "        - 1\n",
    "    )\n",
    "    spow = np.std(dither)\n",
    "    sout = sin + 1e-6 * spow * dither\n",
    "    return sout\n",
    "\n",
    "\n",
    "def get_fft_spectrum(filename, buckets = None):\n",
    "    signal = load_wav(filename, SAMPLE_RATE)\n",
    "    signal *= 2 ** 15\n",
    "\n",
    "    # get FFT spectrum\n",
    "    signal = remove_dc_and_dither(signal, SAMPLE_RATE)\n",
    "    signal = sigproc.preemphasis(signal, coeff = PREEMPHASIS_ALPHA)\n",
    "    frames = sigproc.framesig(\n",
    "        signal,\n",
    "        frame_len = FRAME_LEN * SAMPLE_RATE,\n",
    "        frame_step = FRAME_STEP * SAMPLE_RATE,\n",
    "        winfunc = np.hamming,\n",
    "    )\n",
    "    fft = abs(np.fft.fft(frames, n = NUM_FFT))\n",
    "    fft_norm = normalize_frames(fft.T)\n",
    "    \n",
    "    if buckets:\n",
    "\n",
    "        # truncate to max bucket sizes\n",
    "        rsize = max(k for k in buckets if k <= fft_norm.shape[1])\n",
    "        rstart = int((fft_norm.shape[1] - rsize) / 2)\n",
    "        out = fft_norm[:, rstart : rstart + rsize]\n",
    "        return out\n",
    "    \n",
    "    else:\n",
    "        return fft_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{100: 2,\n",
       " 200: 5,\n",
       " 300: 8,\n",
       " 400: 11,\n",
       " 500: 14,\n",
       " 600: 17,\n",
       " 700: 20,\n",
       " 800: 23,\n",
       " 900: 27,\n",
       " 1000: 30}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_buckets(max_sec, step_sec, frame_step):\n",
    "    buckets = {}\n",
    "    frames_per_sec = int(1 / frame_step)\n",
    "    end_frame = int(max_sec * frames_per_sec)\n",
    "    step_frame = int(step_sec * frames_per_sec)\n",
    "    for i in range(0, end_frame + 1, step_frame):\n",
    "        s = i\n",
    "        s = np.floor((s - 7 + 2) / 2) + 1  # conv1\n",
    "        s = np.floor((s - 3) / 2) + 1  # mpool1\n",
    "        s = np.floor((s - 5 + 2) / 2) + 1  # conv2\n",
    "        s = np.floor((s - 3) / 2) + 1  # mpool2\n",
    "        s = np.floor((s - 3 + 2) / 1) + 1  # conv3\n",
    "        s = np.floor((s - 3 + 2) / 1) + 1  # conv4\n",
    "        s = np.floor((s - 3 + 2) / 1) + 1  # conv5\n",
    "        s = np.floor((s - 3) / 2) + 1  # mpool5\n",
    "        s = np.floor((s - 1) / 1) + 1  # fc6\n",
    "        if s > 0:\n",
    "            buckets[i] = int(s)\n",
    "    return buckets\n",
    "\n",
    "buckets = build_buckets(MAX_SEC, BUCKET_STEP, FRAME_STEP)\n",
    "buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/wav/enroll/27-123349-0000.wav',\n",
       " 'data/wav/enroll/19-227-0000.wav',\n",
       " 'data/wav/enroll/26-495-0000.wav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "files = glob('data/wav/enroll/*.wav')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    r = sess.run(model.logits, feed_dict = {model.X: np.expand_dims([x], -1)})\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred(get_fft_spectrum(files[0], buckets)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "files = glob('../voxceleb/aac/**/*.m4a', recursive = True)\n",
    "files = random.sample(files, 10000)\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf test-*.wav pickle-*.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 198/200 [01:58<00:01,  1.61it/s]\n",
      "100%|██████████| 200/200 [01:58<00:00,  1.68it/s]\n",
      "\n",
      "100%|██████████| 200/200 [01:58<00:00,  1.63it/s]\n",
      " 99%|█████████▉| 198/200 [01:58<00:01,  1.72it/s]\n",
      "100%|██████████| 200/200 [01:59<00:00,  1.68it/s]\n",
      "100%|█████████▉| 199/200 [01:59<00:00,  1.50it/s]\n",
      "\n",
      "100%|██████████| 200/200 [01:59<00:00,  1.67it/s]\n",
      "100%|██████████| 200/200 [01:59<00:00,  1.67it/s]\n",
      "100%|██████████| 200/200 [01:59<00:00,  1.67it/s]\n",
      "100%|██████████| 200/200 [01:59<00:00,  1.67it/s]\n",
      "\n",
      "100%|█████████▉| 199/200 [01:59<00:00,  1.55it/s]\n",
      "100%|██████████| 200/200 [01:59<00:00,  1.67it/s]\n",
      "100%|██████████| 200/200 [02:00<00:00,  1.67it/s]\n",
      "100%|██████████| 200/200 [02:00<00:00,  1.67it/s]\n",
      " 99%|█████████▉| 198/200 [02:00<00:01,  1.47it/s]\n",
      "100%|█████████▉| 199/200 [02:00<00:00,  1.33it/s]\n",
      "100%|██████████| 200/200 [02:00<00:00,  1.66it/s]\n",
      "100%|█████████▉| 199/200 [02:00<00:00,  1.51it/s]\n",
      " 98%|█████████▊| 197/200 [02:00<00:02,  1.36it/s]\n",
      "100%|██████████| 200/200 [02:00<00:00,  1.66it/s]\n",
      "100%|██████████| 200/200 [02:00<00:00,  1.65it/s]\n",
      " 99%|█████████▉| 198/200 [02:00<00:01,  1.39it/s]\n",
      "100%|██████████| 200/200 [02:00<00:00,  1.43it/s]\n",
      "100%|██████████| 200/200 [02:00<00:00,  1.65it/s]\n",
      "100%|█████████▉| 199/200 [02:00<00:00,  1.35it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.65it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.39it/s]\n",
      " 99%|█████████▉| 198/200 [02:01<00:01,  1.39it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.65it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.65it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.65it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.65it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.34it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.28it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.28it/s]\n",
      "\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.64it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.64it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.64it/s]\n",
      "100%|██████████| 200/200 [02:01<00:00,  1.64it/s]\n",
      "100%|██████████| 200/200 [02:02<00:00,  1.64it/s]\n",
      "100%|██████████| 200/200 [02:02<00:00,  1.64it/s]\n",
      "100%|██████████| 200/200 [02:02<00:00,  1.64it/s]\n",
      "100%|██████████| 200/200 [02:02<00:00,  1.28it/s]\n",
      "100%|██████████| 200/200 [02:02<00:00,  1.63it/s]\n",
      "100%|██████████| 200/200 [02:02<00:00,  1.63it/s]\n",
      "100%|██████████| 200/200 [02:02<00:00,  1.63it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-011fe8f45d8d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mfiles_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/vggvox-speaker-identification/mp.py\u001b[0m in \u001b[0;36mmultiprocessing\u001b[0;34m(strings, function, cores, returned)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpooled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import mp\n",
    "from tqdm import tqdm\n",
    "from pydub import AudioSegment\n",
    "\n",
    "def loop(args):\n",
    "    files = args[0]\n",
    "    index = args[1]\n",
    "    results = []\n",
    "    for file in tqdm(files):\n",
    "        \n",
    "        audio = AudioSegment.from_file(file[1])\n",
    "        audio.set_frame_rate(16000).set_channels(1).export(f'test-{index}.wav', format=\"wav\")\n",
    "        l = get_fft_spectrum(f'test-{index}.wav', buckets)\n",
    "        results.append((file[0], file[1], l))\n",
    "        \n",
    "    with open(f'pickle-{index}.pkl', 'wb') as fopen:\n",
    "        pickle.dump(results, fopen)\n",
    "\n",
    "files_index = [(no, f) for no, f in enumerate(files)]\n",
    "r = mp.multiprocessing(files_index, loop, cores = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf test-*.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../voxceleb/vox2_meta.csv')\n",
    "df = df[df['Set '] == 'test ']\n",
    "speakers = df['VoxCeleb2 ID '].unique().tolist()\n",
    "speakers = [s.strip() for s in speakers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "speakers_idx = defaultdict(list)\n",
    "\n",
    "for speaker in speakers:\n",
    "    for file in files:\n",
    "        if speaker in file:\n",
    "            speakers_idx[speaker].append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:18<00:00, 548.35it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "k = 10\n",
    "labels = []\n",
    "\n",
    "def get_id(file):\n",
    "    return file.split('/')[3]\n",
    "\n",
    "for file in tqdm(files):\n",
    "    left_speaker = get_id(file)\n",
    "    for speaker in speakers:\n",
    "        if left_speaker == speaker:\n",
    "            label = 1\n",
    "        else:\n",
    "            label = 0\n",
    "        samples = random.sample(speakers_idx[speaker], min(k, len(speakers_idx[speaker])))\n",
    "        for s in samples:\n",
    "            labels.append((label, file, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "\n",
    "pickles = glob('pickle-*.pkl')\n",
    "\n",
    "pooled = []\n",
    "for p in pickles:\n",
    "    with open(p, 'rb') as fopen:\n",
    "        pooled.append(pickle.load(fopen))\n",
    "        \n",
    "pooled = list(itertools.chain(*pooled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:47<00:00, 210.48it/s]\n"
     ]
    }
   ],
   "source": [
    "mapping = {i[1]: pred(i[2]) for i in tqdm(pooled)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11720000/11720000 [02:07<00:00, 91983.96it/s] \n"
     ]
    }
   ],
   "source": [
    "scores, ls = [], []\n",
    "\n",
    "for i in tqdm(range(len(labels))):\n",
    "    ls.append(labels[i][0])\n",
    "    scores.append(np.sum(mapping[labels[i][1]][0] * mapping[labels[i][2]][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eer(y, y_score):\n",
    "    \n",
    "    from scipy.optimize import brentq\n",
    "    from sklearn.metrics import roc_curve\n",
    "    from scipy.interpolate import interp1d\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y, y_score, pos_label=1)\n",
    "    eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    thresh = interp1d(fpr, thresholds)(eer)\n",
    "    return eer, thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14070736036804815, array(0.18324658))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_eer(ls, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
