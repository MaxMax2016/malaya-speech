{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/huseinzol05/Malay-TTS-Yasmin/resolve/main/tts-malay-yasmin.tar.gz\n",
    "# !wget https://huggingface.co/datasets/huseinzol05/Malay-TTS-Yasmin/resolve/main/populated-text.json\n",
    "# !tar -xf tts-malay-yasmin.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/huseinzol05/Malay-TTS-Yasmin/resolve/main/tts-malay-yasmin.tar.gz\n",
    "# !wget https://huggingface.co/datasets/huseinzol05/Malay-TTS-Yasmin/resolve/main/populated-text.json\n",
    "# !tar -xf tts-malay-yasmin.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://huggingface.co/datasets/huseinzol05/Malay-TTS-Yasmin/resolve/main/tts-malay-yasmin-parliament.tar.gz\n",
    "# !wget https://huggingface.co/datasets/huseinzol05/Malay-TTS-Yasmin/resolve/main/populated-parliament.json\n",
    "# !tar -xf tts-malay-yasmin-parliament.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import parselmouth\n",
    "import librosa\n",
    "import pyworld as pw\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('config.yaml') as fopen:\n",
    "    config = yaml.load(fopen)\n",
    "    \n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# https://github.com/TensorSpeech/TensorFlowTTS/blob/master/tensorflow_tts/utils/outliers.py\n",
    "def is_outlier(x, p25, p75):\n",
    "    \"\"\"Check if value is an outlier.\"\"\"\n",
    "    lower = p25 - 1.5 * (p75 - p25)\n",
    "    upper = p75 + 1.5 * (p75 - p25)\n",
    "    return x <= lower or x >= upper\n",
    "\n",
    "\n",
    "def remove_outlier(x, p_bottom: int = 25, p_top: int = 75):\n",
    "    \"\"\"Remove outlier from x.\"\"\"\n",
    "    p_bottom = np.percentile(x, p_bottom)\n",
    "    p_top = np.percentile(x, p_top)\n",
    "\n",
    "    indices_of_outliers = []\n",
    "    for ind, value in enumerate(x):\n",
    "        if is_outlier(value, p_bottom, p_top):\n",
    "            indices_of_outliers.append(ind)\n",
    "\n",
    "    x[indices_of_outliers] = 0.0\n",
    "    x[indices_of_outliers] = np.max(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "_pad = 'pad'\n",
    "_start = 'start'\n",
    "_eos = 'eos'\n",
    "_punctuation = \"!'(),.:;? \"\n",
    "_special = '-'\n",
    "_letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'\n",
    "_numbers = '0123456789'\n",
    "\n",
    "MALAYA_SPEECH_SYMBOLS = (\n",
    "    [_pad, _start, _eos] + list(_special) + list(_punctuation) + list(_letters)\n",
    ")\n",
    "\n",
    "TTS_AZURE_SYMBOLS = (\n",
    "    [_pad, _start, _eos] + list(_special) + list(_punctuation) + list(_letters) + list(_numbers)\n",
    ")\n",
    "\n",
    "INITIAL_SYMBOLS = list(_letters) + list(_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('populated-text.json') as fopen:\n",
    "    texts = json.load(fopen)\n",
    "    \n",
    "with open('populated-parliament.json') as fopen:\n",
    "    parliament = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tts_encode(string: str, add_eos: bool = True):\n",
    "    r = [TTS_AZURE_SYMBOLS.index(c) for c in string if c in TTS_AZURE_SYMBOLS]\n",
    "    if add_eos:\n",
    "        r = r + [TTS_AZURE_SYMBOLS.index('eos')]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "from malaya.text.normalization import digit, cardinal\n",
    "import malaya\n",
    "\n",
    "#normalizer = malaya.normalize.normalizer(date = False, time = False)\n",
    "normalizer = malaya.normalize.normalizer()\n",
    "\n",
    "def put_spacing_num(string):\n",
    "    string = re.sub('[A-Za-z]+', lambda ele: ' ' + ele[0] + ' ', string)\n",
    "    return re.sub(r'[ ]+', ' ', string).strip()\n",
    "\n",
    "def convert_to_ascii(string):\n",
    "    return unidecode(string)\n",
    "\n",
    "def collapse_whitespace(string):\n",
    "    return re.sub(_whitespace_re, ' ', string)\n",
    "\n",
    "def put_spacing(string, chars = '()-'):\n",
    "    for c in chars:\n",
    "        string = string.replace(c, f' {c} ')\n",
    "    return string\n",
    "\n",
    "before = {';': ',', '_': '', '=': 'sama dengan', '*': 'asterisk',\n",
    "          \"'\": '', '~': '', '`': '', '%': 'peratus'}\n",
    "\n",
    "after = {'/': 'garis miring'}\n",
    "def replace_chars(string, chars):\n",
    "    for k, v in chars.items():\n",
    "        string = string.replace(k, f' {v} ')\n",
    "    return string\n",
    "\n",
    "patterns_num = [(r\"\\b\\d+(?:[\\.,']\\d+)?\\b\\/\\b\\d+(?:[\\.,']\\d+)?\\b\", '/', 'garis miring'),\n",
    "           (r\"\\b\\d+(?:[\\.,']\\d+)?\\b\\-\\b\\d+(?:[\\.,']\\d+)?\\b\", '-', '')]\n",
    "\n",
    "pattern_rm = r\"RM \\b\\d+(?:[\\.,']\\d+)?\\b (?:ribu|puluh|juta)\"\n",
    "\n",
    "replaces = {'dollar bilion': 'bilion dollar', 'dollar ribu': 'ribu dollar', 'dollar juta': 'juta dollar'}\n",
    "\n",
    "def fix_pattern_num(string):\n",
    "    for p in patterns_num:\n",
    "        results = re.findall(p[0], string)\n",
    "        for r in results:\n",
    "            l_, r_ = r.split(p[1])\n",
    "            string = string.replace(r, f'{digit(l_)} {p[2]} {digit(r_)}')\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    return string\n",
    "\n",
    "def fix_dash_num(string):\n",
    "    results = re.findall(r\"-\\d+\", string)\n",
    "    for r in results:\n",
    "        string = string.replace(r, cardinal(r).replace('negatif', 'dash'))\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    return string\n",
    "\n",
    "def fix_num_dash(string):\n",
    "    results = re.findall(r\"\\d+-\", string)\n",
    "    for r in results:\n",
    "        string = string.replace(r, cardinal(r.replace('-', '')))\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    return string\n",
    "\n",
    "def cleaning(string, add_eos = False):\n",
    "    sequence = []\n",
    "    string = convert_to_ascii(string)\n",
    "    string = replace_chars(string, before)\n",
    "    string = fix_1900(string)\n",
    "    string = fix_isbn(string)\n",
    "    string = fix_pattern_num(string)\n",
    "    string = fix_dash_num(string)\n",
    "    string = fix_num_dash(string)\n",
    "    string = fix_rm(string)\n",
    "    string = put_spacing(string)\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    print(string)\n",
    "    string = normalizer.normalize(string, normalize_text = False)['normalize']\n",
    "    string = replace_chars(string, after)\n",
    "    string = replace_chars(string, replaces)\n",
    "\n",
    "    if string[-1] in '-,':\n",
    "        string = string[:-1]\n",
    "    if string[-2] in '-,!:;':\n",
    "        string = string[:-2]\n",
    "    if string[-1] != '.':\n",
    "        string = string + '.'\n",
    "    if string[0] not in INITIAL_SYMBOLS:\n",
    "        string = string[1:]\n",
    "    string = put_spacing_num(string)\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    return string, tts_encode(string, add_eos = add_eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rejected = ['Coppa Italia', 'Pak Ramli memerlukan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning(texts[665]['cleaned'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_rm(string):\n",
    "    t = string + ' .'\n",
    "    compound = ['ribu', 'puluh', 'juta', 'bilion']\n",
    "    pattern_rm = r\"RM \\b\\d+(?:[\\.,']\\d+)?(?:[\\.,']\\d+)?\\b (?:ribu|puluh|juta|bilion)\"\n",
    "    results = re.findall(pattern_rm, string)\n",
    "    for r in results:\n",
    "        splitted = r.split()\n",
    "        if t[string.find(r) + len(r)] in '(/':\n",
    "            s_ = splitted[-2].split('.')\n",
    "            c = ' , '.join([cardinal(s__) for s__ in s_])\n",
    "        else:\n",
    "            c = cardinal(splitted[-2])\n",
    "        if t[-2][-1] == '0' and '.' in splitted[-2]:\n",
    "            c = f'{c} kosong'\n",
    "        if string[string.find(r) + len(r)] in '(/':\n",
    "            string = string.replace(r, f'RM {c} {splitted[-1]}').replace('perpuluhan', ',')\n",
    "        else:\n",
    "            string = string.replace(r, f'{c} {splitted[-1]} RM')\n",
    "\n",
    "    pattern_rm = r\"RM \\b\\d+(?:[\\.,']\\d+)?(?:[\\.,']\\d+)?\\b\"\n",
    "    results = re.findall(pattern_rm, string)\n",
    "    for r in results:\n",
    "        splitted = r.split()\n",
    "        if t[string.find(r) + len(r)] in '(/':\n",
    "            s_ = splitted[-1].split('.')\n",
    "            c = ' , '.join([cardinal(s__) for s__ in s_])\n",
    "        else:\n",
    "            c = cardinal(splitted[-1])\n",
    "        if splitted[-1][-1] == '0' and '.' in splitted[-1]:\n",
    "            c = f'{c} kosong'\n",
    "        if t[string.find(r) + len(r)] in '(/':\n",
    "            string = string.replace(r, f'RM {c}').replace('perpuluhan', ',')\n",
    "        else:\n",
    "            string = string.replace(r, f'{c} RM')\n",
    "    \n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    return string\n",
    "\n",
    "fix_rm('RM 500,000.00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_1900(string):\n",
    "    results = re.findall(r'19\\d\\d', string)\n",
    "    for r in results:\n",
    "        if r[-2:] != '00':\n",
    "            if r[-2] == '0':\n",
    "                c = 'kosong ' + cardinal(r[-1])\n",
    "            else:\n",
    "                c = cardinal(r[-2:])\n",
    "            string = string.replace(r, 'sembilan belas ' + c)\n",
    "        else:\n",
    "            string = string.replace(r, 'sembilan belas ratus')\n",
    "    return string\n",
    "\n",
    "fix_1900('1902')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'output-yasmin'\n",
    "os.system(f'mkdir {directory}')\n",
    "directories = ['audios', 'mels', 'text_ids', 'f0s', 'energies', 'pitches']\n",
    "for d in directories:\n",
    "    os.system(f'mkdir {directory}/{d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = [(f'female/{i}.wav', texts[i]['cleaned'], i, directory) for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio('female/1821.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[11624]['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaning(texts[1821]['cleaned'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[16192]['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_isbn(string):\n",
    "    results = re.findall(r'[0-9\\-]+', string)\n",
    "    results = [r for r in results if string[:string.find(r) - 1].split()[-1].lower() == 'isbn']\n",
    "    for r in results:\n",
    "        splitted = r.split('-')\n",
    "        string = string.replace(r, ' dash '.join([digit(s) for s in splitted]))\n",
    "    return string\n",
    "    \n",
    "fix_isbn(texts[1122]['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in txts:\n",
    "    if len(re.findall(r'13\\d\\d', t[1])):\n",
    "           print(t[2], t[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in txts:\n",
    "    if 'ISBN' in t[1]:\n",
    "        print(t[2])\n",
    "        print(t[1])\n",
    "        print(cleaning(t[1])[0])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya_speech\n",
    "from malaya_speech import Pipeline\n",
    "from tqdm import tqdm\n",
    "vad = malaya_speech.vad.webrtc()\n",
    "\n",
    "def process(txts, \n",
    "            start_silent_trail = int(0.05 * config['sampling_rate']),\n",
    "            middle_silent_trail = int(0.12 * config['sampling_rate']),\n",
    "            end_silent_trail = int(0.1 * config['sampling_rate']),\n",
    "            process_middle_silent = True,\n",
    "            maxlen = 25):\n",
    "    \n",
    "    txts = txts[0]\n",
    "    audios, mels, text_ids, f0s, energies, pitches = [], [], [], [], [], []\n",
    "\n",
    "    for f in txts:\n",
    "        directory = f[3]\n",
    "        index = f[2]\n",
    "        text = f[1]\n",
    "        f = f[0]\n",
    "\n",
    "        text = cleaning(text, f)\n",
    "        audio, _ = malaya_speech.load(f, sr = config['sampling_rate'])\n",
    "        audio = audio[start_silent_trail:]\n",
    "\n",
    "        if config['trim_silence']:\n",
    "            y_= malaya_speech.resample(audio, config['sampling_rate'], 16000)\n",
    "            y_ = malaya_speech.astype.float_to_int(y_)\n",
    "            frames = list(malaya_speech.generator.frames(audio, 30, config['sampling_rate']))\n",
    "            frames_ = list(malaya_speech.generator.frames(y_, 30, 16000, append_ending_trail = False))\n",
    "            frames_webrtc = [(frames[no], vad(frame)) for no, frame in enumerate(frames_)]\n",
    "            grouped_deep = malaya_speech.group.group_frames(frames_webrtc)\n",
    "            grouped_deep = malaya_speech.group.group_frames_threshold(grouped_deep, 0.15)\n",
    "            r = []\n",
    "            for no, g in enumerate(grouped_deep):\n",
    "                if g[1]:\n",
    "                    g = g[0].array\n",
    "                else:\n",
    "                    if no == 0:\n",
    "                        g = g[0].array[-start_silent_trail:]\n",
    "                    elif no == (len(grouped_deep) - 1):\n",
    "                        g = g[0].array[:end_silent_trail]\n",
    "                    else:\n",
    "                        if process_middle_silent:\n",
    "                            g = np.concatenate([g[0].array[:middle_silent_trail], g[0].array[-middle_silent_trail:]])\n",
    "                        else:\n",
    "                            g = g[0].array\n",
    "                        \n",
    "                r.append(g)\n",
    "            audio = np.concatenate(r)\n",
    "        \n",
    "        if (len(audio) / config['sampling_rate']) > maxlen:\n",
    "            print('skipped, audio too long')\n",
    "            continue\n",
    "\n",
    "        D = librosa.stft(\n",
    "            audio,\n",
    "            n_fft=config['fft_size'],\n",
    "            hop_length=config['hop_size'],\n",
    "            win_length=config['win_length'],\n",
    "            window=config['window'],\n",
    "            pad_mode='reflect',\n",
    "        )\n",
    "        S, _ = librosa.magphase(D) \n",
    "        fmin = 0 if config[\"fmin\"] is None else config[\"fmin\"]\n",
    "        fmax = sampling_rate // 2 if config[\"fmax\"] is None else config[\"fmax\"]\n",
    "        mel_basis = librosa.filters.mel(\n",
    "            sr=config['sampling_rate'],\n",
    "            n_fft=config[\"fft_size\"],\n",
    "            n_mels=config[\"num_mels\"],\n",
    "            fmin=fmin,\n",
    "            fmax=fmax,\n",
    "        )\n",
    "        mel = np.log10(np.maximum(np.dot(mel_basis, S), 1e-10)).T\n",
    "        audio = np.pad(audio, (0, config[\"fft_size\"]), mode=\"edge\")\n",
    "        audio = audio[: len(mel) * config['hop_size']]\n",
    "\n",
    "        _f0, t = pw.dio(\n",
    "            audio.astype(np.double),\n",
    "            fs=config['sampling_rate'],\n",
    "            f0_ceil=fmax,\n",
    "            frame_period=1000 * config['hop_size'] / config['sampling_rate'],\n",
    "        )\n",
    "        f0 = pw.stonemask(audio.astype(np.double), _f0, t, config['sampling_rate'])\n",
    "        if len(f0) >= len(mel):\n",
    "            f0 = f0[: len(mel)]\n",
    "        else:\n",
    "            f0 = np.pad(f0, (0, len(mel) - len(f0)))\n",
    "\n",
    "        # extract energy\n",
    "        energy = np.sqrt(np.sum(S ** 2, axis=0))\n",
    "        f0 = remove_outlier(f0)\n",
    "        energy = remove_outlier(energy)\n",
    "        \n",
    "        mel_len = len(mel)\n",
    "        snd = parselmouth.Sound(audio,sampling_frequency=22050)\n",
    "        pitch = snd.to_pitch(time_step=snd.duration / (mel_len + 3)\n",
    "                         ).selected_array['frequency']\n",
    "\n",
    "        if config[\"global_gain_scale\"] > 0.0:\n",
    "            audio *= config[\"global_gain_scale\"]\n",
    "\n",
    "        if len(energy[energy != 0]) == 0 or len(f0[f0 != 0]) == 0:\n",
    "            print('skipped')\n",
    "            continue\n",
    "            \n",
    "        np.save(f'{directory}/audios/{index}.npy', audio)\n",
    "        np.save(f'{directory}/mels/{index}.npy', mel)\n",
    "        np.save(f'{directory}/text_ids/{index}.npy', text)\n",
    "        np.save(f'{directory}/f0s/{index}.npy', f0)\n",
    "        np.save(f'{directory}/energies/{index}.npy', energy)\n",
    "\n",
    "        audios.append(audio)\n",
    "        mels.append(mel)\n",
    "        text_ids.append(text)\n",
    "        f0s.append(f0)\n",
    "        energies.append(energy)\n",
    "        pitches.append(pitch)\n",
    "    \n",
    "    return [[audios, mels, text_ids, f0s, energies, pitches]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1508\n",
    "r = process((txts[i: i + 1], 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(r[2])):\n",
    "    print(n, r[2][n][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "ipd.Audio(r[0][k], rate = 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows = 2\n",
    "fig, ax = plt.subplots(nrows = nrows, ncols = 1)\n",
    "fig.set_figwidth(10)\n",
    "fig.set_figheight(nrows * 3)\n",
    "mel_outputs_ = np.reshape(r[1][k], [-1, 80])\n",
    "im = ax[0].imshow(np.rot90(mel_outputs_), aspect='auto', interpolation='none')\n",
    "fig.colorbar(mappable=im, shrink=0.65, orientation='horizontal', ax=ax[0])\n",
    "ax[1].plot(r[0][k])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp\n",
    "\n",
    "for i in tqdm(range(0, len(txts), 1000)):\n",
    "    index = min(i + 1000, len(txts))\n",
    "    b = txts[i: index]\n",
    "    mp.multiprocessing(b, process, cores = 15, returned = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'output-yasmin-parliament'\n",
    "os.system(f'mkdir {directory}')\n",
    "directories = ['audios', 'mels', 'text_ids', 'f0s', 'energies', 'pitches']\n",
    "for d in directories:\n",
    "    os.system(f'mkdir {directory}/{d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts = [(f'female-parliament/{i}.wav', parliament[i]['cleaned'], i, directory) for i in range(len(parliament))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 80\n",
    "r = process((txts[i: i + 10], 0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(len(r[2])):\n",
    "    print(n, r[2][n][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "ipd.Audio(r[0][k], rate = 22050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(0, len(txts), 1000)):\n",
    "    index = min(i + 1000, len(txts))\n",
    "    b = txts[i: index]\n",
    "    mp.multiprocessing(b, process, cores = 15, returned = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -hs output-yasmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -hs output-yasmin-parliament"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_mel = StandardScaler(copy=False)\n",
    "scaler_energy = StandardScaler(copy=False)\n",
    "scaler_f0 = StandardScaler(copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "mels = glob('output-yasmin/mels/*.npy')\n",
    "len(mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm(mels):\n",
    "    mel = np.load(f)\n",
    "    f0 = np.load(f.replace('mels/', 'f0s/'))\n",
    "    energy = np.load(f.replace('mels/', 'energies/'))\n",
    "    \n",
    "    scaler_mel.partial_fit(mel)\n",
    "    scaler_energy.partial_fit(energy[energy != 0].reshape(-1, 1))\n",
    "    scaler_f0.partial_fit(f0[f0 != 0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mels = glob('output-yasmin-parliament/mels/*.npy')\n",
    "len(mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm(mels):\n",
    "    mel = np.load(f)\n",
    "    f0 = np.load(f.replace('mels/', 'f0s/'))\n",
    "    energy = np.load(f.replace('mels/', 'energies/'))\n",
    "    \n",
    "    scaler_mel.partial_fit(mel)\n",
    "    scaler_energy.partial_fit(energy[energy != 0].reshape(-1, 1))\n",
    "    scaler_f0.partial_fit(f0[f0 != 0].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_stats = 'yasmin-stats'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_statistics_to_file(scaler_list, config):\n",
    "    os.system(f'mkdir {directory_stats}')\n",
    "    for scaler, name in scaler_list:\n",
    "        stats = np.stack((scaler.mean_, scaler.scale_))\n",
    "        np.save(\n",
    "            os.path.join(f\"{directory_stats}/stats{name}.npy\"),\n",
    "            stats.astype(np.float32),\n",
    "            allow_pickle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_list = [(scaler_mel, \"\"), (scaler_energy, \"_energy\"), (scaler_f0, \"_f0\")]\n",
    "save_statistics_to_file(scaler_list, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
