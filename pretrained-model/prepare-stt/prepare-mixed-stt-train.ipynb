{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure run prepare-malay-stt-train.ipynb first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/speech/imda/part3-same-splitted.tar\n",
    "# !tar -xf part3-same-splitted.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/speech/imda/TTS.zip\n",
    "# !unzip TTS.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/speech/imda/part1.tar\n",
    "# !tar -xf part1.tar\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/speech/imda/SCRIPT.zip\n",
    "# !unzip -o SCRIPT.zip -d WAVE-text\n",
    "# !rm SCRIPT.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = '/home/husein/speech-bahasa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6033"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'{base_directory}/CHANNEL0/SCRIPT/FEMALE_01.txt') as fopen:\n",
    "    texts = fopen.read().split('\\n')\n",
    "\n",
    "singlishs = []\n",
    "for t in texts:\n",
    "    splitted = t.split('\\t')\n",
    "    if len(splitted) == 2:\n",
    "        singlishs.append((f'{base_directory}/CHANNEL0/WAVE/FEMALE_01/{splitted[0]}.wav', splitted[1]))\n",
    "        \n",
    "len(singlishs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2034/2034 [00:04<00:00, 453.61it/s]\n"
     ]
    }
   ],
   "source": [
    "wave_texts = glob('WAVE-text/*.TXT')\n",
    "\n",
    "for f in tqdm(wave_texts):\n",
    "    speaker = f.split('/')[1].replace('.TXT', '')\n",
    "    channel = speaker[-1]\n",
    "    speaker = speaker[1:-1]\n",
    "    \n",
    "    with open(f) as fopen:\n",
    "        texts = list(filter(None, fopen.read().split('\\n')))[::2]\n",
    "    \n",
    "    for text in texts:\n",
    "        splitted = text.split('\\t')\n",
    "        wav = unidecode(splitted[0])\n",
    "        path = f'{base_directory}/WAVE/SPEAKER{speaker}/SESSION{channel}/{wav}.WAV'\n",
    "        \n",
    "        if os.path.exists(path) and len(splitted[1]):\n",
    "            singlishs.append((path, splitted[1]))\n",
    "        else:\n",
    "            print(splitted, path)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1261328"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singlish = glob(f'{base_directory}/part3-splitted/wav/*.wav')\n",
    "len(singlish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1261328/1261328 [00:37<00:00, 33288.44it/s]\n"
     ]
    }
   ],
   "source": [
    "def replace_paralinguistic(string, replaces = ['(ppb)', '(ppc)', '(ppl)', '(ppo)', '<UNK>', '<MANDARIN>']):\n",
    "    for r in replaces:\n",
    "        string = string.replace(r, ' ')\n",
    "    string = string.split()\n",
    "    string = [w for w in string if w[0] != '<' and w[-1] != '>']\n",
    "    string = [w for w in string if w[0] != '[' and w[-1] != ']']\n",
    "    return ' '.join(string)\n",
    "\n",
    "for i in tqdm(singlish):\n",
    "    try:\n",
    "        p = i.replace('/wav','/text')\n",
    "        with open(f'{p}.txt') as fopen:\n",
    "            text = fopen.read()\n",
    "        if len(text) < 2:\n",
    "            continue\n",
    "        if text[0] == '<' and text[-1] == '>':\n",
    "            continue\n",
    "        text = replace_paralinguistic(text)\n",
    "        singlishs.append((i, text))\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "singlishs_train, singlishs_test = train_test_split(singlishs, test_size = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('singlish-mixed.json', 'w') as fopen:\n",
    "    json.dump({'train': singlishs_train, 'test': singlishs_test}, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import IPython.display as ipd\n",
    "\n",
    "# ipd.Audio(audiobook[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = singlishs_train\n",
    "audios, texts = zip(*audios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464595, 1464595)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audios), len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordsegment\n",
    "wordsegment.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\"]\n",
    "\n",
    "def preprocessing_text(string):\n",
    "    \n",
    "    string = unicodedata.normalize('NFC', string)\n",
    "    string = ' '.join(wordsegment.segment(string)).lower()\n",
    "    string = string.replace('\\'', '')\n",
    "    string = ''.join([c if c in vocabs else ' ' for c in string])\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(strings):\n",
    "    strings, _ = strings\n",
    "    results = []\n",
    "    for i in tqdm(range(len(strings))):\n",
    "        results.append((strings[i][0], preprocessing_text(strings[i][1])))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_texts = [(i, texts[i]) for i in range(len(texts))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 43.99it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 'happiest moment'),\n",
       " (1, 'yeah'),\n",
       " (2, 'this conversation so'),\n",
       " (3, 'we will still be alive'),\n",
       " (4, 'you talk to her is called emotional cheating'),\n",
       " (5,\n",
       "  'in the past when only urine tests were used they had to be done once or twice a week'),\n",
       " (6,\n",
       "  'country so far is one is in singapore at marina bay sands like usual another one is'),\n",
       " (7, 'i think the public nowadays is getting more and more education'),\n",
       " (8, 'is damn funny you know like or like when when dentist'),\n",
       " (9, 'ya')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop((no_texts[:10], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 53629/73229 [45:02<20:49, 15.69it/s]   "
     ]
    }
   ],
   "source": [
    "processed_text = mp.multiprocessing(no_texts, loop, cores = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1464595"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text_sorted = sorted(processed_text, key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'happiest moment'),\n",
       " (1, 'yeah'),\n",
       " (2, 'this conversation so'),\n",
       " (3, 'we will still be alive'),\n",
       " (4, 'you talk to her is called emotional cheating'),\n",
       " (5,\n",
       "  'in the past when only urine tests were used they had to be done once or twice a week'),\n",
       " (6,\n",
       "  'country so far is one is in singapore at marina bay sands like usual another one is'),\n",
       " (7, 'i think the public nowadays is getting more and more education'),\n",
       " (8, 'is damn funny you know like or like when when dentist'),\n",
       " (9, 'ya')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happiest moment',\n",
       " 'yeah',\n",
       " 'this conversation so',\n",
       " 'we will still be alive',\n",
       " 'you talk to her is called emotional cheating',\n",
       " 'in the past when only urine tests were used they had to be done once or twice a week',\n",
       " 'country so far is one is in singapore at marina bay sands like usual another one is',\n",
       " 'i think the public nowadays is getting more and more education',\n",
       " 'is damn funny you know like or like when when dentist',\n",
       " 'ya']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_only = [i[1] for i in processed_text_sorted]\n",
    "text_only[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya_speech\n",
    "\n",
    "tokenizer = malaya_speech.subword.generate_tokenizer(text_only, max_subword_length = 3)\n",
    "malaya_speech.subword.save(tokenizer, 'transducer-singlish.subword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/husein/speech-bahasa/cv-corpus-5.1-2020-06-22/id/clips/common_voice_id_20425643.mp3']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('bahasa-asr-train.json') as fopen:\n",
    "    bahasa = json.load(fopen)\n",
    "    \n",
    "bahasa['X'][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1464595, 1464595)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audios), len(text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = bahasa['X'] + list(audios)\n",
    "processed_text = bahasa['Y'] + list(text_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mixed-asr-train.json', 'w') as fopen:\n",
    "    json.dump({'X': audios, 'Y':processed_text}, fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = malaya_speech.subword.generate_tokenizer(processed_text, max_subword_length = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "malaya_speech.subword.save(tokenizer, 'transducer-mixed.subword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 389, 862]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = malaya_speech.subword.load('transducer-mixed.subword')\n",
    "malaya_speech.subword.encode(tokenizer, 'i hate', add_blank = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i hate'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "malaya_speech.subword.decode(tokenizer, [0, 2, 389, 862])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydub import AudioSegment\n",
    "# import numpy as np\n",
    "\n",
    "# sr = 16000\n",
    "\n",
    "# def mp3_to_wav(file, sr = sr):\n",
    "#     audio = AudioSegment.from_file(file)\n",
    "#     audio = audio.set_frame_rate(sr).set_channels(1)\n",
    "#     sample = np.array(audio.get_array_of_samples())\n",
    "#     return malaya_speech.astype.int_to_float(sample), sr\n",
    "\n",
    "# def generator(maxlen = 18, min_length_text = 2):\n",
    "#     for i in tqdm(range(len(audios))):\n",
    "#         try:\n",
    "#             if audios[i].endswith('.mp3'):\n",
    "#                 wav_data, _ = mp3_to_wav(audios[i])\n",
    "#             else:\n",
    "#                 wav_data, _ = malaya_speech.load(audios[i])\n",
    "                \n",
    "#             if (len(wav_data) / sr) > maxlen:\n",
    "#                 # print(f'skipped audio too long {audios[i]}')\n",
    "#                 continue\n",
    "                \n",
    "#             if len(processed_text[i]) < min_length_text:\n",
    "#                 print(f'skipped text too short {audios[i]}')\n",
    "#                 continue    \n",
    "\n",
    "#             yield {\n",
    "#                 'waveforms': wav_data.tolist(),\n",
    "#                 'waveform_lens': [len(wav_data)],\n",
    "#                 'targets': malaya_speech.subword.encode(tokenizer, processed_text[i], add_blank = False),\n",
    "#             }\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "            \n",
    "# generator = generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import tensorflow as tf\n",
    "\n",
    "# os.system('rm bahasa-asr/data/*')\n",
    "# DATA_DIR = os.path.expanduser('bahasa-asr/data')\n",
    "# tf.gfile.MakeDirs(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shards = [{'split': 'train', 'shards': 1000}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import malaya_speech.train as train\n",
    "\n",
    "# train.prepare_dataset(generator, DATA_DIR, shards, prefix = 'bahasa-asr')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
