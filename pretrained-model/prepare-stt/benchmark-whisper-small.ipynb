{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model('small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`pyaudio` is not available, `malaya_speech.streaming.pyaudio` is not able to use.\n"
     ]
    }
   ],
   "source": [
    "import malaya_speech\n",
    "import numpy as np\n",
    "\n",
    "def calculate_cer(actual, hyp):\n",
    "    \"\"\"\n",
    "    Calculate CER using `python-Levenshtein`.\n",
    "    \"\"\"\n",
    "    import Levenshtein as Lev\n",
    "\n",
    "    actual = actual.replace(' ', '')\n",
    "    hyp = hyp.replace(' ', '')\n",
    "    return Lev.distance(actual, hyp) / len(actual)\n",
    "\n",
    "\n",
    "def calculate_wer(actual, hyp):\n",
    "    \"\"\"\n",
    "    Calculate WER using `python-Levenshtein`.\n",
    "    \"\"\"\n",
    "    import Levenshtein as Lev\n",
    "\n",
    "    b = set(actual.split() + hyp.split())\n",
    "    word2char = dict(zip(b, range(len(b))))\n",
    "\n",
    "    w1 = [chr(word2char[w]) for w in actual.split()]\n",
    "    w2 = [chr(word2char[w]) for w in hyp.split()]\n",
    "\n",
    "    return Lev.distance(''.join(w1), ''.join(w2)) / len(actual.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya\n",
    "from malaya.text.normalization import cardinal\n",
    "import unicodedata\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "tokenizer = malaya.tokenizer.Tokenizer(hypen = False, parliament = False, time = False, time_pukul = False,\n",
    "                                      temperature = False, distance = False, volume = False, duration = False,\n",
    "                                      weight = False, date = False, money = False)\n",
    "\n",
    "\n",
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\"]\n",
    "\n",
    "def preprocessing_text(string):\n",
    "    \n",
    "    tokenized = tokenizer.tokenize(string)\n",
    "    string = ' '.join(tokenized)\n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c if c in vocabs else ' ' for c in string])\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/home/husein/ssd1/speech-bahasa/malay-asr-test.json') as fopen:\n",
    "    test_set = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 739/739 [07:52<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "wer, cer = [], []\n",
    "\n",
    "for i in tqdm(range(len(test_set['X']))):\n",
    "    batch_y = [test_set['Y'][i]]\n",
    "    y = malaya_speech.load(test_set['X'][i])[0]\n",
    "    o = model.transcribe(y.astype('float32'), task = 'transcribe')\n",
    "    pred = preprocessing_text(o['text'])\n",
    "    wer.append(calculate_wer(test_set['Y'][i], pred))\n",
    "    cer.append(calculate_cer(test_set['Y'][i], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2436472703347558, 0.09136925680411448)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(wer), np.mean(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/husein/malaya-speech/postprocess-malaya-malay-test-set.json') as fopen:\n",
    "    malaya_malay = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 765/765 [05:11<00:00,  2.46it/s]\n"
     ]
    }
   ],
   "source": [
    "wer, cer = [], []\n",
    "\n",
    "for i in tqdm(range(len(malaya_malay))):\n",
    "    if not malaya_malay[i]['accept']:\n",
    "        continue\n",
    "    \n",
    "    y = malaya_speech.load(f'/home/husein/malaya-speech/malay-test/{i}.wav')[0]\n",
    "    o = model.transcribe(y.astype('float32'), task = 'transcribe')\n",
    "    pred = preprocessing_text(o['text'])\n",
    "    \n",
    "    wer.append(calculate_wer(malaya_malay[i]['cleaned'], pred))\n",
    "    cer.append(calculate_cer(malaya_malay[i]['cleaned'], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.2818371132600382, 0.09588120693804351)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(wer), np.mean(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/husein/malaya-speech/singlish-stt-test.json') as fopen:\n",
    "    test_set = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3579/3579 [53:37<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "wer, cer = [], []\n",
    "\n",
    "for i in tqdm(range(len(test_set['X']))):\n",
    "    batch_y = [test_set['Y'][i]]\n",
    "    y = malaya_speech.load(test_set['X'][i])[0]\n",
    "    o = model.transcribe(y.astype('float32'), task = 'transcribe')\n",
    "    pred = preprocessing_text(o['text'])\n",
    "    \n",
    "    wer.append(calculate_wer(test_set['Y'][i], pred))\n",
    "    cer.append(calculate_cer(test_set['Y'][i], pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5971608337432316, 0.500389060166355)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(wer), np.mean(cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
