{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:38: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:39: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:40: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:42: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:43: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/transformer/attention_layer.py:24: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import malaya_speech\n",
    "import tensorflow as tf\n",
    "from malaya_speech.train.model import fastsplit, fastspeech, sepformer\n",
    "from malaya_speech.train.model.transformer import encoder\n",
    "import malaya_speech.augmentation.waveform as augmentation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "wavs = glob('speech/example-speaker/*.wav')\n",
    "len(wavs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "librispeech = glob('../speech-bahasa/LibriSpeech/*/*/*/*.flac')\n",
    "len(librispeech)\n",
    "\n",
    "\n",
    "def get_speaker_librispeech(file):\n",
    "    return file.split('/')[-1].split('-')[0]\n",
    "\n",
    "\n",
    "speakers = defaultdict(list)\n",
    "for f in librispeech:\n",
    "    speakers[get_speaker_librispeech(f)].append(f)\n",
    "\n",
    "vctk = glob('vtck/**/*.flac', recursive = True)\n",
    "vctk_speakers = defaultdict(list)\n",
    "for f in vctk:\n",
    "    s = f.split('/')[-1].split('_')[0]\n",
    "    vctk_speakers[s].append(f)\n",
    "\n",
    "files = glob('../speech-bahasa/ST-CMDS-20170001_1-OS/*.wav')\n",
    "speakers_mandarin = defaultdict(list)\n",
    "for f in files:\n",
    "    speakers_mandarin[f[:-9]].append(f)\n",
    "len(speakers_mandarin)\n",
    "\n",
    "speakers_malay = {}\n",
    "speakers_malay['salina'] = glob(\n",
    "    '../youtube/malay2/salina/output-wav-salina/*.wav'\n",
    ")\n",
    "male = glob('../youtube/malay2/turki/output-wav-turki/*.wav')\n",
    "male.extend(\n",
    "    glob(\n",
    "        '../youtube/malay/dari-pasentran-ke-istana/output-wav-dari-pasentran-ke-istana/*.wav'\n",
    "    )\n",
    ")\n",
    "speakers_malay['male'] = male\n",
    "speakers_malay['haqkiem'] = glob('/home/husein/speech-bahasa/haqkiem/*.wav')\n",
    "husein = glob('/home/husein/speech-bahasa/audio-wattpad/*.wav')\n",
    "husein.extend(glob('/home/husein/speech-bahasa/audio-iium/*.wav'))\n",
    "husein.extend(glob('/home/husein/speech-bahasa/audio/*.wav'))\n",
    "speakers_malay['husein'] = husein\n",
    "\n",
    "s = {**speakers}\n",
    "\n",
    "\n",
    "keys = list(s.keys())\n",
    "\n",
    "\n",
    "def random_speakers(n):\n",
    "    ks = random.sample(keys, n)\n",
    "    r = []\n",
    "    for k in ks:\n",
    "        r.append(random.choice(s[k]))\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sr = 8000\n",
    "speakers_size = 4\n",
    "\n",
    "# noise = malaya_speech.load('noise.wav', sr = sr)[0]\n",
    "\n",
    "def read_wav(f):\n",
    "    return malaya_speech.load(f, sr = sr)\n",
    "\n",
    "\n",
    "def random_sampling(s, length):\n",
    "    return augmentation.random_sampling(s, sr = sr, length = length)\n",
    "\n",
    "def to_mel(y):\n",
    "    mel = malaya_speech.featurization.universal_mel(y)\n",
    "    mel[mel <= np.log(1e-2)] = np.log(1e-2)\n",
    "    return mel\n",
    "\n",
    "def combine_speakers(files, n = 5, limit = 4):\n",
    "    w_samples = random.sample(files, n)\n",
    "    w_samples = [read_wav(f)[0] for f in w_samples]\n",
    "    w_lens = [len(w) / sr for w in w_samples]\n",
    "    print(w_lens)\n",
    "    w_lens = int(min(min(w_lens) * 1000, random.randint(3000, 7000)))\n",
    "    w_samples = [random_sampling(w, length = w_lens) for w in w_samples]\n",
    "    y = [w_samples[0]]\n",
    "    left = w_samples[0].copy()\n",
    "\n",
    "    combined = None\n",
    "\n",
    "    for i in range(1, n):\n",
    "        right = w_samples[i].copy()\n",
    "        overlap = random.uniform(0.98, 1.0)\n",
    "        print(i, overlap)\n",
    "        len_overlap = int(overlap * len(right))\n",
    "        minus = len(left) - len_overlap\n",
    "        if minus < 0:\n",
    "            minus = 0\n",
    "        padded_right = np.pad(right, (minus, 0))\n",
    "        left = np.pad(left, (0, len(padded_right) - len(left)))\n",
    "\n",
    "        left = left + padded_right\n",
    "\n",
    "        if i >= (limit - 1):\n",
    "            if combined is None:\n",
    "                combined = padded_right\n",
    "            else:\n",
    "                combined = np.pad(\n",
    "                    combined, (0, len(padded_right) - len(combined))\n",
    "                )\n",
    "                combined += padded_right\n",
    "\n",
    "        else:\n",
    "            y.append(padded_right)\n",
    "\n",
    "    if combined is not None:\n",
    "        y.append(combined)\n",
    "        \n",
    "    maxs = [max(left)]\n",
    "    for i in range(len(y)):\n",
    "        if len(y[i]) != len(left):\n",
    "            y[i] = np.pad(y[i], (0, len(left) - len(y[i])))\n",
    "            maxs.append(max(y[i]))\n",
    "            \n",
    "    max_amp = max(maxs)\n",
    "    mix_scaling = 1 / max_amp * 0.9\n",
    "    left = left * mix_scaling\n",
    "    \n",
    "    for i in range(len(y)):\n",
    "        y[i] = y[i] * mix_scaling\n",
    "\n",
    "#     for i in range(len(y)):\n",
    "#         if len(y[i]) != len(left):\n",
    "#             y[i] = np.pad(y[i], (0, len(left) - len(y[i])))\n",
    "#             y[i] = y[i] / np.max(np.abs(y[i]))\n",
    "\n",
    "#     left = left / np.max(np.abs(left))\n",
    "        \n",
    "    return left, y\n",
    "\n",
    "# y, _ = malaya_speech.load('../speech/example-speaker/husein-zolkepli.wav')\n",
    "# y = np.expand_dims(y, 0).astype(np.float32)\n",
    "# y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.715, 2.535, 12.04, 6.795]\n",
      "1 0.987890791002743\n",
      "2 0.9954466732271584\n",
      "3 0.9984749981901312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.58125, 4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 4\n",
    "left, y = combine_speakers(random_speakers(count), count)\n",
    "len(left) / sr, len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = malaya_speech.config.transformer_config\n",
    "dim = 256\n",
    "config['hidden_size'] = dim\n",
    "config['num_hidden_layers'] = 8\n",
    "config['attention_dropout'] = 0.0\n",
    "config['relu_dropout'] = 0.0\n",
    "config['filter_size'] = 1024\n",
    "config['norm_before'] = True\n",
    "config['activation'] = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = lambda: encoder.Encoder(config, train = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/transformer/attention_layer.py:41: The name tf.layers.Dense is deprecated. Please use tf.compat.v1.layers.Dense instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = sepformer.Model(transformer, transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/transformer/model_utils.py:31: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/transformer/encoder.py:58: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/transformer/transformer.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/autograph/converters/directives.py:119: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/transformer/ffn_layer.py:94: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/transformer/ffn_layer.py:94: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, None, 1])\n",
    "len_X = tf.placeholder(tf.int32, [None])\n",
    "logits = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = tf.identity(logits, name = 'logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/utils.py:40: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y = tf.placeholder(tf.float32, [None, speakers_size, None])\n",
    "estimate_source = tf.transpose(logits[:, :, :, 0], [1, 0, 2])\n",
    "loss, max_snr, _ = sepformer.calculate_loss(\n",
    "    Y, estimate_source, len_X, C = speakers_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'split-speaker-sepformer/model.ckpt-2571000'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'split-speaker-sepformer'\n",
    "ckpt_path = tf.train.latest_checkpoint(path)\n",
    "ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from split-speaker-sepformer/model.ckpt-2571000\n"
     ]
    }
   ],
   "source": [
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_list)\n",
    "saver.restore(sess, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 4\n",
    "left, y = combine_speakers(wavs, count)\n",
    "len(left) / sr, len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left, y = combine_speakers(random_speakers(count), count)\n",
    "# len(left) / sr, len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l = sess.run(logits, feed_dict = {X: np.expand_dims([left], axis = -1)})\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(l[0,0,:,0], rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([y]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run([loss, max_snr * count], feed_dict = {X: np.expand_dims([left], axis = -1),\n",
    "                              len_X: [len(left)], Y: [y]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(combined_path, speakers_size = 4, sr = 8000):\n",
    "    combined, _ = malaya_speech.load(combined_path, sr = sr, scale = False)\n",
    "    y = []\n",
    "    for i in range(speakers_size):\n",
    "        y_, _ = malaya_speech.load(combined_path.replace('combined', str(i)), sr = sr, scale = False)\n",
    "        y.append(y_)\n",
    "    return combined, y\n",
    "\n",
    "combined = glob('split-speaker-8k-train/combined/*.wav')\n",
    "len(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.949534"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = get_data(combined[1])\n",
    "sess.run(max_snr * count, feed_dict = {X: np.expand_dims([x], -1), \n",
    "                                               len_X: [len(x)], Y: [y]})[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "snrs = []\n",
    "\n",
    "for i in tqdm(range(len(combined))):\n",
    "    x, y = get_data(combined[i])\n",
    "    s = sess.run(max_snr * count, feed_dict = {X: np.expand_dims([x], -1), \n",
    "                                               len_X: [len(x)], Y: [y]})[0,0]\n",
    "    snrs.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(snrs) # 18.645731"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.expand_dims(tf.constant(2), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'sepformer/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'gather' in n.op.lower()\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name)\n",
    "        and 'adam' not in n.name\n",
    "        and 'global_step' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "        and 'ReadVariableOp' not in n.name\n",
    "        and 'Gather' not in n.name\n",
    "        and 'IsVariableInitialized' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_graph('sepformer', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "                \n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "        \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('sepformer/frozen_model.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess = tf.Session(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = g.get_tensor_by_name('import/Placeholder:0')\n",
    "X_len = g.get_tensor_by_name('import/Placeholder_1:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 4\n",
    "left, y = combine_speakers(wavs, count)\n",
    "len(left) / sr, len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l = test_sess.run(logits, feed_dict = {X: np.expand_dims([left], axis = -1)})\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(l[3,0,:,0], rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.graph_transforms import TransformGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = ['add_default_attributes',\n",
    "             'remove_nodes(op=Identity, op=CheckNumerics)',\n",
    "             'fold_batch_norms',\n",
    "             'fold_old_batch_norms',\n",
    "             'quantize_weights(fallback_min=-1024, fallback_max=1024)',\n",
    "             'strip_unused_nodes',\n",
    "             'sort_by_execution_order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb = 'sepformer-6/frozen_model.pb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.FastGFile(pb, 'rb') as f:\n",
    "    input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "transformed_graph_def = TransformGraph(input_graph_def, \n",
    "                                           ['Placeholder'],\n",
    "                                           ['logits'], transforms)\n",
    "    \n",
    "with tf.gfile.GFile(f'{pb}.quantized', 'wb') as f:\n",
    "    f.write(transformed_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('sepformer-6/frozen_model.pb.quantized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess = tf.Session(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = g.get_tensor_by_name('import/Placeholder:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "l = test_sess.run(logits, feed_dict = {X: np.expand_dims([left], axis = -1)})\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(l[2,0,:,0], rate = sr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
