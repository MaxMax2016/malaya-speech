{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/NVIDIA/NeMo/main/examples/speaker_recognition/conf/SpeakerNet_recognition_3x2x512.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat conf/SpeakerNet_recognition_3x2x512.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install nemo-toolkit[asr]==1.0.0b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torchaudio>=0.6.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-10-24 17:47:47 nemo_logging:349] /home/ubuntu/.local/lib/python3.6/site-packages/numba/errors.py:137: UserWarning: Insufficiently recent colorama version found. Numba requires colorama >= 0.3.9\n",
      "      warnings.warn(msg)\n",
      "    \n",
      "[NeMo W 2020-10-24 17:47:48 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.AudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2020-10-24 17:47:48 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.AudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2020-10-24 17:47:48 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.AudioLabelDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2020-10-24 17:47:48 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text._TarredAudioToTextDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2020-10-24 17:47:48 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.TarredAudioToCharDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2020-10-24 17:47:48 experimental:28] Module <class 'nemo.collections.asr.data.audio_to_text.TarredAudioToBPEDataset'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "[NeMo W 2020-10-24 17:47:48 experimental:28] Module <class 'nemo.collections.asr.losses.ctc.CTCLoss'> is experimental, not ready for production and is not fully supported. Use at your own risk.\n",
      "################################################################################\n",
      "### WARNING, path does not exist: KALDI_ROOT=/mnt/matylda5/iveselyk/Tools/kaldi-trunk\n",
      "###          (please add 'export KALDI_ROOT=<your_path>' in your $HOME/.profile)\n",
      "###          (or run as: KALDI_ROOT=<your_path> python <your_script>.py)\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nemo\n",
    "import nemo.collections.asr as nemo_asr\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "config = OmegaConf.load('SpeakerNet_recognition_3x2x512.yaml')\n",
    "# print(OmegaConf.to_yaml(config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/SpeakerNet_verification.nemo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2020-10-24 17:47:49 modelPT:102] Please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: /data/samsungSSD/NVIDIA/repos/NeMo/examples/speaker_recognition/myExps/datasets/combined/train_manifest.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 64\n",
      "    shuffle: true\n",
      "    time_length: 8\n",
      "    \n",
      "[NeMo W 2020-10-24 17:47:49 modelPT:109] Please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /data/samsungSSD/NVIDIA/repos/NeMo/examples/speaker_recognition/myExps/datasets/voxceleb/train/small_manifest.json\n",
      "    sample_rate: 16000\n",
      "    labels: null\n",
      "    batch_size: 128\n",
      "    shuffle: false\n",
      "    time_length: 8\n",
      "    \n",
      "[NeMo W 2020-10-24 17:47:49 nemo_logging:349] /home/ubuntu/.local/lib/python3.6/site-packages/hydra/_internal/utils.py:584: UserWarning: \n",
      "    Field 'params' is deprecated since Hydra 1.0 and will be removed in Hydra 1.1.\n",
      "    Inline the content of params directly at the containing node.\n",
      "    See https://hydra.cc/docs/next/upgrades/0.11_to_1.0/object_instantiation_changes\n",
      "      warnings.warn(category=UserWarning, message=msg)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2020-10-24 17:47:49 features:241] PADDING: 16\n",
      "[NeMo I 2020-10-24 17:47:49 features:258] STFT using torch\n",
      "[NeMo I 2020-10-24 17:47:49 label_models:86] Training with Softmax-CrossEntropy loss\n",
      "[NeMo I 2020-10-24 17:47:49 modelPT:237] Model ExtractSpeakerEmbeddingsModel was successfully restored from SpeakerNet_verification.nemo.\n"
     ]
    }
   ],
   "source": [
    "verification_model = nemo_asr.models.ExtractSpeakerEmbeddingsModel.restore_from('SpeakerNet_verification.nemo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/malay/malaya-speech/malaya_speech/train/model/quartznet/layer.py:6: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import malaya_speech\n",
    "import malaya_speech.train.model.speakernet as speakernet\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32, [None, None, 64])\n",
    "inputs_length = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/malay/malaya-speech/malaya_speech/train/model/speakernet/abstract.py:141: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/malay/malaya-speech/malaya_speech/train/model/speakernet/layer.py:205: conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv1D` instead.\n",
      "WARNING:tensorflow:From /home/ubuntu/.local/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:218: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From /home/ubuntu/malay/malaya-speech/malaya_speech/train/model/speakernet/layer.py:219: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
      "WARNING:tensorflow:From /home/ubuntu/malay/malaya-speech/malaya_speech/train/model/speakernet/layer.py:238: separable_conv1d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.SeparableConv1D` instead.\n",
      "WARNING:tensorflow:From /home/ubuntu/malay/malaya-speech/malaya_speech/train/model/speakernet/abstract.py:369: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/ubuntu/malay/malaya-speech/malaya_speech/train/model/speakernet/model.py:86: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Relu_1:0' shape=(?, 512) dtype=float32>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = speakernet.Model(inputs, inputs_length)\n",
    "model.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [v for v in tf.get_collection('variables')]\n",
    "encoder = [v for v in variables if 'encoder' in v.name]\n",
    "decoder = [v for v in variables if 'encoder' not in v.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder[0].load(verification_model.encoder.encoder[0].res[0][0].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[1].load(verification_model.encoder.encoder[0].res[0][1].weight.detach().numpy())\n",
    "encoder[2].load(verification_model.encoder.encoder[0].res[0][1].bias.detach().numpy())\n",
    "encoder[3].load(verification_model.encoder.encoder[0].res[0][1].running_mean.detach().numpy())\n",
    "encoder[4].load(verification_model.encoder.encoder[0].res[0][1].running_var.detach().numpy())\n",
    "encoder[5].load(verification_model.encoder.encoder[0].mconv[0].conv.weight.permute([2,0,1]).detach().numpy())\n",
    "encoder[6].load(verification_model.encoder.encoder[0].mconv[1].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[7].load(verification_model.encoder.encoder[0].mconv[2].weight.detach().numpy())\n",
    "encoder[8].load(verification_model.encoder.encoder[0].mconv[2].bias.detach().numpy())\n",
    "encoder[9].load(verification_model.encoder.encoder[0].mconv[2].running_mean.detach().numpy())\n",
    "encoder[10].load(verification_model.encoder.encoder[0].mconv[2].running_var.detach().numpy())\n",
    "\n",
    "encoder[11].load(verification_model.encoder.encoder[1].mconv[0].conv.weight.permute([2,0,1]).detach().numpy())\n",
    "encoder[12].load(verification_model.encoder.encoder[1].mconv[1].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[13].load(verification_model.encoder.encoder[1].mconv[2].weight.detach().numpy())\n",
    "encoder[14].load(verification_model.encoder.encoder[1].mconv[2].bias.detach().numpy())\n",
    "encoder[15].load(verification_model.encoder.encoder[1].mconv[2].running_mean.detach().numpy())\n",
    "encoder[16].load(verification_model.encoder.encoder[1].mconv[2].running_var.detach().numpy())\n",
    "encoder[17].load(verification_model.encoder.encoder[1].res[0][0].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[18].load(verification_model.encoder.encoder[1].res[0][1].weight.detach().numpy())\n",
    "encoder[19].load(verification_model.encoder.encoder[1].res[0][1].bias.detach().numpy())\n",
    "encoder[20].load(verification_model.encoder.encoder[1].res[0][1].running_mean.detach().numpy())\n",
    "encoder[21].load(verification_model.encoder.encoder[1].res[0][1].running_var.detach().numpy())\n",
    "encoder[22].load(verification_model.encoder.encoder[1].mconv[5].conv.weight.permute([2,0,1]).detach().numpy())\n",
    "encoder[23].load(verification_model.encoder.encoder[1].mconv[6].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[24].load(verification_model.encoder.encoder[1].mconv[7].weight.detach().numpy())\n",
    "encoder[25].load(verification_model.encoder.encoder[1].mconv[7].bias.detach().numpy())\n",
    "encoder[26].load(verification_model.encoder.encoder[1].mconv[7].running_mean.detach().numpy())\n",
    "encoder[27].load(verification_model.encoder.encoder[1].mconv[7].running_var.detach().numpy())\n",
    "\n",
    "encoder[28].load(verification_model.encoder.encoder[2].mconv[0].conv.weight.permute([2,0,1]).detach().numpy())\n",
    "encoder[29].load(verification_model.encoder.encoder[2].mconv[1].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[30].load(verification_model.encoder.encoder[2].mconv[2].weight.detach().numpy())\n",
    "encoder[31].load(verification_model.encoder.encoder[2].mconv[2].bias.detach().numpy())\n",
    "encoder[32].load(verification_model.encoder.encoder[2].mconv[2].running_mean.detach().numpy())\n",
    "encoder[33].load(verification_model.encoder.encoder[2].mconv[2].running_var.detach().numpy())\n",
    "encoder[34].load(verification_model.encoder.encoder[2].res[0][0].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[35].load(verification_model.encoder.encoder[2].res[0][1].weight.detach().numpy())\n",
    "encoder[36].load(verification_model.encoder.encoder[2].res[0][1].bias.detach().numpy())\n",
    "encoder[37].load(verification_model.encoder.encoder[2].res[0][1].running_mean.detach().numpy())\n",
    "encoder[38].load(verification_model.encoder.encoder[2].res[0][1].running_var.detach().numpy())\n",
    "encoder[39].load(verification_model.encoder.encoder[2].mconv[5].conv.weight.permute([2,0,1]).detach().numpy())\n",
    "encoder[40].load(verification_model.encoder.encoder[2].mconv[6].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[41].load(verification_model.encoder.encoder[2].mconv[7].weight.detach().numpy())\n",
    "encoder[42].load(verification_model.encoder.encoder[2].mconv[7].bias.detach().numpy())\n",
    "encoder[43].load(verification_model.encoder.encoder[2].mconv[7].running_mean.detach().numpy())\n",
    "encoder[44].load(verification_model.encoder.encoder[2].mconv[7].running_var.detach().numpy())\n",
    "\n",
    "encoder[45].load(verification_model.encoder.encoder[3].mconv[0].conv.weight.permute([2,0,1]).detach().numpy())\n",
    "encoder[46].load(verification_model.encoder.encoder[3].mconv[1].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[47].load(verification_model.encoder.encoder[3].mconv[2].weight.detach().numpy())\n",
    "encoder[48].load(verification_model.encoder.encoder[3].mconv[2].bias.detach().numpy())\n",
    "encoder[49].load(verification_model.encoder.encoder[3].mconv[2].running_mean.detach().numpy())\n",
    "encoder[50].load(verification_model.encoder.encoder[3].mconv[2].running_var.detach().numpy())\n",
    "encoder[51].load(verification_model.encoder.encoder[3].res[0][0].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[52].load(verification_model.encoder.encoder[3].res[0][1].weight.detach().numpy())\n",
    "encoder[53].load(verification_model.encoder.encoder[3].res[0][1].bias.detach().numpy())\n",
    "encoder[54].load(verification_model.encoder.encoder[3].res[0][1].running_mean.detach().numpy())\n",
    "encoder[55].load(verification_model.encoder.encoder[3].res[0][1].running_var.detach().numpy())\n",
    "encoder[56].load(verification_model.encoder.encoder[3].mconv[5].conv.weight.permute([2,0,1]).detach().numpy())\n",
    "encoder[57].load(verification_model.encoder.encoder[3].mconv[6].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[58].load(verification_model.encoder.encoder[3].mconv[7].weight.detach().numpy())\n",
    "encoder[59].load(verification_model.encoder.encoder[3].mconv[7].bias.detach().numpy())\n",
    "encoder[60].load(verification_model.encoder.encoder[3].mconv[7].running_mean.detach().numpy())\n",
    "encoder[61].load(verification_model.encoder.encoder[3].mconv[7].running_var.detach().numpy())\n",
    "\n",
    "encoder[62].load(verification_model.encoder.encoder[4].mconv[0].conv.weight.permute([2,0,1]).detach().numpy())\n",
    "encoder[63].load(verification_model.encoder.encoder[4].mconv[1].conv.weight.permute([2,1,0]).detach().numpy())\n",
    "encoder[64].load(verification_model.encoder.encoder[4].mconv[2].weight.detach().numpy())\n",
    "encoder[65].load(verification_model.encoder.encoder[4].mconv[2].bias.detach().numpy())\n",
    "encoder[66].load(verification_model.encoder.encoder[4].mconv[2].running_mean.detach().numpy())\n",
    "encoder[67].load(verification_model.encoder.encoder[4].mconv[2].running_var.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder[0].load(verification_model.decoder.emb_layers[0][0].weight.T.detach().numpy())\n",
    "decoder[1].load(verification_model.decoder.emb_layers[0][0].bias.detach().numpy())\n",
    "decoder[4].load(verification_model.decoder.emb_layers[0][1].running_mean.detach().numpy())\n",
    "decoder[5].load(verification_model.decoder.emb_layers[0][1].running_var.detach().numpy())\n",
    "\n",
    "decoder[6].load(verification_model.decoder.emb_layers[1][0].weight.T.detach().numpy())\n",
    "decoder[7].load(verification_model.decoder.emb_layers[1][0].bias.detach().numpy())\n",
    "decoder[10].load(verification_model.decoder.emb_layers[1][1].running_mean.detach().numpy())\n",
    "decoder[11].load(verification_model.decoder.emb_layers[1][1].running_var.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'speakernet/model.ckpt'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'speakernet/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya_speech.config\n",
    "\n",
    "config = malaya_speech.config.speakernet_featurizer_config\n",
    "featurizer = malaya_speech.featurization.SpeakerNetFeaturizer(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speech/example-speaker/khalil-nooh.wav',\n",
       " 'speech/example-speaker/husein-zolkepli.wav',\n",
       " 'speech/example-speaker/mas-aisyah.wav',\n",
       " 'speech/example-speaker/shafiqah-idayu.wav']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "\n",
    "speakers = glob('speech/example-speaker/*.wav')\n",
    "speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 564, 64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wavs = [malaya_speech.load(f, sr = 16000)[0] for f in speakers]\n",
    "vectors = [featurizer.vectorize(w) for w in wavs]\n",
    "padded, l = malaya_speech.padding.sequence_nd(vectors, dim = 0, return_len = True)\n",
    "padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = sess.run(tf.math.l2_normalize(model.logits, axis=1), feed_dict = {inputs: padded, inputs_length: l})\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00000000e+00, 8.52561152e-03, 1.28270620e-02, 1.47797685e-02],\n",
       "       [8.52561152e-03, 1.11022302e-16, 1.37110717e-02, 1.30838853e-02],\n",
       "       [1.28270620e-02, 1.37110717e-02, 0.00000000e+00, 1.60428165e-02],\n",
       "       [1.47797685e-02, 1.30838853e-02, 1.60428165e-02, 0.00000000e+00]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "cdist(logits, logits, metric = 'cosine')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
