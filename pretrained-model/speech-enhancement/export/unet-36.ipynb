{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "SOURCE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__name__)))\n",
    "sys.path.insert(0, SOURCE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:38: The name tf.train.AdagradOptimizer is deprecated. Please use tf.compat.v1.train.AdagradOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:39: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:40: The name tf.train.FtrlOptimizer is deprecated. Please use tf.compat.v1.train.FtrlOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:42: The name tf.train.RMSPropOptimizer is deprecated. Please use tf.compat.v1.train.RMSPropOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/optimizer/__init__.py:43: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/openseq2seq/layer.py:6: The name tf.layers.Conv1D is deprecated. Please use tf.compat.v1.layers.Conv1D instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/husein/malaya-speech/malaya_speech/train/model/openseq2seq/attention.py:4: The name tf.layers.Layer is deprecated. Please use tf.compat.v1.layers.Layer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import malaya_speech\n",
    "import malaya_speech.config\n",
    "import malaya_speech.augmentation.waveform as augmentation\n",
    "import malaya_speech.train.model.unet_enhancement as unet\n",
    "from malaya_speech.train.model import enhancement\n",
    "from malaya_speech.utils import tf_featurization\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 22050\n",
    "partition_size = 4096\n",
    "length = 5500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Tensor(\"strided_slice_3:0\", shape=(?, 2048, 36), dtype=float32)\n",
      "Tensor(\"strided_slice_4:0\", shape=(?, 1024, 72), dtype=float32)\n",
      "Tensor(\"strided_slice_5:0\", shape=(?, 512, 108), dtype=float32)\n",
      "Tensor(\"strided_slice_6:0\", shape=(?, 256, 144), dtype=float32)\n",
      "Tensor(\"strided_slice_7:0\", shape=(?, 128, 180), dtype=float32)\n",
      "Tensor(\"strided_slice_8:0\", shape=(?, 64, 216), dtype=float32)\n",
      "Tensor(\"strided_slice_9:0\", shape=(?, 32, 252), dtype=float32)\n",
      "Tensor(\"strided_slice_10:0\", shape=(?, 16, 288), dtype=float32)\n",
      "Tensor(\"strided_slice_11:0\", shape=(?, 8, 324), dtype=float32)\n",
      "Tensor(\"strided_slice_12:0\", shape=(?, 4, 360), dtype=float32)\n",
      "Tensor(\"strided_slice_13:0\", shape=(?, 2, 396), dtype=float32)\n",
      "Tensor(\"strided_slice_14:0\", shape=(?, 1, 432), dtype=float32)\n",
      "Tensor(\"sequential_12/leaky_re_lu_12/LeakyRelu:0\", shape=(?, 1, 432), dtype=float32)\n",
      "Tensor(\"sequential_13/leaky_re_lu_13/LeakyRelu:0\", shape=(?, 2, 432), dtype=float32)\n",
      "Tensor(\"sequential_14/leaky_re_lu_14/LeakyRelu:0\", shape=(?, 4, 396), dtype=float32)\n",
      "Tensor(\"sequential_15/leaky_re_lu_15/LeakyRelu:0\", shape=(?, 8, 360), dtype=float32)\n",
      "Tensor(\"sequential_16/leaky_re_lu_16/LeakyRelu:0\", shape=(?, 16, 324), dtype=float32)\n",
      "Tensor(\"sequential_17/leaky_re_lu_17/LeakyRelu:0\", shape=(?, 32, 288), dtype=float32)\n",
      "Tensor(\"sequential_18/leaky_re_lu_18/LeakyRelu:0\", shape=(?, 64, 252), dtype=float32)\n",
      "Tensor(\"sequential_19/leaky_re_lu_19/LeakyRelu:0\", shape=(?, 128, 216), dtype=float32)\n",
      "Tensor(\"sequential_20/leaky_re_lu_20/LeakyRelu:0\", shape=(?, 256, 180), dtype=float32)\n",
      "Tensor(\"sequential_21/leaky_re_lu_21/LeakyRelu:0\", shape=(?, 512, 144), dtype=float32)\n",
      "Tensor(\"sequential_22/leaky_re_lu_22/LeakyRelu:0\", shape=(?, 1024, 108), dtype=float32)\n",
      "Tensor(\"sequential_23/leaky_re_lu_23/LeakyRelu:0\", shape=(?, 2048, 72), dtype=float32)\n",
      "Tensor(\"sequential_24/leaky_re_lu_24/LeakyRelu:0\", shape=(?, 4096, 36), dtype=float32)\n",
      "Tensor(\"sequential_24/leaky_re_lu_24/LeakyRelu:0\", shape=(?, 4096, 36), dtype=float32) Tensor(\"Reshape:0\", shape=(?, 4096, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None,))\n",
    "x_ = tf.expand_dims(x, 1)\n",
    "partitioned_x = tf_featurization.pad_and_partition(x_, partition_size)\n",
    "model = unet.Model(partitioned_x, channels_interval = 36)\n",
    "\n",
    "model.logits.set_shape((None, partition_size, 1))\n",
    "logits = tf.reshape(model.logits, (-1, 1))\n",
    "logits = logits[:tf.shape(x)[0], 0]\n",
    "\n",
    "logits = tf.identity(logits, name = 'logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'speech-enhancement-unet-36/model.ckpt-2500000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'speech-enhancement-unet-36'\n",
    "ckpt_path = tf.train.latest_checkpoint(path)\n",
    "ckpt_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from speech-enhancement-unet-36/model.ckpt-2500000\n"
     ]
    }
   ],
   "source": [
    "var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_list)\n",
    "saver.restore(sess, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import museval\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(f):\n",
    "    return f.split('/')[-1].split('-')[0]\n",
    "\n",
    "\n",
    "def read_wav(f):\n",
    "    return malaya_speech.load(f, sr = sr)\n",
    "\n",
    "Y = glob('testset-speech-enhancement/*-y.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [06:37<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "sdrs, isrs, sars = [], [], []\n",
    "\n",
    "for y in tqdm(Y):\n",
    "    y_ = f'testset-speech-enhancement/{get_pair(y)}-y_.wav'\n",
    "    y, _ = read_wav(y)\n",
    "    y_, _ = read_wav(y_)\n",
    "    y__ = sess.run(logits, feed_dict = {x: y_})\n",
    "    sdr, isr, _, sar = museval.evaluate(np.expand_dims(y__, 0), \n",
    "                                    np.expand_dims(y, 0))\n",
    "    sdrs.append(np.nanmean(sdr))\n",
    "    isrs.append(np.nanmean(isr))\n",
    "    sars.append(np.nanmean(sar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.749694447030965, 14.512658736655416, 13.963656608371153)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sdrs), np.mean(isrs), np.mean(sars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l, sr_ = malaya_speech.load('wadi-annuar.wav', sr = sr)\n",
    "len(l) / sr_, sr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ipd.Audio(l, rate = sr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = sess.run(logits, feed_dict = {x: l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipd.Audio(y_, rate = sr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(l, alpha = 0.5, label = 'c')\n",
    "plt.plot(y_, alpha = 0.5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'speech-enhancement-36/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name)\n",
    "        and 'adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "        and 'global_step' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "    ]\n",
    ")\n",
    "strings.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeze_graph('speech-enhancement-36', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph(frozen_graph_filename, **kwargs):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # https://github.com/onnx/tensorflow-onnx/issues/77#issuecomment-445066091\n",
    "    # to fix import T5\n",
    "    for node in graph_def.node:\n",
    "        if node.op == 'RefSwitch':\n",
    "            node.op = 'Switch'\n",
    "            for index in xrange(len(node.input)):\n",
    "                if 'moving_' in node.input[index]:\n",
    "                    node.input[index] = node.input[index] + '/read'\n",
    "        elif node.op == 'AssignSub':\n",
    "            node.op = 'Sub'\n",
    "            if 'use_locking' in node.attr:\n",
    "                del node.attr['use_locking']\n",
    "        elif node.op == 'AssignAdd':\n",
    "            node.op = 'Add'\n",
    "            if 'use_locking' in node.attr:\n",
    "                del node.attr['use_locking']\n",
    "        elif node.op == 'Assign':\n",
    "            node.op = 'Identity'\n",
    "            if 'use_locking' in node.attr:\n",
    "                del node.attr['use_locking']\n",
    "            if 'validate_shape' in node.attr:\n",
    "                del node.attr['validate_shape']\n",
    "            if len(node.input) == 2:\n",
    "                node.input[0] = node.input[1]\n",
    "                del node.input[1]\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('speech-enhancement-36/frozen_model.pb')\n",
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = test_sess.run(logits, feed_dict = {x: l})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(y_, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.tools.graph_transforms import TransformGraph\n",
    "\n",
    "pb = 'speech-enhancement-36/frozen_model.pb'\n",
    "\n",
    "transforms = ['add_default_attributes',\n",
    "             'remove_nodes(op=Identity, op=CheckNumerics)',\n",
    "             'fold_batch_norms',\n",
    "             'fold_old_batch_norms',\n",
    "             'quantize_weights(fallback_min=-1024, fallback_max=1024)',\n",
    "             'strip_unused_nodes',\n",
    "             'sort_by_execution_order']\n",
    "\n",
    "input_graph_def = tf.GraphDef()\n",
    "with tf.gfile.FastGFile(pb, 'rb') as f:\n",
    "    input_graph_def.ParseFromString(f.read())\n",
    "\n",
    "transformed_graph_def = TransformGraph(input_graph_def, \n",
    "                                           ['Placeholder'],\n",
    "                                           ['logits'], transforms)\n",
    "    \n",
    "with tf.gfile.GFile(f'{pb}.quantized', 'wb') as f:\n",
    "    f.write(transformed_graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('speech-enhancement-36/frozen_model.pb.quantized')\n",
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')\n",
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = test_sess.run(logits, feed_dict = {x: l})\n",
    "ipd.Audio(y_, rate = sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
