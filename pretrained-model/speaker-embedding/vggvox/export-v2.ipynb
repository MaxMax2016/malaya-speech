{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Activation, Conv1D, Conv2D, Input, Lambda\n",
    "from tensorflow.keras.layers import BatchNormalization, Flatten, Dense, Reshape\n",
    "from tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block_2D(input_tensor, kernel_size, filters, stage, block, trainable=True):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filterss of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = 3\n",
    "\n",
    "    conv_name_1 = 'conv' + str(stage) + '_' + str(block) + '_1x1_reduce'\n",
    "    bn_name_1 = 'conv' + str(stage) + '_' + str(block) + '_1x1_reduce/bn'\n",
    "    x = Conv2D(filters1, (1, 1),\n",
    "               kernel_initializer='orthogonal',\n",
    "               use_bias=False,\n",
    "               trainable=trainable,\n",
    "               kernel_regularizer=l2(weight_decay),\n",
    "               name=conv_name_1)(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    conv_name_2 = 'conv' + str(stage) + '_' + str(block) + '_3x3'\n",
    "    bn_name_2 = 'conv' + str(stage) + '_' + str(block) + '_3x3/bn'\n",
    "    x = Conv2D(filters2, kernel_size,\n",
    "               padding='same',\n",
    "               kernel_initializer='orthogonal',\n",
    "               use_bias=False,\n",
    "               trainable=trainable,\n",
    "               kernel_regularizer=l2(weight_decay),\n",
    "               name=conv_name_2)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_2)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    conv_name_3 = 'conv' + str(stage) + '_' + str(block) + '_1x1_increase'\n",
    "    bn_name_3 = 'conv' + str(stage) + '_' + str(block) + '_1x1_increase/bn'\n",
    "    x = Conv2D(filters3, (1, 1),\n",
    "               kernel_initializer='orthogonal',\n",
    "               use_bias=False,\n",
    "               trainable=trainable,\n",
    "               kernel_regularizer=l2(weight_decay),\n",
    "               name=conv_name_3)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_3)(x)\n",
    "\n",
    "    x = layers.add([x, input_tensor])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def conv_block_2D(input_tensor, kernel_size, filters, stage, block, strides=(2, 2), trainable=True):\n",
    "    \"\"\"A block that has a conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of middle conv layer at main path\n",
    "        filters: list of integers, the filterss of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    Note that from stage 3, the first conv layer at main path is with strides=(2,2)\n",
    "    And the shortcut should have strides=(2,2) as well\n",
    "    \"\"\"\n",
    "    filters1, filters2, filters3 = filters\n",
    "    bn_axis = 3\n",
    "\n",
    "    conv_name_1 = 'conv' + str(stage) + '_' + str(block) + '_1x1_reduce'\n",
    "    bn_name_1 = 'conv' + str(stage) + '_' + str(block) + '_1x1_reduce/bn'\n",
    "    x = Conv2D(filters1, (1, 1),\n",
    "               strides=strides,\n",
    "               kernel_initializer='orthogonal',\n",
    "               use_bias=False,\n",
    "               trainable=trainable,\n",
    "               kernel_regularizer=l2(weight_decay),\n",
    "               name=conv_name_1)(input_tensor)\n",
    "    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_1)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    conv_name_2 = 'conv' + str(stage) + '_' + str(block) + '_3x3'\n",
    "    bn_name_2 = 'conv' + str(stage) + '_' + str(block) + '_3x3/bn'\n",
    "    x = Conv2D(filters2, kernel_size, padding='same',\n",
    "               kernel_initializer='orthogonal',\n",
    "               use_bias=False,\n",
    "               trainable=trainable,\n",
    "               kernel_regularizer=l2(weight_decay),\n",
    "               name=conv_name_2)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_2)(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    conv_name_3 = 'conv' + str(stage) + '_' + str(block) + '_1x1_increase'\n",
    "    bn_name_3 = 'conv' + str(stage) + '_' + str(block) + '_1x1_increase/bn'\n",
    "    x = Conv2D(filters3, (1, 1),\n",
    "               kernel_initializer='orthogonal',\n",
    "               use_bias=False,\n",
    "               trainable=trainable,\n",
    "               kernel_regularizer=l2(weight_decay),\n",
    "               name=conv_name_3)(x)\n",
    "    x = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_3)(x)\n",
    "\n",
    "    conv_name_4 = 'conv' + str(stage) + '_' + str(block) + '_1x1_proj'\n",
    "    bn_name_4 = 'conv' + str(stage) + '_' + str(block) + '_1x1_proj/bn'\n",
    "    shortcut = Conv2D(filters3, (1, 1), strides=strides,\n",
    "                      kernel_initializer='orthogonal',\n",
    "                      use_bias=False,\n",
    "                      trainable=trainable,\n",
    "                      kernel_regularizer=l2(weight_decay),\n",
    "                      name=conv_name_4)(input_tensor)\n",
    "    shortcut = BatchNormalization(axis=bn_axis, trainable=trainable, name=bn_name_4)(shortcut)\n",
    "\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_2D_v1(inputs, mode='train'):\n",
    "    bn_axis = 3\n",
    "#     if mode == 'train':\n",
    "#         inputs = Input(shape=input_dim, name='input')\n",
    "#     else:\n",
    "#         inputs = Input(shape=(input_dim[0], None, input_dim[-1]), name='input')\n",
    "    # ===============================================\n",
    "    #            Convolution Block 1\n",
    "    # ===============================================\n",
    "    x1 = Conv2D(64, (7, 7),\n",
    "                kernel_initializer='orthogonal',\n",
    "                use_bias=False, trainable=True,\n",
    "                kernel_regularizer=l2(weight_decay),\n",
    "                padding='same',\n",
    "                name='conv1_1/3x3_s1')(inputs)\n",
    "\n",
    "    x1 = BatchNormalization(axis=bn_axis, name='conv1_1/3x3_s1/bn', trainable=True)(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    x1 = MaxPooling2D((2, 2), strides=(2, 2))(x1)\n",
    "\n",
    "    # ===============================================\n",
    "    #            Convolution Section 2\n",
    "    # ===============================================\n",
    "    x2 = conv_block_2D(x1, 3, [48, 48, 96], stage=2, block='a', strides=(1, 1), trainable=True)\n",
    "    x2 = identity_block_2D(x2, 3, [48, 48, 96], stage=2, block='b', trainable=True)\n",
    "\n",
    "    # ===============================================\n",
    "    #            Convolution Section 3\n",
    "    # ===============================================\n",
    "    x3 = conv_block_2D(x2, 3, [96, 96, 128], stage=3, block='a', trainable=True)\n",
    "    x3 = identity_block_2D(x3, 3, [96, 96, 128], stage=3, block='b', trainable=True)\n",
    "    x3 = identity_block_2D(x3, 3, [96, 96, 128], stage=3, block='c', trainable=True)\n",
    "    # ===============================================\n",
    "    #            Convolution Section 4\n",
    "    # ===============================================\n",
    "    x4 = conv_block_2D(x3, 3, [128, 128, 256], stage=4, block='a', trainable=True)\n",
    "    x4 = identity_block_2D(x4, 3, [128, 128, 256], stage=4, block='b', trainable=True)\n",
    "    x4 = identity_block_2D(x4, 3, [128, 128, 256], stage=4, block='c', trainable=True)\n",
    "    # ===============================================\n",
    "    #            Convolution Section 5\n",
    "    # ===============================================\n",
    "    x5 = conv_block_2D(x4, 3, [256, 256, 512], stage=5, block='a', trainable=True)\n",
    "    x5 = identity_block_2D(x5, 3, [256, 256, 512], stage=5, block='b', trainable=True)\n",
    "    x5 = identity_block_2D(x5, 3, [256, 256, 512], stage=5, block='c', trainable=True)\n",
    "    y = MaxPooling2D((3, 1), strides=(2, 1), name='mpool2')(x5)\n",
    "    return inputs, y\n",
    "\n",
    "\n",
    "def resnet_2D_v2(inputs, mode='train'):\n",
    "    bn_axis = 3\n",
    "#     if mode == 'train':\n",
    "#         inputs = Input(shape=input_dim, name='input')\n",
    "#     else:\n",
    "#         inputs = Input(shape=(input_dim[0], None, input_dim[-1]), name='input')\n",
    "    # ===============================================\n",
    "    #            Convolution Block 1\n",
    "    # ===============================================\n",
    "    x1 = Conv2D(64, (7, 7), strides=(2, 2),\n",
    "                kernel_initializer='orthogonal',\n",
    "                use_bias=False, trainable=True,\n",
    "                kernel_regularizer=l2(weight_decay),\n",
    "                padding='same',\n",
    "                name='conv1_1/3x3_s1')(inputs)\n",
    "\n",
    "    x1 = BatchNormalization(axis=bn_axis, name='conv1_1/3x3_s1/bn', trainable=True)(x1)\n",
    "    x1 = Activation('relu')(x1)\n",
    "    x1 = MaxPooling2D((2, 2), strides=(2, 2))(x1)\n",
    "\n",
    "    # ===============================================\n",
    "    #            Convolution Section 2\n",
    "    # ===============================================\n",
    "    x2 = conv_block_2D(x1, 3, [64, 64, 256], stage=2, block='a', strides=(1, 1), trainable=True)\n",
    "    x2 = identity_block_2D(x2, 3, [64, 64, 256], stage=2, block='b', trainable=True)\n",
    "    x2 = identity_block_2D(x2, 3, [64, 64, 256], stage=2, block='c', trainable=True)\n",
    "    # ===============================================\n",
    "    #            Convolution Section 3\n",
    "    # ===============================================\n",
    "    x3 = conv_block_2D(x2, 3, [128, 128, 512], stage=3, block='a', trainable=True)\n",
    "    x3 = identity_block_2D(x3, 3, [128, 128, 512], stage=3, block='b', trainable=True)\n",
    "    x3 = identity_block_2D(x3, 3, [128, 128, 512], stage=3, block='c', trainable=True)\n",
    "    # ===============================================\n",
    "    #            Convolution Section 4\n",
    "    # ===============================================\n",
    "    x4 = conv_block_2D(x3, 3, [256, 256, 1024], stage=4, block='a', strides=(1, 1), trainable=True)\n",
    "    x4 = identity_block_2D(x4, 3, [256, 256, 1024], stage=4, block='b', trainable=True)\n",
    "    x4 = identity_block_2D(x4, 3, [256, 256, 1024], stage=4, block='c', trainable=True)\n",
    "    # ===============================================\n",
    "    #            Convolution Section 5\n",
    "    # ===============================================\n",
    "    x5 = conv_block_2D(x4, 3, [512, 512, 2048], stage=5, block='a', trainable=True)\n",
    "    x5 = identity_block_2D(x5, 3, [512, 512, 2048], stage=5, block='b', trainable=True)\n",
    "    x5 = identity_block_2D(x5, 3, [512, 512, 2048], stage=5, block='c', trainable=True)\n",
    "    y = MaxPooling2D((3, 1), strides=(2, 1), name='mpool2')(x5)\n",
    "    return inputs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VladPooling(keras.layers.Layer):\n",
    "    '''\n",
    "    This layer follows the NetVlad, GhostVlad\n",
    "    '''\n",
    "    def __init__(self, mode, k_centers, g_centers=0, **kwargs):\n",
    "        self.k_centers = k_centers\n",
    "        self.g_centers = g_centers\n",
    "        self.mode = mode\n",
    "        super(VladPooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.cluster = self.add_weight(shape=[self.k_centers+self.g_centers, input_shape[0][-1]],\n",
    "                                       name='centers',\n",
    "                                       initializer='orthogonal')\n",
    "        self.built = True\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape\n",
    "        return (input_shape[0][0], self.k_centers*input_shape[0][-1])\n",
    "\n",
    "    def call(self, x):\n",
    "        # feat : bz x W x H x D, cluster_score: bz X W x H x clusters.\n",
    "        feat, cluster_score = x\n",
    "        num_features = feat.shape[-1]\n",
    "\n",
    "        # softmax normalization to get soft-assignment.\n",
    "        # A : bz x W x H x clusters\n",
    "        max_cluster_score = K.max(cluster_score, -1, keepdims=True)\n",
    "        exp_cluster_score = K.exp(cluster_score - max_cluster_score)\n",
    "        A = exp_cluster_score / K.sum(exp_cluster_score, axis=-1, keepdims = True)\n",
    "\n",
    "        # Now, need to compute the residual, self.cluster: clusters x D\n",
    "        A = K.expand_dims(A, -1)    # A : bz x W x H x clusters x 1\n",
    "        feat_broadcast = K.expand_dims(feat, -2)    # feat_broadcast : bz x W x H x 1 x D\n",
    "        feat_res = feat_broadcast - self.cluster    # feat_res : bz x W x H x clusters x D\n",
    "        weighted_res = tf.multiply(A, feat_res)     # weighted_res : bz x W x H x clusters x D\n",
    "        cluster_res = K.sum(weighted_res, [1, 2])\n",
    "\n",
    "        if self.mode == 'gvlad':\n",
    "            cluster_res = cluster_res[:, :self.k_centers, :]\n",
    "\n",
    "        cluster_l2 = K.l2_normalize(cluster_res, -1)\n",
    "        outputs = K.reshape(cluster_l2, [-1, int(self.k_centers) * int(num_features)])\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def amsoftmax_loss(y_true, y_pred, scale=30, margin=0.35):\n",
    "    y_pred = y_true * (y_pred - margin) + (1 - y_true) * y_pred\n",
    "    y_pred *= scale\n",
    "    return K.categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
    "\n",
    "\n",
    "def vggvox_resnet2d_icassp(inputs, num_class=8631, mode='train', args=None):\n",
    "    \n",
    "    # python predict.py --gpu 1 --net resnet34s --ghost_cluster 2 \n",
    "    # --vlad_cluster 8 --loss softmax --resume\n",
    "    \n",
    "    net='resnet34s'\n",
    "    loss='softmax'\n",
    "    vlad_clusters=8\n",
    "    ghost_clusters=2\n",
    "    bottleneck_dim=512\n",
    "    aggregation = 'gvlad'\n",
    "    mgpu = 0\n",
    "\n",
    "    if net == 'resnet34s':\n",
    "        inputs, x = resnet_2D_v1(inputs, mode=mode)\n",
    "    else:\n",
    "        inputs, x = resnet_2D_v2(inputs, mode=mode)\n",
    "    # ===============================================\n",
    "    #            Fully Connected Block 1\n",
    "    # ===============================================\n",
    "    x_fc = keras.layers.Conv2D(bottleneck_dim, (7, 1),\n",
    "                               strides=(1, 1),\n",
    "                               activation='relu',\n",
    "                               kernel_initializer='orthogonal',\n",
    "                               use_bias=True, trainable=True,\n",
    "                               kernel_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                               bias_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                               name='x_fc')(x)\n",
    "\n",
    "    # ===============================================\n",
    "    #            Feature Aggregation\n",
    "    # ===============================================\n",
    "    if aggregation == 'avg':\n",
    "        if mode == 'train':\n",
    "            x = keras.layers.AveragePooling2D((1, 5), strides=(1, 1), name='avg_pool')(x)\n",
    "            x = keras.layers.Reshape((-1, bottleneck_dim))(x)\n",
    "        else:\n",
    "            x = keras.layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "            x = keras.layers.Reshape((1, bottleneck_dim))(x)\n",
    "\n",
    "    elif aggregation == 'vlad':\n",
    "        x_k_center = keras.layers.Conv2D(vlad_clusters, (7, 1),\n",
    "                                         strides=(1, 1),\n",
    "                                         kernel_initializer='orthogonal',\n",
    "                                         use_bias=True, trainable=True,\n",
    "                                         kernel_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                                         bias_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                                         name='vlad_center_assignment')(x)\n",
    "        x = VladPooling(k_centers=vlad_clusters, mode='vlad', name='vlad_pool')([x_fc, x_k_center])\n",
    "\n",
    "    elif aggregation == 'gvlad':\n",
    "        x_k_center = keras.layers.Conv2D(vlad_clusters+ghost_clusters, (7, 1),\n",
    "                                         strides=(1, 1),\n",
    "                                         kernel_initializer='orthogonal',\n",
    "                                         use_bias=True, trainable=True,\n",
    "                                         kernel_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                                         bias_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                                         name='gvlad_center_assignment')(x)\n",
    "        x = VladPooling(k_centers=vlad_clusters, g_centers=ghost_clusters, mode='gvlad', name='gvlad_pool')([x_fc, x_k_center])\n",
    "\n",
    "    else:\n",
    "        raise IOError('==> unknown aggregation mode')\n",
    "\n",
    "    # ===============================================\n",
    "    #            Fully Connected Block 2\n",
    "    # ===============================================\n",
    "    x = keras.layers.Dense(bottleneck_dim, activation='relu',\n",
    "                           kernel_initializer='orthogonal',\n",
    "                           use_bias=True, trainable=True,\n",
    "                           kernel_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                           bias_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                           name='fc6')(x)\n",
    "\n",
    "    # ===============================================\n",
    "    #            Softmax Vs AMSoftmax\n",
    "    # ===============================================\n",
    "    if loss == 'softmax':\n",
    "        y = keras.layers.Dense(num_class, activation='softmax',\n",
    "                               kernel_initializer='orthogonal',\n",
    "                               use_bias=False, trainable=True,\n",
    "                               kernel_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                               bias_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                               name='prediction')(x)\n",
    "        trnloss = 'categorical_crossentropy'\n",
    "\n",
    "    elif loss == 'amsoftmax':\n",
    "        x_l2 = keras.layers.Lambda(lambda x: K.l2_normalize(x, 1))(x)\n",
    "        y = keras.layers.Dense(num_class,\n",
    "                               kernel_initializer='orthogonal',\n",
    "                               use_bias=False, trainable=True,\n",
    "                               kernel_constraint=keras.constraints.unit_norm(),\n",
    "                               kernel_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                               bias_regularizer=keras.regularizers.l2(weight_decay),\n",
    "                               name='prediction')(x_l2)\n",
    "        trnloss = amsoftmax_loss\n",
    "\n",
    "    else:\n",
    "        raise IOError('==> unknown loss.')\n",
    "\n",
    "    if mode == 'eval':\n",
    "        y = keras.layers.Lambda(lambda x: keras.backend.l2_normalize(x, 1))(x)\n",
    "        \n",
    "    return y\n",
    "\n",
    "#     model = keras.models.Model(inputs, y, name='vggvox_resnet2D_{}_{}'.format(loss, aggregation))\n",
    "\n",
    "#     if mode == 'train':\n",
    "#         if mgpu > 1:\n",
    "#             model = ModelMGPU(model, gpus=mgpu)\n",
    "#         # set up optimizer.\n",
    "#         if args.optimizer == 'adam':  opt = keras.optimizers.Adam(lr=1e-3)\n",
    "#         elif args.optimizer =='sgd':  opt = keras.optimizers.SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=True)\n",
    "#         else: raise IOError('==> unknown optimizer type')\n",
    "#         model.compile(optimizer=opt, loss=trnloss, metrics=['acc'])\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.X = tf.placeholder(tf.float32, [None, 257, None, 1])\n",
    "        \n",
    "        params = {'dim': (257, None, 1),\n",
    "            'nfft': 512,\n",
    "            'spec_len': 250,\n",
    "            'win_length': 400,\n",
    "            'hop_length': 160,\n",
    "            'n_classes': 5994,\n",
    "            'sampling_rate': 16000,\n",
    "            'normalize': True,\n",
    "        }\n",
    "        self.logits = vggvox_resnet2d_icassp(self.X, num_class=params['n_classes'], mode='eval')\n",
    "        self.logits = tf.identity(self.logits, name = 'logits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_path = 'v2/vggvox.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "model = Model()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from v2/vggvox.ckpt\n"
     ]
    }
   ],
   "source": [
    "var_lists = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "saver = tf.train.Saver(var_list = var_lists)\n",
    "saver.restore(sess, ckpt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# ===============================================\n",
    "#       code from Arsha for loading data.\n",
    "# ===============================================\n",
    "def load_wav(vid_path, sr, mode='train'):\n",
    "    wav, sr_ret = librosa.load(vid_path, sr=sr)\n",
    "    assert sr_ret == sr\n",
    "    if mode == 'train':\n",
    "        extended_wav = np.append(wav, wav)\n",
    "        if np.random.random() < 0.3:\n",
    "            extended_wav = extended_wav[::-1]\n",
    "        return extended_wav\n",
    "    else:\n",
    "        extended_wav = np.append(wav, wav[::-1])\n",
    "        return extended_wav\n",
    "\n",
    "\n",
    "def lin_spectogram_from_wav(wav, hop_length, win_length, n_fft=1024):\n",
    "    linear = librosa.stft(wav, n_fft=n_fft, win_length=win_length, hop_length=hop_length) # linear spectrogram\n",
    "    return linear.T\n",
    "\n",
    "\n",
    "def load_data(path, win_length=400, sr=16000, hop_length=160, n_fft=512, spec_len=250, mode='train'):\n",
    "    wav = load_wav(path, sr=sr, mode=mode)\n",
    "    linear_spect = lin_spectogram_from_wav(wav, hop_length, win_length, n_fft)\n",
    "    mag, _ = librosa.magphase(linear_spect)  # magnitude\n",
    "    mag_T = mag.T\n",
    "    freq, time = mag_T.shape\n",
    "    if mode == 'train':\n",
    "        if time > spec_len:\n",
    "            randtime = np.random.randint(0, time-spec_len)\n",
    "            spec_mag = mag_T[:, randtime:randtime+spec_len]\n",
    "        else:\n",
    "            spec_mag = np.pad(mag_T, ((0, 0), (0, spec_len - time)), 'constant')\n",
    "    else:\n",
    "        spec_mag = mag_T\n",
    "    # preprocessing, subtract mean, divided by time-wise var\n",
    "    mu = np.mean(spec_mag, 0, keepdims=True)\n",
    "    std = np.std(spec_mag, 0, keepdims=True)\n",
    "    return (spec_mag - mu) / (std + 1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "files = glob('data/wav/enroll/*.wav')\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(257, 2538), (257, 3006), (257, 1936)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "files = glob('data/wav/enroll/*.wav')\n",
    "wavs = [load_data(wav, mode = 'eval') for wav in files]\n",
    "[wav.shape for wav in wavs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 512)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pred(x):\n",
    "    return sess.run(model.logits, feed_dict = {model.X: np.expand_dims([x], -1)})\n",
    "\n",
    "r = [pred(wav) for wav in wavs]\n",
    "results = np.concatenate(r)\n",
    "results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'v2/model.ckpt'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'v2/model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings = ','.join(\n",
    "    [\n",
    "        n.name\n",
    "        for n in tf.get_default_graph().as_graph_def().node\n",
    "        if ('Variable' in n.op\n",
    "        or 'Placeholder' in n.name\n",
    "        or 'logits' in n.name\n",
    "        or 'alphas' in n.name\n",
    "        or 'self/Softmax' in n.name)\n",
    "        and 'adam' not in n.name\n",
    "        and 'beta' not in n.name\n",
    "        and 'global_step' not in n.name\n",
    "        and 'Assign' not in n.name\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            'directory: %s' % model_dir\n",
    "        )\n",
    "\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "    absolute_model_dir = '/'.join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + '/frozen_model.pb'\n",
    "    clear_devices = True\n",
    "    with tf.Session(graph = tf.Graph()) as sess:\n",
    "        saver = tf.train.import_meta_graph(\n",
    "            input_checkpoint + '.meta', clear_devices = clear_devices\n",
    "        )\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess,\n",
    "            tf.get_default_graph().as_graph_def(),\n",
    "            output_node_names.split(','),\n",
    "        )\n",
    "        with tf.gfile.GFile(output_graph, 'wb') as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print('%d ops in the final graph.' % len(output_graph_def.node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from v2/model.ckpt\n",
      "INFO:tensorflow:Froze 198 variables.\n",
      "INFO:tensorflow:Converted 198 variables to const ops.\n",
      "1372 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "freeze_graph('v2', strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_graph(frozen_graph_filename):\n",
    "#     with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "#         graph_def = tf.GraphDef()\n",
    "#         graph_def.ParseFromString(f.read())\n",
    "#     with tf.Graph().as_default() as graph:\n",
    "#         tf.import_graph_def(graph_def)\n",
    "#     return graph\n",
    "\n",
    "def load_graph(frozen_graph_filename, **kwargs):\n",
    "    with tf.gfile.GFile(frozen_graph_filename, 'rb') as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # https://github.com/onnx/tensorflow-onnx/issues/77#issuecomment-445066091\n",
    "    # to fix import T5\n",
    "    for node in graph_def.node:\n",
    "        if node.op == 'RefSwitch':\n",
    "            node.op = 'Switch'\n",
    "            for index in xrange(len(node.input)):\n",
    "                if 'moving_' in node.input[index]:\n",
    "                    node.input[index] = node.input[index] + '/read'\n",
    "        elif node.op == 'AssignSub':\n",
    "            node.op = 'Sub'\n",
    "            if 'use_locking' in node.attr:\n",
    "                del node.attr['use_locking']\n",
    "        elif node.op == 'AssignAdd':\n",
    "            node.op = 'Add'\n",
    "            if 'use_locking' in node.attr:\n",
    "                del node.attr['use_locking']\n",
    "        elif node.op == 'Assign':\n",
    "            node.op = 'Identity'\n",
    "            if 'use_locking' in node.attr:\n",
    "                del node.attr['use_locking']\n",
    "            if 'validate_shape' in node.attr:\n",
    "                del node.attr['validate_shape']\n",
    "            if len(node.input) == 2:\n",
    "                node.input[0] = node.input[1]\n",
    "                del node.input[1]\n",
    "\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_graph('v2/frozen_model.pb')\n",
    "x = g.get_tensor_by_name('import/Placeholder:0')\n",
    "logits = g.get_tensor_by_name('import/logits:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/husein/.local/lib/python3.6/site-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "test_sess = tf.InteractiveSession(graph = g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(o):\n",
    "    return test_sess.run(logits, feed_dict = {x: np.expand_dims([o], -1)})\n",
    "\n",
    "r = [pred(wav) for wav in wavs]\n",
    "r = np.concatenate(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
