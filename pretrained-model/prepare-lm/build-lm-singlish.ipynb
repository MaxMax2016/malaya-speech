{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/singlish/singlish.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/singlish/sg-news.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/twitter/manglish.json\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/imda/singlish-text.json\n",
    "\n",
    "files = ['singlish.txt',\n",
    "        'sg-news.txt',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "vocabs = list(string.ascii_lowercase + string.digits) + [' ']\n",
    "directory = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "def preprocessing_text(string):\n",
    "    \n",
    "    string = re.sub(\n",
    "        'http\\\\S+|www.\\\\S+',\n",
    "        '',\n",
    "        ' '.join(\n",
    "            [\n",
    "                word\n",
    "                for word in string.split()\n",
    "                if word.find('#') < 0 and word.find('@') < 0\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "        \n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c if c in vocabs else ' ' for c in string])\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singlish.txt\n",
      "sg-news.txt\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    with open(os.path.join(directory, f)) as fopen:\n",
    "        text = list(filter(None, fopen.read().split('\\n')))\n",
    "        texts.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('bahasa-asr-train-combined.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "    \n",
    "texts.extend(data['Y'])\n",
    "\n",
    "with open('bahasa-asr-test.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "    \n",
    "texts.extend(data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('manglish.json') as fopen:\n",
    "    t = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "908120"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts.extend(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27585979"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('singlish-text.json') as fopen:\n",
    "    t = json.load(fopen)\n",
    "    \n",
    "texts.extend(t)\n",
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1724123/1724123 [01:33<00:00, 18446.27it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 9887.99it/s]450.04it/s]\n",
      " 81%|████████  | 1391727/1724123 [01:46<00:27, 12199.85it/s]\n",
      "100%|██████████| 1724123/1724123 [01:57<00:00, 14724.66it/s]\n",
      " 90%|████████▉ | 1543472/1724123 [01:53<00:11, 15381.61it/s]\n",
      " 87%|████████▋ | 1505604/1724123 [01:46<00:11, 18224.64it/s]\n",
      "100%|██████████| 1724123/1724123 [02:02<00:00, 14060.15it/s]\n",
      "100%|██████████| 1724123/1724123 [02:03<00:00, 13949.68it/s]\n",
      "100%|██████████| 1724123/1724123 [01:57<00:00, 14673.40it/s]\n",
      "100%|██████████| 1724123/1724123 [02:12<00:00, 13040.20it/s]\n",
      "100%|██████████| 1724123/1724123 [02:08<00:00, 13392.99it/s]\n",
      "100%|██████████| 1724123/1724123 [02:13<00:00, 12925.89it/s]\n",
      "100%|██████████| 1724123/1724123 [02:04<00:00, 13864.63it/s]\n",
      "100%|██████████| 1724123/1724123 [02:14<00:00, 12863.66it/s]\n",
      "100%|██████████| 1724123/1724123 [02:19<00:00, 12386.54it/s]\n",
      "100%|██████████| 1724123/1724123 [03:04<00:00, 9333.44it/s] \n",
      "100%|██████████| 1724123/1724123 [04:34<00:00, 6277.80it/s] \n"
     ]
    }
   ],
   "source": [
    "import mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "def loop(texts):\n",
    "    texts, _ = texts\n",
    "    cleaned_texts = []\n",
    "    for i in tqdm(range(len(texts))):\n",
    "        t = preprocessing_text(texts[i])\n",
    "        if len(t):\n",
    "            cleaned_texts.append(t)\n",
    "    return cleaned_texts\n",
    "\n",
    "cleaned_texts = mp.multiprocessing(texts, loop, cores = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27585979, 27355756)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), len(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(cleaned_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/husein/malaya-speech/text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 315567492 types 717175\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:8606100 2:75179687936 3:140961906688\n",
      "Statistics:\n",
      "1 717175 D1=0.686594 D2=1.03398 D3+=1.32634\n",
      "2 9941650/22150543 D1=0.72001 D2=1.06433 D3+=1.36637\n",
      "3 29758066/94062726 D1=0.674279 D2=1.42124 D3+=1.4373\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 756 assuming -p 1.5\n",
      "probing 815 assuming -r models -p 1.5\n",
      "trie    325 without quantization\n",
      "trie    188 assuming -q 8 -b 8 quantization \n",
      "trie    305 assuming -a 22 array pointer compression\n",
      "trie    168 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:8606100 2:159066400 3:595161320\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "###########********************#####################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:8606100 2:159066400 3:595161320\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:211232748 kB\tVmRSS:28384 kB\tRSSMax:50576856 kB\tuser:172.564\tsys:41.5169\tCPU:214.081\treal:223.907\n"
     ]
    }
   ],
   "source": [
    "!./kenlm/build/bin/lmplz --text text.txt --arpa out.arpa -o 3 --prune 0 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading out.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Identifying n-grams omitted by SRI\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Quantizing\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Writing trie\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!./kenlm/build/bin/build_binary -q 8 -b 7 -a 256 trie out.arpa out.trie.klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm text.txt out.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_application_key_id = os.environ['b2_application_key_id']\n",
    "b2_application_key = os.environ['b2_application_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from b2sdk.v1 import *\n",
    "info = InMemoryAccountInfo()\n",
    "b2_api = B2Api(info)\n",
    "application_key_id = b2_application_key_id\n",
    "application_key = b2_application_key\n",
    "b2_api.authorize_account(\"production\", application_key_id, application_key)\n",
    "file_info = {'how': 'good-file'}\n",
    "b2_bucket = b2_api.get_bucket_by_name('malaya-speech-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b2sdk.file_version.FileVersionInfo at 0x7fd10b51f470>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outPutname = 'language-model/manglish/model.trie.klm'\n",
    "b2_bucket.upload_local_file(\n",
    "    local_file='out.trie.klm',\n",
    "    file_name=outPutname,\n",
    "    file_infos=file_info,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
