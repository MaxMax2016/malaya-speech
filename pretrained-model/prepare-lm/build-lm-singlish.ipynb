{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/singlish/singlish.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/singlish/sg-news.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/twitter/manglish.json\n",
    "\n",
    "files = ['singlish.txt',\n",
    "        'sg-news.txt',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "vocabs = list(string.ascii_lowercase + string.digits) + [' ']\n",
    "directory = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "def preprocessing_text(string):\n",
    "    \n",
    "    string = re.sub(\n",
    "        'http\\\\S+|www.\\\\S+',\n",
    "        '',\n",
    "        ' '.join(\n",
    "            [\n",
    "                word\n",
    "                for word in string.split()\n",
    "                if word.find('#') < 0 and word.find('@') < 0\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    "        \n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c if c in vocabs else ' ' for c in string])\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "singlish.txt\n",
      "sg-news.txt\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    with open(os.path.join(directory, f)) as fopen:\n",
    "        text = list(filter(None, fopen.read().split('\\n')))\n",
    "        texts.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('bahasa-asr-train-combined.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "    \n",
    "texts.extend(data['Y'])\n",
    "\n",
    "with open('bahasa-asr-test.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "    \n",
    "texts.extend(data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('manglish.json') as fopen:\n",
    "    t = json.load(fopen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "908120"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1460357/1460357 [01:16<00:00, 19056.10it/s]\n",
      " 34%|███▍      | 500570/1460357 [01:14<02:32, 6274.72it/s] ]\n",
      "100%|██████████| 1460357/1460357 [01:34<00:00, 15471.57it/s]\n",
      "100%|██████████| 1460357/1460357 [01:33<00:00, 15561.60it/s]\n",
      "100%|██████████| 1460357/1460357 [01:37<00:00, 15042.55it/s]\n",
      "100%|██████████| 1460357/1460357 [01:35<00:00, 15227.97it/s]\n",
      "100%|██████████| 1460357/1460357 [01:39<00:00, 14625.52it/s]\n",
      "100%|██████████| 1460357/1460357 [01:37<00:00, 14992.24it/s]\n",
      "100%|██████████| 1460357/1460357 [01:47<00:00, 13537.89it/s]\n",
      "100%|██████████| 1460357/1460357 [01:47<00:00, 13567.21it/s]\n",
      "100%|██████████| 1460357/1460357 [01:44<00:00, 13940.22it/s]\n",
      "100%|██████████| 1460357/1460357 [01:48<00:00, 13420.48it/s]\n",
      "100%|██████████| 1460357/1460357 [01:48<00:00, 13485.91it/s]\n",
      "100%|██████████| 1460357/1460357 [01:49<00:00, 13345.69it/s]\n",
      "100%|██████████| 1460357/1460357 [01:47<00:00, 13530.75it/s]\n",
      "100%|██████████| 1460357/1460357 [02:53<00:00, 8418.47it/s] \n",
      "100%|██████████| 1460357/1460357 [03:56<00:00, 6176.70it/s] \n"
     ]
    }
   ],
   "source": [
    "import mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "def loop(texts):\n",
    "    texts, _ = texts\n",
    "    cleaned_texts = []\n",
    "    for i in tqdm(range(len(texts))):\n",
    "        t = preprocessing_text(texts[i])\n",
    "        if len(t):\n",
    "            cleaned_texts.append(t)\n",
    "    return cleaned_texts\n",
    "\n",
    "cleaned_texts = mp.multiprocessing(texts, loop, cores = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23365724, 23135501)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), len(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(cleaned_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/husein/malaya-speech/text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 273001650 types 695765\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:8349180 2:75179769856 3:140962070528\n",
      "Statistics:\n",
      "1 695765 D1=0.684986 D2=1.02996 D3+=1.33605\n",
      "2 8959693/21423984 D1=0.720398 D2=1.0641 D3+=1.36516\n",
      "3 24790903/89937448 D1=0.721381 D2=1.3111 D3+=1.47455\n",
      "Memory estimate for binary LM:\n",
      "type     MB\n",
      "probing 647 assuming -p 1.5\n",
      "probing 701 assuming -r models -p 1.5\n",
      "trie    281 without quantization\n",
      "trie    163 assuming -q 8 -b 8 quantization \n",
      "trie    264 assuming -a 22 array pointer compression\n",
      "trie    146 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:8349180 2:143355088 3:495818060\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "########*******************#########################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:8349180 2:143355088 3:495818060\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:211232748 kB\tVmRSS:22684 kB\tRSSMax:50495752 kB\tuser:158.286\tsys:40.7429\tCPU:199.029\treal:207.573\n"
     ]
    }
   ],
   "source": [
    "!./kenlm/build/bin/lmplz --text text.txt --arpa out.arpa -o 3 --prune 0 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading out.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Identifying n-grams omitted by SRI\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Quantizing\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Writing trie\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!./kenlm/build/bin/build_binary -q 8 -b 7 -a 256 trie out.arpa out.trie.klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm text.txt out.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_application_key_id = os.environ['b2_application_key_id']\n",
    "b2_application_key = os.environ['b2_application_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from b2sdk.v1 import *\n",
    "info = InMemoryAccountInfo()\n",
    "b2_api = B2Api(info)\n",
    "application_key_id = b2_application_key_id\n",
    "application_key = b2_application_key\n",
    "b2_api.authorize_account(\"production\", application_key_id, application_key)\n",
    "file_info = {'how': 'good-file'}\n",
    "b2_bucket = b2_api.get_bucket_by_name('malaya-speech-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b2sdk.file_version.FileVersionInfo at 0x7f5309055d68>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outPutname = 'language-model/manglish/model.trie.klm'\n",
    "b2_bucket.upload_local_file(\n",
    "    local_file='out.trie.klm',\n",
    "    file_name=outPutname,\n",
    "    file_infos=file_info,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
