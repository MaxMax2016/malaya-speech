{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/clean/filtered-dumping-academia.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/clean/filtered-dumping-wiki.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/clean/dumping-cleaned-news.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/clean/dumping-iium.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/clean/dumping-parliament.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/clean/dumping-watpadd.txt\n",
    "# !wget https://f000.backblazeb2.com/file/malay-dataset/dumping/clean/filtered-dumping-cleaned-common-crawl.txt\n",
    "\n",
    "files = ['filtered-dumping-academia.txt',\n",
    "        'filtered-dumping-wiki.txt',\n",
    "        'dumping-cleaned-news.txt',\n",
    "        'dumping-iium.txt',\n",
    "        'dumping-parliament.txt',\n",
    "        'dumping-watpadd.txt',\n",
    "        'filtered-dumping-cleaned-common-crawl.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "vocabs = list(string.ascii_lowercase + string.digits) + [' ']\n",
    "directory = '/home/husein/pure-text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "def preprocessing_text(string):\n",
    "        \n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = ''.join([c if c in vocabs else ' ' for c in string])\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered-dumping-academia.txt\n",
      "filtered-dumping-wiki.txt\n",
      "dumping-cleaned-news.txt\n",
      "dumping-iium.txt\n",
      "dumping-parliament.txt\n",
      "dumping-watpadd.txt\n",
      "filtered-dumping-cleaned-common-crawl.txt\n"
     ]
    }
   ],
   "source": [
    "texts = []\n",
    "\n",
    "for f in files:\n",
    "    print(f)\n",
    "    with open(os.path.join(directory, f)) as fopen:\n",
    "        text = list(filter(None, fopen.read().split('\\n')))\n",
    "        texts.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('bahasa-asr-train-combined.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "    \n",
    "texts.extend(data['Y'])\n",
    "\n",
    "with open('bahasa-asr-test.json') as fopen:\n",
    "    data = json.load(fopen)\n",
    "    \n",
    "texts.extend(data['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3490847/3490847 [03:38<00:00, 15956.62it/s]\n",
      "100%|██████████| 13/13 [00:00<00:00, 12451.69it/s]18.89it/s]\n",
      "100%|██████████| 3490847/3490847 [03:44<00:00, 15541.87it/s]\n",
      " 92%|█████████▏| 3195735/3490847 [03:28<00:16, 18300.78it/s]\n",
      "100%|██████████| 3490847/3490847 [03:45<00:00, 15485.29it/s]\n",
      "100%|██████████| 3490847/3490847 [03:44<00:00, 15552.27it/s]\n",
      "100%|██████████| 3490847/3490847 [03:42<00:00, 15674.29it/s]\n",
      "100%|██████████| 3490847/3490847 [03:40<00:00, 15823.05it/s]\n",
      "100%|██████████| 3490847/3490847 [03:43<00:00, 15605.64it/s]\n",
      "100%|██████████| 3490847/3490847 [03:41<00:00, 15735.65it/s]\n",
      "100%|██████████| 3490847/3490847 [03:46<00:00, 15400.32it/s]\n",
      "100%|██████████| 3490847/3490847 [04:04<00:00, 14268.06it/s]\n",
      "100%|██████████| 3490847/3490847 [03:44<00:00, 15553.65it/s]\n",
      "100%|██████████| 3490847/3490847 [03:54<00:00, 14856.22it/s]\n",
      "100%|██████████| 3490847/3490847 [06:22<00:00, 9121.85it/s] \n",
      "100%|██████████| 3490847/3490847 [06:25<00:00, 9054.89it/s] \n",
      "100%|██████████| 3490847/3490847 [06:54<00:00, 8422.59it/s] \n"
     ]
    }
   ],
   "source": [
    "import mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "def loop(texts):\n",
    "    texts, _ = texts\n",
    "    cleaned_texts = []\n",
    "    for i in tqdm(range(len(texts))):\n",
    "        t = preprocessing_text(texts[i])\n",
    "        if len(t):\n",
    "            cleaned_texts.append(t)\n",
    "    return cleaned_texts\n",
    "\n",
    "cleaned_texts = mp.multiprocessing(texts, loop, cores = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55853565, 55826054)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts), len(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19870767"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('singlish.txt') as fopen:\n",
    "    s = list(filter(None, fopen.read().split('\\n')))\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_texts.extend(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('text.txt', 'w') as fopen:\n",
    "    fopen.write('\\n'.join(cleaned_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/husein/malaya-speech/text.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 805900650 types 4918146\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:59017752 2:75162148864 3:140929040384\n",
      "Statistics:\n",
      "1 4918146 D1=0.748703 D2=1.03679 D3+=1.28125\n",
      "2 25120009/64555612 D1=0.76653 D2=1.06148 D3+=1.31234\n",
      "3 61233336/212507344 D1=0.717087 D2=1.30735 D3+=1.42433\n",
      "Memory estimate for binary LM:\n",
      "type      MB\n",
      "probing 1748 assuming -p 1.5\n",
      "probing 1910 assuming -r models -p 1.5\n",
      "trie     842 without quantization\n",
      "trie     533 assuming -q 8 -b 8 quantization \n",
      "trie     788 assuming -a 22 array pointer compression\n",
      "trie     480 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:59017752 2:401920144 3:1224666720\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "#########*******************########################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:59017752 2:401920144 3:1224666720\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:211257332 kB\tVmRSS:6416 kB\tRSSMax:52980956 kB\tuser:416.784\tsys:73.096\tCPU:489.88\treal:514.28\n"
     ]
    }
   ],
   "source": [
    "!./kenlm/build/bin/lmplz --text text.txt --arpa out.arpa -o 3 --prune 0 1 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading out.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Identifying n-grams omitted by SRI\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Quantizing\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Writing trie\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "!./kenlm/build/bin/build_binary -q 8 -b 7 -a 256 trie out.arpa out-mixed.trie.klm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm text.txt out.arpa singlish.txt sg-news.txt manglish.json singlish-text.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2_application_key_id = os.environ['b2_application_key_id']\n",
    "b2_application_key = os.environ['b2_application_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from b2sdk.v1 import *\n",
    "info = InMemoryAccountInfo()\n",
    "b2_api = B2Api(info)\n",
    "application_key_id = b2_application_key_id\n",
    "application_key = b2_application_key\n",
    "b2_api.authorize_account(\"production\", application_key_id, application_key)\n",
    "file_info = {'how': 'good-file'}\n",
    "b2_bucket = b2_api.get_bucket_by_name('malaya-speech-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<b2sdk.file_version.FileVersionInfo at 0x7f3664eb72b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outPutname = 'language-model/bahasa-manglish-combined/model.trie.klm'\n",
    "b2_bucket.upload_local_file(\n",
    "    local_file='out-mixed.trie.klm',\n",
    "    file_name=outPutname,\n",
    "    file_infos=file_info,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
