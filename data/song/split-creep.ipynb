{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from torchaudio.pipelines import HDEMUCS_HIGH_MUSDB_PLUS\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    try:\n",
    "        import google.colab\n",
    "\n",
    "        print(\n",
    "            \"\"\"\n",
    "            To enable running this notebook in Google Colab, install nightly\n",
    "            torch and torchaudio builds by adding the following code block to the top\n",
    "            of the notebook before running it:\n",
    "            !pip3 uninstall -y torch torchvision torchaudio\n",
    "            !pip3 install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cpu\n",
    "            !pip3 install mir_eval\n",
    "            \"\"\"\n",
    "        )\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = HDEMUCS_HIGH_MUSDB_PLUS\n",
    "\n",
    "model = bundle.get_model()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "sample_rate = bundle.sample_rate\n",
    "\n",
    "print(f\"Sample rate: {sample_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import Fade\n",
    "\n",
    "\n",
    "def separate_sources(\n",
    "        model,\n",
    "        mix,\n",
    "        segment=10.,\n",
    "        overlap=0.1,\n",
    "        device=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply model to a given mixture. Use fade, and add segments together in order to add model segment by segment.\n",
    "\n",
    "    Args:\n",
    "        segment (int): segment length in seconds\n",
    "        device (torch.device, str, or None): if provided, device on which to\n",
    "            execute the computation, otherwise `mix.device` is assumed.\n",
    "            When `device` is different from `mix.device`, only local computations will\n",
    "            be on `device`, while the entire tracks will be stored on `mix.device`.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = mix.device\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch, channels, length = mix.shape\n",
    "\n",
    "    chunk_len = int(sample_rate * segment * (1 + overlap))\n",
    "    start = 0\n",
    "    end = chunk_len\n",
    "    overlap_frames = overlap * sample_rate\n",
    "    fade = Fade(fade_in_len=0, fade_out_len=int(overlap_frames), fade_shape='linear')\n",
    "\n",
    "    final = torch.zeros(batch, len(model.sources), channels, length, device=device)\n",
    "\n",
    "    while start < length - overlap_frames:\n",
    "        chunk = mix[:, :, start:end]\n",
    "        with torch.no_grad():\n",
    "            out = model.forward(chunk)\n",
    "        out = fade(out)\n",
    "        final[:, :, :, start:end] += out\n",
    "        if start == 0:\n",
    "            fade.fade_in_len = int(overlap_frames)\n",
    "            start += int(chunk_len - overlap_frames)\n",
    "        else:\n",
    "            start += chunk_len\n",
    "        end += chunk_len\n",
    "        if end >= length:\n",
    "            fade.fade_out_len = 0\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from tqdm import tqdm\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "segment = 10\n",
    "overlap = 0.1\n",
    "length = 10\n",
    "\n",
    "def get_split(s):\n",
    "    waveform, sample_rate = torchaudio.load(s)\n",
    "    # waveform = torch.concat([waveform, waveform])\n",
    "    waveform = waveform.to(device)\n",
    "    mixture = waveform\n",
    "    \n",
    "    ref = waveform.mean(0)\n",
    "    waveform = (waveform - ref.mean()) / ref.std()\n",
    "\n",
    "    sources = separate_sources(\n",
    "        model,\n",
    "        waveform[None],\n",
    "        device=device,\n",
    "        segment=segment,\n",
    "        overlap=overlap,\n",
    "    )[0]\n",
    "    sources = sources * ref.std() + ref.mean()\n",
    "\n",
    "    sources_list = model.sources\n",
    "    sources = list(sources)\n",
    "\n",
    "    audios = dict(zip(sources_list, sources))\n",
    "    return audios, sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted, sample_rate_ = get_split('Radiohead - Creep.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = {\n",
    "    'drums': 0.7, 'bass': 1.0,\n",
    "    'other': 0.7, 'vocals': 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for k in splitted:\n",
    "    c += splitted[k] * ratio[k]\n",
    "    \n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(c.numpy()[0], rate = sample_rate_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted['vocals'].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io.wavfile import write\n",
    "\n",
    "write('creep-vocal.wav', sample_rate_, splitted['vocals'].numpy()[0][60 * sample_rate_ : 120 * sample_rate_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio('/home/husein/ssd3/so-vits-svc/results/creep-vocal.wav_0key_speaker_0.flac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
