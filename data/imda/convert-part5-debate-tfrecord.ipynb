{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "539d06b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'mesolitica-tpu.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "005ebcb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.7/site-packages/malaya_boilerplate/frozen_graph.py:25: UserWarning: Cannot import beam_search_ops from Tensorflow Addons, `deep_model` for stemmer will not available to use, make sure Tensorflow Addons version >= 0.12.0\n",
      "  'Cannot import beam_search_ops from Tensorflow Addons, `deep_model` for stemmer will not available to use, make sure Tensorflow Addons version >= 0.12.0'\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "import malaya_speech\n",
    "from malaya_speech.utils import subword\n",
    "import numpy as np\n",
    "import mp\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9048d900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234465"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singlish = glob('debate/wav/*.wav')\n",
    "len(singlish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed38d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "subwords = subword.load('transducer-singlish.subword')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c13e32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "vocabs = [\" \", \"a\", \"e\", \"n\", \"i\", \"t\", \"o\", \"u\", \"s\", \"k\", \"r\", \"l\", \"h\", \"d\", \"m\", \"g\", \"y\", \"b\", \"p\", \"w\", \"c\", \"f\", \"j\", \"v\", \"z\", \"0\", \"1\", \"x\", \"2\", \"q\", \"5\", \"3\", \"4\", \"6\", \"9\", \"8\", \"7\"]\n",
    "\n",
    "def preprocessing_text(string):\n",
    "    \n",
    "    string = unicodedata.normalize('NFC', string.lower())\n",
    "    string = string.replace('\\'', '')\n",
    "    string = ''.join([c if c in vocabs else ' ' for c in string])\n",
    "    string = re.sub(r'[ ]+', ' ', string).strip()\n",
    "    string = (\n",
    "        ''.join(''.join(s)[:2] for _, s in itertools.groupby(string))\n",
    "    )\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82752ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_after_mandarin(word):\n",
    "    if '<mandarin>' in word:\n",
    "        w = word.split('>')[1].split(':')[1]\n",
    "        return w.split('</')[0]\n",
    "    else:\n",
    "        return word\n",
    "    \n",
    "def get_before_mandarin(word):\n",
    "    if '</mandarin>' in word:\n",
    "        return word.split('</')[0]\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def replace_paralinguistic(string, replaces = ['(ppb)', '(ppc)', '(ppl)', '(ppo)', '<UNK>', '<MANDARIN>']):\n",
    "    for r in replaces:\n",
    "        string = string.replace(r, ' ')\n",
    "    string = string.split()\n",
    "    string = [get_after_mandarin(w) for w in string]\n",
    "    string = [get_before_mandarin(w) for w in string]\n",
    "    string = [w for w in string if w[0] not in '<[(' and w[-1] not in '>])']\n",
    "    return ' '.join(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86aa3a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(files):\n",
    "    files, index = files\n",
    "    results = []\n",
    "    for i in tqdm(files):\n",
    "        try:\n",
    "            p = i.replace('/wav','/text')\n",
    "            with open(f'{p}.txt') as fopen:\n",
    "                text = fopen.read()\n",
    "            if len(text) < 2:\n",
    "                continue\n",
    "            if text[0] == '<' and text[-1] == '>':\n",
    "                continue\n",
    "            text = replace_paralinguistic(text)\n",
    "            text = preprocessing_text(text)\n",
    "            if len(text):\n",
    "                results.append((i, text))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0f3660d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['debate/wav/app_4165_6329_phnd_deb-3-0-179.wav',\n",
       " 'debate/wav/app_4154_6308_phnd_deb-1-0-4.wav',\n",
       " 'debate/wav/app_4221_6441_phnd_deb-1-0-59.wav',\n",
       " 'debate/wav/app_4238_6475_phnd_deb-1-0-170.wav',\n",
       " 'debate/wav/app_4074_6148_phnd_deb-1-0-49.wav',\n",
       " 'debate/wav/app_4153_6306_phnd_deb-3-0-178.wav',\n",
       " 'debate/wav/app_4127_6254_phnd_deb-1-0-8.wav',\n",
       " 'debate/wav/app_4228_6456_phnd_deb-2-0-138.wav',\n",
       " 'debate/wav/app_4149_6297_phnd_deb-2-0-74.wav',\n",
       " 'debate/wav/app_4168_6336_phnd_deb-3-0-52.wav']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "singlish[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1ac919d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 4980.77it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('debate/wav/app_4221_6441_phnd_deb-1-0-59.wav',\n",
       "  'so theres not really a a a job issue here is just more for the opportunity or whether people want to take it in the first place'),\n",
       " ('debate/wav/app_4238_6475_phnd_deb-1-0-170.wav',\n",
       "  'for brands and businesses to gain some sort of traction some sort reputation and i feel like that in a business point of view is'),\n",
       " ('debate/wav/app_4149_6297_phnd_deb-2-0-74.wav',\n",
       "  'recycling so what should have to have been done what should have been done is that the latter')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loop((singlish[:10], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "008618f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19538/19538 [02:06<00:00, 153.98it/s]\n",
      "100%|██████████| 19538/19538 [02:06<00:00, 153.89it/s]\n",
      "100%|██████████| 9/9 [00:00<00:00, 145.53it/s]89it/s] \n",
      "100%|██████████| 19538/19538 [02:07<00:00, 152.97it/s]\n",
      "100%|██████████| 19538/19538 [02:08<00:00, 152.57it/s]\n",
      "100%|██████████| 19538/19538 [02:08<00:00, 152.61it/s]\n",
      "100%|██████████| 19538/19538 [02:08<00:00, 152.49it/s]\n",
      "100%|██████████| 19538/19538 [02:08<00:00, 152.28it/s]\n",
      "100%|██████████| 19538/19538 [02:08<00:00, 152.19it/s]\n",
      "100%|██████████| 19538/19538 [02:08<00:00, 151.73it/s]\n",
      "100%|██████████| 19538/19538 [02:08<00:00, 151.61it/s]\n",
      "100%|██████████| 19538/19538 [02:09<00:00, 150.52it/s]\n",
      "100%|██████████| 19538/19538 [02:11<00:00, 149.14it/s]\n"
     ]
    }
   ],
   "source": [
    "singlishs = mp.multiprocessing(singlish, loop, cores = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "184f7a56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126842"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(singlishs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a23cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "\n",
    "def to_example(dictionary):\n",
    "    \"\"\"Helper: build tf.Example from (string -> int/float/str list) dictionary.\"\"\"\n",
    "    features = {}\n",
    "    for (k, v) in six.iteritems(dictionary):\n",
    "        if not v:\n",
    "            raise ValueError('Empty generated field: %s' % str((k, v)))\n",
    "        # Subtly in PY2 vs PY3, map is not scriptable in py3. As a result,\n",
    "        # map objects will fail with TypeError, unless converted to a list.\n",
    "        if six.PY3 and isinstance(v, map):\n",
    "            v = list(v)\n",
    "        if isinstance(v[0], six.integer_types) or np.issubdtype(\n",
    "            type(v[0]), np.integer\n",
    "        ):\n",
    "            features[k] = tf.train.Feature(\n",
    "                int64_list=tf.train.Int64List(value=v)\n",
    "            )\n",
    "        elif isinstance(v[0], float):\n",
    "            features[k] = tf.train.Feature(\n",
    "                float_list=tf.train.FloatList(value=v)\n",
    "            )\n",
    "        elif isinstance(v[0], six.string_types):\n",
    "            if not six.PY2:  # Convert in python 3.\n",
    "                v = [bytes(x, 'utf-8') for x in v]\n",
    "            features[k] = tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=v)\n",
    "            )\n",
    "        elif isinstance(v[0], bytes):\n",
    "            features[k] = tf.train.Feature(\n",
    "                bytes_list=tf.train.BytesList(value=v)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Value for %s is not a recognized type; v: %s type: %s'\n",
    "                % (k, str(v[0]), str(type(v[0])))\n",
    "            )\n",
    "    return tf.train.Example(features=tf.train.Features(feature=features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1ac88c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 16000\n",
    "maxlen = 18\n",
    "minlen_text = 1\n",
    "global_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0356b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(files):\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket('mesolitica-tpu-general')\n",
    "    files, index = files\n",
    "    output_file = f'{index}-{global_count}.tfrecord'\n",
    "    writer = tf.io.TFRecordWriter(output_file)\n",
    "    for s in tqdm(files):\n",
    "        try:\n",
    "            if len(s[1]) < minlen_text:\n",
    "                continue\n",
    "            y, _ = malaya_speech.load(s[0])\n",
    "            if (len(y) / sr) > maxlen:\n",
    "                continue\n",
    "            t = subword.encode(subwords, s[1], add_blank=False)\n",
    "            example = to_example({'waveforms': y.tolist(), \n",
    "                                  'targets': t, \n",
    "                                  'targets_length': [len(t)]})\n",
    "            writer.write(example.SerializeToString())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    writer.close()\n",
    "    blob = bucket.blob(f'imda/part5-debate/{output_file}')\n",
    "    blob.upload_from_filename(output_file)\n",
    "    os.system(f'rm {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "720c6c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 14.62it/s]\n"
     ]
    }
   ],
   "source": [
    "loop((singlishs[:10], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7efaab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 3249/4166 [04:19<00:54, 16.85it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 47%|████▋     | 1959/4166 [02:12<02:40, 13.72it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 4166/4166 [04:31<00:00, 15.35it/s]\n",
      "100%|██████████| 4166/4166 [04:32<00:00, 15.28it/s]\n",
      "100%|██████████| 4166/4166 [04:32<00:00, 15.26it/s]\n",
      "100%|██████████| 4166/4166 [04:33<00:00, 15.22it/s]\n",
      "100%|██████████| 4166/4166 [04:34<00:00, 15.19it/s]\n",
      "100%|██████████| 4166/4166 [04:34<00:00, 15.18it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 65.38it/s]\n",
      " 23%|██▎       | 952/4166 [01:01<03:53, 13.77it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "100%|██████████| 4166/4166 [04:19<00:00, 16.03it/s]\n",
      " 96%|█████████▋| 4020/4166 [04:21<00:09, 15.76it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      " 66%|██████▌   | 2755/4166 [03:12<01:19, 17.81it/s]IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 25000\n",
    "for i in range(0, len(singlishs), batch_size):\n",
    "    batch = singlishs[i: i + batch_size]\n",
    "    mp.multiprocessing(batch, loop, cores = 6, returned = False)\n",
    "    global_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2638dac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from malaya_speech.utils import tf_featurization\n",
    "\n",
    "config = malaya_speech.config.transducer_featurizer_config\n",
    "featurizer = tf_featurization.STTFeaturizer(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a0d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 80\n",
    "\n",
    "def preprocess_inputs(example):\n",
    "    s = featurizer.vectorize(example['waveforms'])\n",
    "    mel_fbanks = tf.reshape(s, (-1, n_mels))\n",
    "    example['inputs'] = mel_fbanks\n",
    "    return example\n",
    "\n",
    "def parse(serialized_example):\n",
    "\n",
    "    data_fields = {\n",
    "        'waveforms': tf.compat.v1.VarLenFeature(tf.float32),\n",
    "        'targets': tf.compat.v1.VarLenFeature(tf.int64),\n",
    "        'targets_length': tf.compat.v1.VarLenFeature(tf.int64),\n",
    "    }\n",
    "    features = tf.compat.v1.parse_single_example(\n",
    "        serialized_example, features = data_fields\n",
    "    )\n",
    "    for k in features.keys():\n",
    "        features[k] = features[k].values\n",
    "        \n",
    "    features = preprocess_inputs(features)\n",
    "\n",
    "    keys = list(features.keys())\n",
    "    for k in keys:\n",
    "        if k not in ['waveforms', 'inputs', 'targets', 'targets_length']:\n",
    "            features.pop(k, None)\n",
    "\n",
    "    return features\n",
    "\n",
    "def get_dataset(files, batch_size = 2, shuffle_size = 32, thread_count = 24):\n",
    "    def get():\n",
    "        dataset = tf.data.TFRecordDataset(files)\n",
    "        dataset = dataset.shuffle(shuffle_size)\n",
    "        dataset = dataset.map(parse, num_parallel_calls = thread_count)\n",
    "        dataset = dataset.repeat()\n",
    "        return dataset\n",
    "\n",
    "    return get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e778de34",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = tf.io.gfile.glob('gs://mesolitica-tpu-general/imda/part5-debate/*.tfrecord')\n",
    "d = get_dataset(files)()\n",
    "d = d.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81239514",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ee91e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
